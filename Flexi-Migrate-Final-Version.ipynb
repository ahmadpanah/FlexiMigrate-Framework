{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enums for Migration States and Strategies\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class MigrationState(Enum):\n",
    "    PENDING = 1\n",
    "    PLANNING = 2\n",
    "    PREPARATION = 3\n",
    "    EXECUTION = 4\n",
    "    VERIFICATION = 5\n",
    "    COMPLETED = 6\n",
    "    FAILED = 7\n",
    "    ROLLBACK = 8\n",
    "\n",
    "class MigrationStrategy(Enum):\n",
    "    COLD_MIGRATION = 1\n",
    "    PRE_COPY = 2\n",
    "    POST_COPY = 3\n",
    "    LIVE_MIGRATION = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Classes\n",
    "\n",
    "class Container:\n",
    "    def __init__(self, container_id, image, cpu_limit, memory_limit, storage_limit):\n",
    "        self.container_id = container_id\n",
    "        self.image = image\n",
    "        self.cpu_limit = cpu_limit\n",
    "        self.memory_limit = memory_limit\n",
    "        self.storage_limit = storage_limit\n",
    "        self.state = None\n",
    "        self.host = None\n",
    "\n",
    "class Host:\n",
    "    def __init__(self, host_id, total_cpu, total_memory, total_storage):\n",
    "        self.host_id = host_id\n",
    "        self.total_cpu = total_cpu\n",
    "        self.total_memory = total_memory\n",
    "        self.total_storage = total_storage\n",
    "        self.available_cpu = total_cpu\n",
    "        self.available_memory = total_memory\n",
    "        self.available_storage = total_storage\n",
    "        self.containers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migration Manager Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MigrationManager:\n",
    "    def __init__(self, decision_engine, state_synchronizer, migration_strategy_selector, logging_monitoring, network_manager):\n",
    "        self.decision_engine = decision_engine\n",
    "        self.state_synchronizer = state_synchronizer\n",
    "        self.migration_strategy_selector = migration_strategy_selector\n",
    "        self.logging_monitoring = logging_monitoring\n",
    "        self.network_manager = network_manager\n",
    "        self.migration_requests = []\n",
    "        self.docker_client = docker.from_env()\n",
    "        \n",
    "    \n",
    "        # Initialize a graph to represent the cluster topology\n",
    "        self.cluster_topology = nx.Graph()\n",
    "        \n",
    "        # Cache to store host information\n",
    "        self.host_cache = {}\n",
    "        \n",
    "        # Threshold for host selection (in percentage)\n",
    "        self.cpu_threshold = 80\n",
    "        self.memory_threshold = 80\n",
    "        self.network_threshold = 70\n",
    "\n",
    "    async def get_current_host(self, container: Any) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Determines the current host of a given container.\n",
    "\n",
    "        Args:\n",
    "            container: The container object.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing information about the current host.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the current host cannot be determined.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First, try to get information from Docker\n",
    "            container_info = self.docker_client.containers.get(container.id)\n",
    "            node_id = container_info.attrs['Node']['ID']\n",
    "            \n",
    "            if node_id in self.host_cache:\n",
    "                return self.host_cache[node_id]\n",
    "            \n",
    "            node_info = self.docker_client.nodes.get(node_id)\n",
    "            host_info = {\n",
    "                'id': node_id,\n",
    "                'name': node_info.attrs['Description']['Hostname'],\n",
    "                'address': node_info.attrs['Status']['Addr'],\n",
    "                'architecture': node_info.attrs['Description']['Platform']['Architecture'],\n",
    "                'os': node_info.attrs['Description']['Platform']['OS'],\n",
    "                'resources': node_info.attrs['Description']['Resources'],\n",
    "            }\n",
    "            \n",
    "            # Cache the host information\n",
    "            self.host_cache[node_id] = host_info\n",
    "            \n",
    "            return host_info\n",
    "\n",
    "        except docker.errors.NotFound:\n",
    "\n",
    "                raise Exception(f\"Unable to determine current host for container {container.id}\")\n",
    "\n",
    "    async def get_potential_hosts(self, container: Any) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Identifies potential hosts for migrating a given container.\n",
    "\n",
    "        Args:\n",
    "            container: The container object to be migrated.\n",
    "\n",
    "        Returns:\n",
    "            A list of dictionaries, each containing information about a potential host.\n",
    "        \"\"\"\n",
    "        potential_hosts = []\n",
    "        current_host = await self.get_current_host(container)\n",
    "        \n",
    "        # Get all nodes in the cluster\n",
    "        try:\n",
    "            nodes = self.docker_client.nodes.list()\n",
    "        except client.exceptions.ApiException:\n",
    "            nodes = self.docker_client.nodes.list()\n",
    "        \n",
    "        for node in nodes:\n",
    "            # Skip the current host\n",
    "            if self._is_same_host(node, current_host):\n",
    "                continue\n",
    "            \n",
    "            host_info = self._get_host_info(node)\n",
    "            \n",
    "            # Check if the host meets the resource requirements\n",
    "            if self._meets_resource_requirements(host_info, container):\n",
    "                # Check network conditions\n",
    "                network_suitable = await self._check_network_conditions(current_host, host_info)\n",
    "                \n",
    "                if network_suitable:\n",
    "                    potential_hosts.append(host_info)\n",
    "        \n",
    "        # Sort potential hosts based on suitability\n",
    "        sorted_hosts = await self._sort_hosts_by_suitability(potential_hosts, container)\n",
    "        \n",
    "        return sorted_hosts\n",
    "\n",
    "    def _is_same_host(self, node: Any, current_host: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if a given node is the same as the current host.\n",
    "        \"\"\"\n",
    "        if isinstance(node, client.models.v1_node.V1Node):\n",
    "            return node.metadata.uid == current_host['id']\n",
    "        else:  # Docker node\n",
    "            return node.id == current_host['id']\n",
    "\n",
    "    def _get_host_info(self, node: Any) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extracts relevant information from a node object.\n",
    "        \"\"\"\n",
    "        if isinstance(node, client.models.v1_node.V1Node):\n",
    "            return {\n",
    "                'id': node.metadata.uid,\n",
    "                'name': node.metadata.name,\n",
    "                'address': node.status.addresses[0].address,\n",
    "                'architecture': node.status.node_info.architecture,\n",
    "                'os': node.status.node_info.os_image,\n",
    "                'resources': {\n",
    "                    'NanoCPUs': int(node.status.capacity['cpu']) * 1e9,\n",
    "                    'MemoryBytes': int(node.status.capacity['memory'].rstrip('Ki')) * 1024,\n",
    "                },\n",
    "            }\n",
    "        else:  # Docker node\n",
    "            return {\n",
    "                'id': node.id,\n",
    "                'name': node.attrs['Description']['Hostname'],\n",
    "                'address': node.attrs['Status']['Addr'],\n",
    "                'architecture': node.attrs['Description']['Platform']['Architecture'],\n",
    "                'os': node.attrs['Description']['Platform']['OS'],\n",
    "                'resources': node.attrs['Description']['Resources'],\n",
    "            }\n",
    "\n",
    "    def _meets_resource_requirements(self, host: Dict[str, Any], container: Any) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if a host meets the resource requirements of a container.\n",
    "        \"\"\"\n",
    "        # Get container resource requirements\n",
    "        container_info = self.docker_client.containers.get(container.id)\n",
    "        container_cpu = container_info.attrs['HostConfig']['NanoCpus']\n",
    "        container_memory = container_info.attrs['HostConfig']['Memory']\n",
    "        \n",
    "        # Check available resources on the host\n",
    "        host_cpu_available = host['resources']['NanoCPUs'] * (100 - self.cpu_threshold) / 100\n",
    "        host_memory_available = host['resources']['MemoryBytes'] * (100 - self.memory_threshold) / 100\n",
    "        \n",
    "        return container_cpu <= host_cpu_available and container_memory <= host_memory_available\n",
    "\n",
    "    async def _check_network_conditions(self, source_host: Dict[str, Any], destination_host: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if network conditions are suitable for migration between two hosts.\n",
    "        \"\"\"\n",
    "        network_metrics = await self.network_manager.get_network_metrics(source_host['id'], destination_host['id'])\n",
    "        \n",
    "        # Check if bandwidth utilization is below threshold\n",
    "        bandwidth_utilization = network_metrics['bandwidth_utilization']\n",
    "        return bandwidth_utilization <= self.network_threshold\n",
    "\n",
    "    async def _sort_hosts_by_suitability(self, hosts: List[Dict[str, Any]], container: Any) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Sorts potential hosts based on their suitability for the given container.\n",
    "        \"\"\"\n",
    "        host_scores = []\n",
    "        \n",
    "        for host in hosts:\n",
    "            score = await self._calculate_host_suitability(host, container)\n",
    "            host_scores.append((host, score))\n",
    "        \n",
    "        # Sort hosts by score in descending order\n",
    "        sorted_hosts = [host for host, score in sorted(host_scores, key=lambda x: x[1], reverse=True)]\n",
    "        \n",
    "        return sorted_hosts\n",
    "\n",
    "    async def _calculate_host_suitability(self, host: Dict[str, Any], container: Any) -> float:\n",
    "        \"\"\"\n",
    "        Calculates a suitability score for a host with respect to a container.\n",
    "        \"\"\"\n",
    "        # Get container resource requirements\n",
    "        container_info = self.docker_client.containers.get(container.id)\n",
    "        container_cpu = container_info.attrs['HostConfig']['NanoCpus']\n",
    "        container_memory = container_info.attrs['HostConfig']['Memory']\n",
    "        \n",
    "        # Calculate resource availability scores\n",
    "        cpu_score = (host['resources']['NanoCPUs'] - container_cpu) / host['resources']['NanoCPUs']\n",
    "        memory_score = (host['resources']['MemoryBytes'] - container_memory) / host['resources']['MemoryBytes']\n",
    "        \n",
    "        # Get network conditions\n",
    "        network_metrics = await self.network_manager.get_network_metrics(container.attrs['Node']['ID'], host['id'])\n",
    "        network_score = 1 - (network_metrics['bandwidth_utilization'] / 100)\n",
    "        \n",
    "        # Calculate overall score (you can adjust weights as needed)\n",
    "        overall_score = 0.4 * cpu_score + 0.4 * memory_score + 0.2 * network_score\n",
    "        \n",
    "        return overall_score\n",
    "    \n",
    "    async def process_migration(self, migration_request):\n",
    "        try:\n",
    "            migration_request.state = MigrationState.PLANNING\n",
    "            plan = await self.decision_engine.plan_migration(migration_request)\n",
    "            \n",
    "            migration_request.state = MigrationState.PREPARATION\n",
    "            await self.state_synchronizer.prepare_migration(migration_request, plan)\n",
    "            \n",
    "            migration_request.state = MigrationState.EXECUTION\n",
    "            success = await self.state_synchronizer.perform_migration(migration_request, plan)\n",
    "            \n",
    "            if success:\n",
    "                migration_request.state = MigrationState.VERIFICATION\n",
    "                verified = await self.state_synchronizer.verify_migration(migration_request)\n",
    "                \n",
    "                if verified:\n",
    "                    migration_request.state = MigrationState.COMPLETED\n",
    "                else:\n",
    "                    raise Exception(\"Migration verification failed\")\n",
    "            else:\n",
    "                raise Exception(\"Migration execution failed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            migration_request.state = MigrationState.FAILED\n",
    "            await self.handle_migration_failure(migration_request, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Policy Enforcer\n",
    "import time\n",
    "from prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n",
    "\n",
    "class PolicyEnforcer:\n",
    "    def __init__(self, policies):\n",
    "        self.policies = policies  # List of policy dictionaries\n",
    "\n",
    "    def enforce_policies(self, migration_request):\n",
    "        for policy in self.policies:\n",
    "            context = policy['CONTEXT']\n",
    "            conditions = policy['CONDITIONS']\n",
    "            actions = policy['ACTIONS']\n",
    "            constraints = policy['CONSTRAINTS']\n",
    "            priority = policy['PRIORITY']\n",
    "            # Evaluate conditions based on context\n",
    "            condition_met = self.evaluate_conditions(conditions, migration_request)\n",
    "            if condition_met:\n",
    "                self.execute_actions(actions, migration_request)\n",
    "                if 'rollback' in actions:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def evaluate_conditions(self, conditions: str, migration_request: Any) -> bool:\n",
    "\n",
    "        env = self.get_evaluation_environment(migration_request)\n",
    "        try:\n",
    "            return eval(conditions, {}, env)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating conditions: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_evaluation_environment(self, migration_request: Any) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'source_cpu_utilization': self.get_cpu_utilization(migration_request.source_host),\n",
    "            'destination_cpu_utilization': self.get_cpu_utilization(migration_request.destination_host),\n",
    "            'time_of_day': int(time.strftime(\"%H\")),\n",
    "            'network_congestion_prob': self.get_network_congestion_probability(migration_request),\n",
    "            'service_type': self.get_service_type(migration_request.container_id)\n",
    "        }\n",
    "\n",
    "    def get_cpu_utilization(self, host: Any) -> float:\n",
    "        return (host.total_cpu - host.available_cpu) / host.total_cpu * 100\n",
    "\n",
    "    def get_network_congestion_probability(self, migration_request: Any) -> float:\n",
    "        source_host = migration_request.source_host\n",
    "        destination_host = migration_request.destination_host\n",
    "\n",
    "        # Get network metrics from the network manager\n",
    "        bandwidth_usage = self.network_manager.get_bandwidth_usage(source_host, destination_host)\n",
    "        packet_loss = self.network_manager.get_packet_loss(source_host, destination_host)\n",
    "        latency = self.network_manager.get_latency(source_host, destination_host)\n",
    "\n",
    "\n",
    "        congestion_prob = (bandwidth_usage / 100 + packet_loss + latency / 1000) / 3\n",
    "\n",
    "        # Ensure the probability is between 0 and 1\n",
    "        congestion_prob = max(0, min(1, congestion_prob))\n",
    "\n",
    "        # Update Prometheus metric\n",
    "        self.network_congestion_gauge.set(congestion_prob)\n",
    "        push_to_gateway(self.prometheus_gateway, job='network_congestion', registry=self.registry)\n",
    "\n",
    "        return congestion_prob\n",
    "\n",
    "    def execute_actions(self, actions, migration_request):\n",
    "        for action in actions:\n",
    "            \n",
    "            if action == 'allow_migration':\n",
    "                migration_request.state = MigrationState.PLANNING\n",
    "                continue  # No action needed; migration is allowed\n",
    "            elif action.startswith('set_priority'):\n",
    "                # Extract priority level\n",
    "                priority = action.split('(')[1].rstrip(')')\n",
    "                migration_request.priority = priority\n",
    "            elif action.startswith('trigger_load_balancer_reconfiguration'):\n",
    "                self.trigger_load_balancer_reconfiguration(migration_request)\n",
    "            elif action == 'rollback':\n",
    "                migration_request.state = MigrationState.ROLLBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoggingAndMonitoring\n",
    "import json\n",
    "import logging\n",
    "from elasticsearch import Elasticsearch\n",
    "from logstash import TCPLogstashHandler\n",
    "from prometheus_client import start_http_server, Gauge\n",
    "import time\n",
    "import requests\n",
    "\n",
    "class LoggingAndMonitoringModule:\n",
    "    def __init__(self, elasticsearch_host='localhost', elasticsearch_port=9200, \n",
    "                 logstash_host='localhost', logstash_port=5000,\n",
    "                 kibana_host='localhost', kibana_port=5601,\n",
    "                 grafana_host='localhost', grafana_port=3000,\n",
    "                 prometheus_port=9090):\n",
    "        # Initialize Elasticsearch client\n",
    "        self.es = Elasticsearch([f'http://{elasticsearch_host}:{elasticsearch_port}'])\n",
    "        \n",
    "        # Initialize Logger\n",
    "        self.logger = logging.getLogger('FlexiMigrate')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Add Logstash handler\n",
    "        logstash_handler = TCPLogstashHandler(logstash_host, logstash_port, version=1)\n",
    "        self.logger.addHandler(logstash_handler)\n",
    "        \n",
    "        # Initialize Prometheus metrics\n",
    "        self.init_prometheus_metrics(prometheus_port)\n",
    "        \n",
    "        # Store visualization tool URLs\n",
    "        self.kibana_url = f'http://{kibana_host}:{kibana_port}'\n",
    "        self.grafana_url = f'http://{grafana_host}:{grafana_port}'\n",
    "\n",
    "    def init_prometheus_metrics(self, prometheus_port):\n",
    "        # Start Prometheus HTTP server\n",
    "        start_http_server(prometheus_port)\n",
    "        \n",
    "        # Define Prometheus metrics\n",
    "        self.migration_count = Gauge('fleximigrate_migration_count', 'Number of migrations')\n",
    "        self.migration_duration = Gauge('fleximigrate_migration_duration', 'Duration of migrations')\n",
    "        self.resource_utilization = Gauge('fleximigrate_resource_utilization', 'Resource utilization', ['resource_type'])\n",
    "\n",
    "    def log_event(self, event_type, message, additional_data=None):\n",
    "        log_entry = {\n",
    "            'timestamp': time.time(),\n",
    "            'event_type': event_type,\n",
    "            'message': message,\n",
    "            'additional_data': additional_data or {}\n",
    "        }\n",
    "        \n",
    "        # Log to Elasticsearch\n",
    "        self.es.index(index='fleximigrate-logs', body=log_entry)\n",
    "        \n",
    "        # Log using Python logger (which will send to Logstash)\n",
    "        self.logger.info(json.dumps(log_entry))\n",
    "\n",
    "    def update_prometheus_metrics(self, metric_name, value, labels=None):\n",
    "        if metric_name == 'migration_count':\n",
    "            self.migration_count.inc(value)\n",
    "        elif metric_name == 'migration_duration':\n",
    "            self.migration_duration.set(value)\n",
    "        elif metric_name == 'resource_utilization':\n",
    "            self.resource_utilization.labels(resource_type=labels['resource_type']).set(value)\n",
    "\n",
    "    def create_kibana_dashboard(self, dashboard_name):\n",
    "        dashboard_url = f'{self.kibana_url}/app/kibana#/dashboard/{dashboard_name}'\n",
    "        print(f\"Kibana dashboard created: {dashboard_url}\")\n",
    "\n",
    "    def create_grafana_dashboard(self, dashboard_name):\n",
    "        dashboard_url = f'{self.grafana_url}/d/{dashboard_name}'\n",
    "        print(f\"Grafana dashboard created: {dashboard_url}\")\n",
    "\n",
    "    def analyze_logs(self, query):\n",
    "        # Perform a search query on Elasticsearch\n",
    "        results = self.es.search(index='fleximigrate-logs', body=query)\n",
    "        return results['hits']['hits']\n",
    "\n",
    "    def get_metric_data(self, metric_name, time_range):\n",
    "        query = f'{metric_name}[{time_range}]'\n",
    "        response = requests.get(f'http://localhost:9090/api/v1/query', params={'query': query})\n",
    "        return response.json()['data']['result']\n",
    "\n",
    "\n",
    "    def generate_report(self, start_time, end_time):\n",
    "        log_data = self.analyze_logs({\n",
    "            \"query\": {\n",
    "                \"range\": {\n",
    "                    \"timestamp\": {\n",
    "                        \"gte\": start_time,\n",
    "                        \"lte\": end_time\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        metric_data = self.get_metric_data('fleximigrate_migration_count', f'{end_time - start_time}s')\n",
    "        \n",
    "        report = {\n",
    "            'log_summary': log_data,\n",
    "            'metric_summary': metric_data\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Usage example:\n",
    "monitoring = LoggingAndMonitoringModule()\n",
    "\n",
    "# Log an event\n",
    "monitoring.log_event('migration_started', 'Container migration initiated', {'container_id': 'abc123'})\n",
    "\n",
    "# Update a Prometheus metric\n",
    "monitoring.update_prometheus_metrics('migration_count', 1)\n",
    "\n",
    "# Create dashboards\n",
    "monitoring.create_kibana_dashboard('fleximigrate-overview')\n",
    "monitoring.create_grafana_dashboard('fleximigrate-metrics')\n",
    "\n",
    "# Analyze logs\n",
    "log_analysis = monitoring.analyze_logs({\"query\": {\"match\": {\"event_type\": \"migration_started\"}}})\n",
    "\n",
    "# Get metric data\n",
    "metric_data = monitoring.get_metric_data('fleximigrate_migration_count', '1h')\n",
    "\n",
    "# Set up an alert\n",
    "monitoring.alert_on_condition(monitoring.migration_count.get() > 100, \"High number of migrations detected\")\n",
    "\n",
    "# Generate a report\n",
    "report = monitoring.generate_report(time.time() - 3600, time.time())  # Last hour\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Manager\n",
    "class StateSynchronizer:\n",
    "    def __init__(self, checkpointing_module, delta_tracker, state_restoration_module, network_manager, resource_monitor):\n",
    "        self.checkpointing_module = checkpointing_module\n",
    "        self.delta_tracker = delta_tracker\n",
    "        self.state_restoration_module = state_restoration_module\n",
    "        self.network_manager = network_manager\n",
    "        self.resource_monitor = resource_monitor\n",
    "\n",
    "    async def perform_migration(self, container, source_host, destination_host, strategy):\n",
    "        try:\n",
    "            # Step 1: Create an initial checkpoint\n",
    "            checkpoint_id = await self.checkpointing_module.create_checkpoint(container, checkpoint_type='full')\n",
    "            print(f\"Created initial checkpoint: {checkpoint_id}\")\n",
    "\n",
    "            # Step 2: Transfer the initial checkpoint to the destination\n",
    "            optimized_checkpoint_id = await self.checkpointing_module.optimize_checkpoint(checkpoint_id)\n",
    "            await self.transfer_checkpoint(optimized_checkpoint_id, source_host, destination_host)\n",
    "\n",
    "            # Step 3: Start delta tracking\n",
    "            await self.delta_tracker.initialize_delta_tracking(container.container_id)\n",
    "\n",
    "            # Step 4: Capture and transfer deltas iteratively\n",
    "            while not await self._migration_is_ready(container, source_host, destination_host):\n",
    "                deltas = await self.delta_tracker.get_deltas_since_checkpoint(container.container_id, checkpoint_id)\n",
    "                if deltas:\n",
    "                    await self.transfer_deltas(deltas, source_host, destination_host)\n",
    "                await asyncio.sleep(1)\n",
    "\n",
    "            # Step 5: Final checkpoint and transfer\n",
    "            final_checkpoint_id = await self.checkpointing_module.create_checkpoint(container, checkpoint_type='incremental')\n",
    "            optimized_final_checkpoint_id = await self.checkpointing_module.optimize_checkpoint(final_checkpoint_id)\n",
    "            await self.transfer_checkpoint(optimized_final_checkpoint_id, source_host, destination_host)\n",
    "\n",
    "            # Step 6: Restore the container at the destination\n",
    "            success = await self.state_restoration_module.restore_state(container, final_checkpoint_id, destination_host)\n",
    "            if success:\n",
    "                print(f\"Container {container.container_id} restored successfully on {destination_host.host_id}.\")\n",
    "                container.state = MigrationState.COMPLETED\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Failed to restore container {container.container_id} on {destination_host.host_id}.\")\n",
    "                container.state = MigrationState.FAILED\n",
    "                return False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Migration failed: {e}\")\n",
    "            container.state = MigrationState.FAILED\n",
    "            return False\n",
    "\n",
    "    def _migration_is_ready(self, container: Any, source_host: Any, destination_host: Any) -> bool:\n",
    "        if container.state != MigrationState.PREPARATION:\n",
    "            return False\n",
    "        \n",
    "        \"\"\"\n",
    "        Determines if the migration is ready to proceed based on various factors.\n",
    "        \n",
    "        Args:\n",
    "            container: The container being migrated.\n",
    "            source_host: The source host of the migration.\n",
    "            destination_host: The destination host of the migration.\n",
    "        \n",
    "        Returns:\n",
    "            True if the migration is ready to proceed, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check network conditions\n",
    "            network_metrics = self.network_manager.get_network_metrics(source_host.host_id, destination_host.host_id)\n",
    "            bandwidth_utilization = network_metrics['current_bandwidth'] / network_metrics['max_bandwidth']\n",
    "            latency = network_metrics['latency']\n",
    "\n",
    "            # Check resource availability on destination host\n",
    "            dest_resources = self.resource_monitor.get_host_resources(destination_host.host_id)\n",
    "            cpu_availability = dest_resources['available_cpu'] / dest_resources['total_cpu']\n",
    "            memory_availability = dest_resources['available_memory'] / dest_resources['total_memory']\n",
    "\n",
    "            # Check container state\n",
    "            container_state = self.checkpointing_module.get_container_state(container.container_id)\n",
    "            delta_size = self.delta_tracker.get_total_delta_size(container.container_id)\n",
    "            \n",
    "            # Calculate migration readiness score\n",
    "            readiness_score = self._calculate_readiness_score(\n",
    "                bandwidth_utilization,\n",
    "                latency,\n",
    "                cpu_availability,\n",
    "                memory_availability,\n",
    "                container_state,\n",
    "                delta_size\n",
    "            )\n",
    "\n",
    "            print(f\"Migration readiness score: {readiness_score}\")\n",
    "\n",
    "            return readiness_score >= self.migration_readiness_threshold\n",
    "        \n",
    "        \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in migration readiness check: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _calculate_readiness_score(self, \n",
    "                                   bandwidth_utilization: float, \n",
    "                                   latency: float, \n",
    "                                   cpu_availability: float, \n",
    "                                   memory_availability: float, \n",
    "                                   container_state: str, \n",
    "                                   delta_size: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculates a readiness score based on various metrics.\n",
    "        \n",
    "        Args:\n",
    "            bandwidth_utilization: Current bandwidth utilization (0-1).\n",
    "            latency: Network latency in milliseconds.\n",
    "            cpu_availability: Available CPU on destination host (0-1).\n",
    "            memory_availability: Available memory on destination host (0-1).\n",
    "            container_state: Current state of the container.\n",
    "            delta_size: Size of accumulated deltas since last checkpoint.\n",
    "        \n",
    "        Returns:\n",
    "            A float representing the readiness score (0-1).\n",
    "        \"\"\"\n",
    "        # Define weights for each factor\n",
    "        weights = {\n",
    "            'bandwidth': 0.25,\n",
    "            'latency': 0.2,\n",
    "            'cpu': 0.15,\n",
    "            'memory': 0.15,\n",
    "            'container_state': 0.15,\n",
    "            'delta_size': 0.1\n",
    "        }\n",
    "\n",
    "        # Normalize inputs\n",
    "        normalized_bandwidth = 1 - bandwidth_utilization  # Higher is better\n",
    "        normalized_latency = 1 / (1 + latency / 100)  # Transform to 0-1 range, lower latency is better\n",
    "        normalized_delta_size = 1 / (1 + delta_size / 1e6)  # Transform to 0-1 range, smaller delta is better\n",
    "\n",
    "        # Score container state\n",
    "        state_scores = {'running': 1.0, 'paused': 0.8, 'restarting': 0.5, 'exited': 0.2}\n",
    "        container_state_score = state_scores.get(container_state, 0)\n",
    "\n",
    "        # Calculate weighted score\n",
    "        score = (\n",
    "            weights['bandwidth'] * normalized_bandwidth +\n",
    "            weights['latency'] * normalized_latency +\n",
    "            weights['cpu'] * cpu_availability +\n",
    "            weights['memory'] * memory_availability +\n",
    "            weights['container_state'] * container_state_score +\n",
    "            weights['delta_size'] * normalized_delta_size\n",
    "        )\n",
    "\n",
    "        return score\n",
    "\n",
    "    async def transfer_checkpoint(self, checkpoint_id: str, source_host: Any, destination_host: Any):\n",
    "        \"\"\"\n",
    "        Transfers a checkpoint from the source host to the destination host asynchronously.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_id: The ID of the checkpoint to transfer.\n",
    "            source_host: The source host object.\n",
    "            destination_host: The destination host object.\n",
    "        \n",
    "        Raises:\n",
    "            Exception: If there's an error during the transfer process.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            checkpoint_path = os.path.join(source_host.checkpoint_dir, checkpoint_id)\n",
    "            destination_path = os.path.join(destination_host.checkpoint_dir, checkpoint_id)\n",
    "\n",
    "            # Ensure the destination directory exists\n",
    "            os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "\n",
    "            # Calculate the total size of the checkpoint\n",
    "            total_size = os.path.getsize(checkpoint_path)\n",
    "\n",
    "            # Create a progress bar\n",
    "            progress_bar = tqdm(total=total_size, unit='B', unit_scale=True, desc=f\"Transferring checkpoint {checkpoint_id}\")\n",
    "\n",
    "            # Use aiofiles for asynchronous file I/O\n",
    "            async with aiofiles.open(checkpoint_path, 'rb') as source_file:\n",
    "                async with aiofiles.open(destination_path, 'wb') as dest_file:\n",
    "                    while True:\n",
    "                        chunk = await source_file.read(8192)  # 8KB chunks\n",
    "                        if not chunk:\n",
    "                            break\n",
    "                        await dest_file.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "\n",
    "            progress_bar.close()\n",
    "\n",
    "            # Verify the transfer\n",
    "            if not await self._verify_transfer(checkpoint_path, destination_path):\n",
    "                raise Exception(\"Checkpoint transfer verification failed\")\n",
    "\n",
    "            print(f\"Checkpoint {checkpoint_id} transferred successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error transferring checkpoint {checkpoint_id}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def transfer_deltas(self, deltas: List[Any], source_host: Any, destination_host: Any):\n",
    "        \"\"\"\n",
    "        Transfers delta changes from the source host to the destination host asynchronously.\n",
    "        \n",
    "        Args:\n",
    "            deltas: List of delta objects to transfer.\n",
    "            source_host: The source host object.\n",
    "            destination_host: The destination host object.\n",
    "        \n",
    "        Raises:\n",
    "            Exception: If there's an error during the transfer process.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            total_delta_size = sum(delta.size for delta in deltas)\n",
    "            progress_bar = tqdm(total=total_delta_size, unit='B', unit_scale=True, desc=\"Transferring deltas\")\n",
    "\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                for delta in deltas:\n",
    "                    delta_path = os.path.join(source_host.delta_dir, delta.id)\n",
    "                    destination_url = f\"http://{destination_host.address}:{destination_host.port}/receive_delta\"\n",
    "\n",
    "                    async with aiofiles.open(delta_path, 'rb') as delta_file:\n",
    "                        delta_data = await delta_file.read()\n",
    "\n",
    "                    async with session.post(destination_url, data=delta_data) as response:\n",
    "                        if response.status != 200:\n",
    "                            raise Exception(f\"Failed to transfer delta {delta.id}: {await response.text()}\")\n",
    "\n",
    "                    progress_bar.update(delta.size)\n",
    "\n",
    "            progress_bar.close()\n",
    "\n",
    "            print(f\"All deltas transferred successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error transferring deltas: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def _verify_transfer(self, source_path: str, destination_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Verifies the integrity of the transferred file using SHA256 hash.\n",
    "        \n",
    "        Args:\n",
    "            source_path: Path to the source file.\n",
    "            destination_path: Path to the destination file.\n",
    "        \n",
    "        Returns:\n",
    "            True if the transfer is verified, False otherwise.\n",
    "        \"\"\"\n",
    "        async def calculate_hash(file_path):\n",
    "            hash_sha256 = hashlib.sha256()\n",
    "            async with aiofiles.open(file_path, 'rb') as f:\n",
    "                while chunk := await f.read(8192):\n",
    "                    hash_sha256.update(chunk)\n",
    "            return hash_sha256.hexdigest()\n",
    "\n",
    "        source_hash = await calculate_hash(source_path)\n",
    "        destination_hash = await calculate_hash(destination_path)\n",
    "\n",
    "        return source_hash == destination_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MigrationStrategySelector\n",
    "\n",
    "class MigrationStrategySelector:\n",
    "    def __init__(self, performance_metrics_collector, network_manager):\n",
    "        self.performance_metrics_collector = performance_metrics_collector\n",
    "        self.network_manager = network_manager\n",
    "        self.strategy_weights = {\n",
    "            'memory_usage': 0.3,\n",
    "            'cpu_usage': 0.2,\n",
    "            'network_bandwidth': 0.2,\n",
    "            'disk_io': 0.1,\n",
    "            'container_size': 0.1,\n",
    "            'downtime_tolerance': 0.1\n",
    "        }\n",
    "\n",
    "    def select_strategy(self, container: Any, source_host: Any, destination_host: Any) -> MigrationStrategy:\n",
    "        \"\"\"\n",
    "        Selects the most appropriate migration strategy based on container and environment characteristics.\n",
    "\n",
    "        Args:\n",
    "            container: The container object to be migrated.\n",
    "            source_host: The source host object.\n",
    "            destination_host: The destination host object.\n",
    "\n",
    "        Returns:\n",
    "            MigrationStrategy: The selected migration strategy.\n",
    "        \"\"\"\n",
    "        # Collect relevant metrics\n",
    "        container_metrics = self.performance_metrics_collector.get_container_metrics(container.id)\n",
    "        network_metrics = self.network_manager.get_network_metrics(source_host.id, destination_host.id)\n",
    "        \n",
    "        # Calculate scores for each strategy\n",
    "        cold_score = self._calculate_cold_migration_score(container_metrics, network_metrics)\n",
    "        pre_copy_score = self._calculate_pre_copy_score(container_metrics, network_metrics)\n",
    "        post_copy_score = self._calculate_post_copy_score(container_metrics, network_metrics)\n",
    "        live_score = self._calculate_hybrid_score(container_metrics, network_metrics)\n",
    "\n",
    "        # Select the strategy with the highest score\n",
    "        scores = {\n",
    "            MigrationStrategy.COLD_MIGRATION: cold_score,\n",
    "            MigrationStrategy.PRE_COPY: pre_copy_score,\n",
    "            MigrationStrategy.POST_COPY: post_copy_score,\n",
    "            MigrationStrategy.LIVE_MIGRATION: live_score\n",
    "        }\n",
    "\n",
    "        selected_strategy = max(scores, key=scores.get)\n",
    "        \n",
    "        self._log_strategy_selection(container.id, scores, selected_strategy)\n",
    "        \n",
    "        return selected_strategy\n",
    "\n",
    "    def _calculate_cold_migration_score(self, container_metrics: Dict[str, float], network_metrics: Dict[str, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the score for cold migration strategy.\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        score += (1 - container_metrics['memory_usage']) * self.strategy_weights['memory_usage']\n",
    "        score += (1 - container_metrics['cpu_usage']) * self.strategy_weights['cpu_usage']\n",
    "        score += network_metrics['bandwidth'] / 1000 * self.strategy_weights['network_bandwidth']  # Assuming bandwidth is in Mbps\n",
    "        score += (1 - container_metrics['disk_io']) * self.strategy_weights['disk_io']\n",
    "        score += (1 - container_metrics['container_size'] / 10000) * self.strategy_weights['container_size']  # Assuming size is in MB\n",
    "        score += container_metrics['downtime_tolerance'] * self.strategy_weights['downtime_tolerance']\n",
    "        return score\n",
    "\n",
    "    def _calculate_pre_copy_score(self, container_metrics: Dict[str, float], network_metrics: Dict[str, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the score for pre-copy migration strategy.\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        score += container_metrics['memory_usage'] * self.strategy_weights['memory_usage']\n",
    "        score += (1 - container_metrics['cpu_usage']) * self.strategy_weights['cpu_usage']\n",
    "        score += network_metrics['bandwidth'] / 1000 * self.strategy_weights['network_bandwidth']\n",
    "        score += (1 - container_metrics['disk_io']) * self.strategy_weights['disk_io']\n",
    "        score += (1 - container_metrics['container_size'] / 10000) * self.strategy_weights['container_size']\n",
    "        score += (1 - container_metrics['downtime_tolerance']) * self.strategy_weights['downtime_tolerance']\n",
    "        return score\n",
    "\n",
    "    def _calculate_post_copy_score(self, container_metrics: Dict[str, float], network_metrics: Dict[str, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the score for post-copy migration strategy.\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        score += (1 - container_metrics['memory_usage']) * self.strategy_weights['memory_usage']\n",
    "        score += container_metrics['cpu_usage'] * self.strategy_weights['cpu_usage']\n",
    "        score += network_metrics['bandwidth'] / 1000 * self.strategy_weights['network_bandwidth']\n",
    "        score += container_metrics['disk_io'] * self.strategy_weights['disk_io']\n",
    "        score += container_metrics['container_size'] / 10000 * self.strategy_weights['container_size']\n",
    "        score += (1 - container_metrics['downtime_tolerance']) * self.strategy_weights['downtime_tolerance']\n",
    "        return score\n",
    "\n",
    "    def _calculate_hybrid_score(self, container_metrics: Dict[str, float], network_metrics: Dict[str, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the score for Live migration strategy.\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        score += container_metrics['memory_usage'] * self.strategy_weights['memory_usage']\n",
    "        score += container_metrics['cpu_usage'] * self.strategy_weights['cpu_usage']\n",
    "        score += network_metrics['bandwidth'] / 1000 * self.strategy_weights['network_bandwidth']\n",
    "        score += container_metrics['disk_io'] * self.strategy_weights['disk_io']\n",
    "        score += container_metrics['container_size'] / 10000 * self.strategy_weights['container_size']\n",
    "        score += (1 - container_metrics['downtime_tolerance']) * self.strategy_weights['downtime_tolerance']\n",
    "        return score\n",
    "\n",
    "    def _log_strategy_selection(self, container_id: str, scores: Dict[MigrationStrategy, float], selected_strategy: MigrationStrategy):\n",
    "        \"\"\"\n",
    "        Logs the strategy selection process for analysis and debugging.\n",
    "        \"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': time.time(),\n",
    "            'container_id': container_id,\n",
    "            'scores': {str(strategy): score for strategy, score in scores.items()},\n",
    "            'selected_strategy': str(selected_strategy)\n",
    "        }\n",
    "      \n",
    "        print(f\"Strategy Selection Log: {log_entry}\")\n",
    "\n",
    "    def update_strategy_weights(self, new_weights: Dict[str, float]):\n",
    "        \"\"\"\n",
    "        Updates the weights used for strategy selection.\n",
    "        \"\"\"\n",
    "        if sum(new_weights.values()) != 1.0:\n",
    "            raise ValueError(\"The sum of weights must be 1.0\")\n",
    "        self.strategy_weights.update(new_weights)\n",
    "\n",
    "    def analyze_migration_performance(self, migration_history: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Analyzes past migration performance to potentially adjust strategy selection.\n",
    "        \"\"\"\n",
    "        strategy_performance = {strategy: [] for strategy in MigrationStrategy}\n",
    "        \n",
    "        for migration in migration_history:\n",
    "            strategy = migration['strategy']\n",
    "            performance = migration['performance_score']\n",
    "            strategy_performance[strategy].append(performance)\n",
    "        \n",
    "        for strategy, performances in strategy_performance.items():\n",
    "            if performances:\n",
    "                avg_performance = np.mean(performances)\n",
    "                print(f\"Average performance for {strategy}: {avg_performance}\")\n",
    "                \n",
    "                # Adjust weights based on performance\n",
    "                if avg_performance < 0.5:  # Assuming performance score is between 0 and 1\n",
    "                    self._decrease_strategy_weight(strategy)\n",
    "                elif avg_performance > 0.8:\n",
    "                    self._increase_strategy_weight(strategy)\n",
    "\n",
    "    def _decrease_strategy_weight(self, strategy: MigrationStrategy):\n",
    "        \"\"\"\n",
    "        Decreases the weight of factors favoring the given strategy.\n",
    "        \"\"\"\n",
    "        if strategy == MigrationStrategy.COLD_MIGRATION:\n",
    "            self.strategy_weights['downtime_tolerance'] *= 0.9\n",
    "        elif strategy == MigrationStrategy.PRE_COPY:\n",
    "            self.strategy_weights['memory_usage'] *= 0.9\n",
    "        elif strategy == MigrationStrategy.POST_COPY:\n",
    "            self.strategy_weights['cpu_usage'] *= 0.9\n",
    "        elif strategy == MigrationStrategy.LIVE_MIGRATION:\n",
    "            self.strategy_weights['network_bandwidth'] *= 0.9\n",
    "        \n",
    "        self._normalize_weights()\n",
    "\n",
    "    def _increase_strategy_weight(self, strategy: MigrationStrategy):\n",
    "        \"\"\"\n",
    "        Increases the weight of factors favoring the given strategy.\n",
    "        \"\"\"\n",
    "        if strategy == MigrationStrategy.COLD_MIGRATION:\n",
    "            self.strategy_weights['downtime_tolerance'] *= 1.1\n",
    "        elif strategy == MigrationStrategy.PRE_COPY:\n",
    "            self.strategy_weights['memory_usage'] *= 1.1\n",
    "        elif strategy == MigrationStrategy.POST_COPY:\n",
    "            self.strategy_weights['cpu_usage'] *= 1.1\n",
    "        elif strategy == MigrationStrategy.LIVE_MIGRATION:\n",
    "            self.strategy_weights['network_bandwidth'] *= 1.1\n",
    "        \n",
    "        self._normalize_weights()\n",
    "\n",
    "    def _normalize_weights(self):\n",
    "        \"\"\"\n",
    "        Normalizes the strategy weights to ensure they sum to 1.\n",
    "        \"\"\"\n",
    "        total_weight = sum(self.strategy_weights.values())\n",
    "        self.strategy_weights = {k: v / total_weight for k, v in self.strategy_weights.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetworkOrchestrator\n",
    "\n",
    "import asyncio\n",
    "import ipaddress\n",
    "from typing import Dict, Any\n",
    "import docker\n",
    "from os_ken.app.simple_switch_13 import SimpleSwitch13\n",
    "from os_ken.controller import ofp_event\n",
    "from os_ken.controller.handler import MAIN_DISPATCHER, set_ev_cls\n",
    "from os_ken.ofproto import ofproto_v1_3\n",
    "from os_ken.lib.packet import packet, ethernet, arp, ipv4\n",
    "\n",
    "class NetworkOrchestrator(SimpleSwitch13):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NetworkOrchestrator, self).__init__(*args, **kwargs)\n",
    "        self.migration_coordinator = kwargs.get('migration_coordinator')\n",
    "        self.active_migrations = {}\n",
    "        self.docker_client = docker.from_env()\n",
    "        config.load_kube_config()\n",
    "        self.network_policies = {}\n",
    "\n",
    "    async def handle_network_changes(self, migration_request: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Handles network changes during container migration.\n",
    "\n",
    "        Args:\n",
    "            migration_request: A dictionary containing migration details.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if network changes were handled successfully, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            container_id = migration_request['container_id']\n",
    "            source_host = migration_request['source_host']\n",
    "            destination_host = migration_request['destination_host']\n",
    "\n",
    "            # Step 1: Prepare the network on the destination host\n",
    "            await self._prepare_destination_network(container_id, destination_host)\n",
    "\n",
    "            # Step 2: Set up tunneling between source and destination hosts\n",
    "            tunnel_id = await self._setup_tunnel(source_host, destination_host)\n",
    "\n",
    "            # Step 3: Update SDN flow rules for traffic redirection\n",
    "            await self._update_sdn_flow_rules(container_id, source_host, destination_host)\n",
    "\n",
    "            # Step 4: Update DNS records\n",
    "            await self._update_dns_records(container_id, destination_host)\n",
    "\n",
    "            # Step 5: Apply network policies on the destination host\n",
    "            await self._apply_network_policies(container_id, destination_host)\n",
    "\n",
    "            # Step 6: Handle service mesh configuration (if applicable)\n",
    "            await self._update_service_mesh_config(container_id, source_host, destination_host)\n",
    "\n",
    "            # Store migration details for cleanup\n",
    "            self.active_migrations[container_id] = {\n",
    "                'source_host': source_host,\n",
    "                'destination_host': destination_host,\n",
    "                'tunnel_id': tunnel_id\n",
    "            }\n",
    "\n",
    "            self.logger.info(f\"Network changes handled successfully for container {container_id}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error handling network changes for container {container_id}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    async def _prepare_destination_network(self, container_id: str, destination_host: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Prepares the network on the destination host for the migrating container.\n",
    "        \"\"\"\n",
    "        container = self.docker_client.containers.get(container_id)\n",
    "        network_settings = container.attrs['NetworkSettings']\n",
    "\n",
    "        for network_name, network_config in network_settings['Networks'].items():\n",
    "            # Create the network on the destination host if it doesn't exist\n",
    "            try:\n",
    "                self.docker_client.networks.get(network_name)\n",
    "            except docker.errors.NotFound:\n",
    "                self.docker_client.networks.create(\n",
    "                    name=network_name,\n",
    "                    driver=network_config.get('Driver', 'bridge'),\n",
    "                    ipam=network_config.get('IPAM', None)\n",
    "                )\n",
    "\n",
    "            # Reserve the same IP address for the container on the destination host\n",
    "            if 'IPAddress' in network_config:\n",
    "                await self._reserve_ip_address(network_name, network_config['IPAddress'], destination_host)\n",
    "\n",
    "    async def _setup_tunnel(self, source_host: Dict[str, Any], destination_host: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Sets up a network tunnel between source and destination hosts.\n",
    "        \"\"\"\n",
    "        tunnel_id = f\"mig_tunnel_{source_host['id']}_{destination_host['id']}\"\n",
    "        \n",
    "        # Set up VXLAN tunnel\n",
    "        cmd = f\"ip link add {tunnel_id} type vxlan id 100 remote {destination_host['ip']} dstport 4789 dev {source_host['interface']}\"\n",
    "        await self._run_ssh_command(source_host, cmd)\n",
    "        \n",
    "        cmd = f\"ip link set {tunnel_id} up\"\n",
    "        await self._run_ssh_command(source_host, cmd)\n",
    "        \n",
    "        # Set up the receiving end on the destination host\n",
    "        cmd = f\"ip link add {tunnel_id} type vxlan id 100 remote {source_host['ip']} dstport 4789 dev {destination_host['interface']}\"\n",
    "        await self._run_ssh_command(destination_host, cmd)\n",
    "        \n",
    "        cmd = f\"ip link set {tunnel_id} up\"\n",
    "        await self._run_ssh_command(destination_host, cmd)\n",
    "\n",
    "        return tunnel_id\n",
    "\n",
    "    async def _update_sdn_flow_rules(self, container_id: str, source_host: Dict[str, Any], destination_host: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Updates SDN flow rules to redirect traffic for the migrating container.\n",
    "        \"\"\"\n",
    "        container = self.docker_client.containers.get(container_id)\n",
    "        container_ip = container.attrs['NetworkSettings']['IPAddress']\n",
    "        \n",
    "        # Add flow rules to redirect traffic to the tunnel\n",
    "        for switch in self.switches:\n",
    "            datapath = switch.dp\n",
    "            ofproto = datapath.ofproto\n",
    "            parser = datapath.ofproto_parser\n",
    "\n",
    "            # Redirect incoming traffic to the container via the tunnel\n",
    "            match = parser.OFPMatch(eth_type=0x0800, ipv4_dst=container_ip)\n",
    "            actions = [parser.OFPActionSetField(ipv4_dst=destination_host['ip']),\n",
    "                       parser.OFPActionOutput(self._get_tunnel_port(datapath, destination_host))]\n",
    "            self.add_flow(datapath, 10, match, actions)\n",
    "\n",
    "            # Redirect outgoing traffic from the container via the tunnel\n",
    "            match = parser.OFPMatch(eth_type=0x0800, ipv4_src=container_ip)\n",
    "            actions = [parser.OFPActionSetField(ipv4_src=source_host['ip']),\n",
    "                       parser.OFPActionOutput(self._get_tunnel_port(datapath, source_host))]\n",
    "            self.add_flow(datapath, 10, match, actions)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    async def _reserve_ip_address(self, network_name: str, ip_address: str, host: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Reserves an IP address in the specified network on the given host.\n",
    "        \"\"\"\n",
    "        cmd = f\"docker network connect --ip {ip_address} {network_name} {host['id']}\"\n",
    "        await self._run_ssh_command(host, cmd)\n",
    "\n",
    "    async def _run_ssh_command(self, host: Dict[str, Any], command: str):\n",
    "        \"\"\"\n",
    "        Runs a command on a remote host via SSH.\n",
    "        \"\"\"\n",
    "        ssh_command = f\"ssh {host['user']}@{host['ip']} '{command}'\"\n",
    "        process = await asyncio.create_subprocess_shell(\n",
    "            ssh_command,\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE\n",
    "        )\n",
    "        stdout, stderr = await process.communicate()\n",
    "        if process.returncode != 0:\n",
    "            raise Exception(f\"SSH command failed: {stderr.decode()}\")\n",
    "        return stdout.decode()\n",
    "\n",
    "    def _get_tunnel_port(self, datapath, host):\n",
    "        \"\"\"\n",
    "        Gets the port number for the tunnel interface on the given datapath.\n",
    "        \"\"\"\n",
    "        # This is a placeholder. In a real implementation, you'd need to maintain\n",
    "        # a mapping of tunnel interfaces to switch ports.\n",
    "        return 1  # Assuming port 1 is always the tunnel port for simplicity\n",
    "\n",
    "    def _policy_applies_to_container(self, policy, container_labels):\n",
    "        \"\"\"\n",
    "        Checks if a network policy applies to a container based on its labels.\n",
    "        \"\"\"\n",
    "        selector = policy.spec.pod_selector.match_labels\n",
    "        return all(container_labels.get(k) == v for k, v in selector.items())\n",
    "\n",
    "    @set_ev_cls(ofp_event.EventOFPPacketIn, MAIN_DISPATCHER)\n",
    "    def _packet_in_handler(self, ev):\n",
    "        msg = ev.msg\n",
    "        datapath = msg.datapath\n",
    "        ofproto = datapath.ofproto\n",
    "        parser = datapath.ofproto_parser\n",
    "        in_port = msg.match['in_port']\n",
    "\n",
    "        pkt = packet.Packet(msg.data)\n",
    "        eth = pkt.get_protocols(ethernet.ethernet)[0]\n",
    "\n",
    "        if eth.ethertype == ether_types.ETH_TYPE_LLDP:\n",
    "            # Ignore LLDP packets\n",
    "            return\n",
    "\n",
    "        dst = eth.dst\n",
    "        src = eth.src\n",
    "\n",
    "        dpid = datapath.id\n",
    "        self.mac_to_port.setdefault(dpid, {})\n",
    "\n",
    "        # Learn a mac address to avoid FLOOD next time.\n",
    "        self.mac_to_port[dpid][src] = in_port\n",
    "\n",
    "        if dst in self.mac_to_port[dpid]:\n",
    "            out_port = self.mac_to_port[dpid][dst]\n",
    "        else:\n",
    "            out_port = ofproto.OFPP_FLOOD\n",
    "\n",
    "        actions = [parser.OFPActionOutput(out_port)]\n",
    "\n",
    "        # Install a flow to avoid packet_in next time\n",
    "        if out_port != ofproto.OFPP_FLOOD:\n",
    "            match = parser.OFPMatch(in_port=in_port, eth_dst=dst, eth_src=src)\n",
    "            # Verify if we have a valid buffer_id, if yes avoid to send both\n",
    "            # flow_mod & packet_out\n",
    "            if msg.buffer_id != ofproto.OFP_NO_BUFFER:\n",
    "                self.add_flow(datapath, 1, match, actions, msg.buffer_id)\n",
    "                return\n",
    "            else:\n",
    "                self.add_flow(datapath, 1, match, actions)\n",
    "\n",
    "        data = None\n",
    "        if msg.buffer_id == ofproto.OFP_NO_BUFFER:\n",
    "            data = msg.data\n",
    "\n",
    "        out = parser.OFPPacketOut(datapath=datapath, buffer_id=msg.buffer_id,\n",
    "                                  in_port=in_port, actions=actions, data=data)\n",
    "        datapath.send_msg(out)\n",
    "\n",
    "    def add_flow(self, datapath, priority, match, actions, buffer_id=None):\n",
    "        ofproto = datapath.ofproto\n",
    "        parser = datapath.ofproto_parser\n",
    "\n",
    "        inst = [parser.OFPInstructionActions(ofproto.OFPIT_APPLY_ACTIONS,\n",
    "                                             actions)]\n",
    "        if buffer_id:\n",
    "            mod = parser.OFPFlowMod(datapath=datapath, buffer_id=buffer_id,\n",
    "                                    priority=priority, match=match,\n",
    "                                    instructions=inst)\n",
    "        else:\n",
    "            mod = parser.OFPFlowMod(datapath=datapath, priority=priority,\n",
    "                                    match=match, instructions=inst)\n",
    "        datapath.send_msg(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Monitor Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PerformanceMetricsCollector\n",
    "import time\n",
    "import psutil\n",
    "import docker\n",
    "from prometheus_client import start_http_server, Gauge, CollectorRegistry\n",
    "from typing import Dict, Any\n",
    "\n",
    "class PerformanceMetricsCollector:\n",
    "    def __init__(self):\n",
    "        self.registry = CollectorRegistry()\n",
    "        self.docker_client = docker.from_env()\n",
    "        self.metrics = self._initialize_metrics()\n",
    "        self.thresholds = self._set_thresholds()\n",
    "\n",
    "    def _initialize_metrics(self):\n",
    "        metrics = {}\n",
    "        metric_definitions = [\n",
    "            ('cpu_usage', 'CPU usage percentage'),\n",
    "            ('memory_usage', 'Memory usage percentage'),\n",
    "            ('disk_io', 'Disk I/O operations per second'),\n",
    "            ('network_throughput', 'Network throughput in MB/min'),\n",
    "            ('container_startup_time', 'Container startup time in seconds'),\n",
    "            ('response_time', 'Response time in milliseconds'),\n",
    "            ('error_rate', 'Error rate percentage'),\n",
    "            ('network_latency', 'Network latency in milliseconds'),\n",
    "            ('cpu_load_average', 'CPU load average', ['interval']),\n",
    "            ('memory_page_faults', 'Memory page faults per minute'),\n",
    "            ('network_packet_loss', 'Network packet loss percentage'),\n",
    "            ('disk_queue_length', 'Disk queue length'),\n",
    "            ('container_restart_count', 'Container restart count'),\n",
    "            ('network_connection_count', 'Network connection count'),\n",
    "            ('system_call_rate', 'System call rate per second')\n",
    "        ]\n",
    "        \n",
    "        for name, description, *labels in metric_definitions:\n",
    "            metrics[name] = Gauge(name, description, labels or None, registry=self.registry)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def _set_thresholds(self):\n",
    "        return {\n",
    "            'cpu_usage': 80,\n",
    "            'memory_usage': 70,\n",
    "            'disk_io': 100,\n",
    "            'network_throughput': 500,\n",
    "            'container_startup_time': 5,\n",
    "            'response_time': 200,\n",
    "            'error_rate': 1,\n",
    "            'network_latency': 100,\n",
    "            'cpu_load_average': 2.0,\n",
    "            'memory_page_faults': 50,\n",
    "            'network_packet_loss': 1,\n",
    "            'disk_queue_length': 5,\n",
    "            'container_restart_count': 3,\n",
    "            'network_connection_count': 100,\n",
    "            'system_call_rate': 1000\n",
    "        }\n",
    "\n",
    "    def update_host_metrics(self, host):\n",
    "        self.cpu_usage.set(psutil.cpu_percent())\n",
    "        self.memory_usage.set(psutil.virtual_memory().percent)\n",
    "        net_io = psutil.net_io_counters()\n",
    "        self.network_in.set(net_io.bytes_recv)\n",
    "        self.network_out.set(net_io.bytes_sent)\n",
    "    \n",
    "    def collect_host_metrics(self):\n",
    "        # CPU Usage\n",
    "        self.metrics['cpu_usage'].set(psutil.cpu_percent())\n",
    "\n",
    "        # Memory Usage\n",
    "        mem = psutil.virtual_memory()\n",
    "        self.metrics['memory_usage'].set(mem.percent)\n",
    "\n",
    "        # Disk I/O\n",
    "        disk_io = psutil.disk_io_counters()\n",
    "        self.metrics['disk_io'].set(disk_io.read_count + disk_io.write_count)\n",
    "\n",
    "        # Network Throughput\n",
    "        net_io = psutil.net_io_counters()\n",
    "        throughput = (net_io.bytes_sent + net_io.bytes_recv) / (1024 * 1024)  # Convert to MB\n",
    "        self.metrics['network_throughput'].set(throughput)\n",
    "\n",
    "        # CPU Load Average\n",
    "        load1, load5, load15 = psutil.getloadavg()\n",
    "        self.metrics['cpu_load_average'].labels('1min').set(load1)\n",
    "        self.metrics['cpu_load_average'].labels('5min').set(load5)\n",
    "        self.metrics['cpu_load_average'].labels('15min').set(load15)\n",
    "\n",
    "        # Memory Page Faults\n",
    "        self.metrics['memory_page_faults'].set(mem.pgfault)\n",
    "\n",
    "        # Disk Queue Length\n",
    "        disk_usage = psutil.disk_usage('/')\n",
    "        self.metrics['disk_queue_length'].set(disk_usage.used)\n",
    "\n",
    "        # Network Connection Count\n",
    "        connections = len(psutil.net_connections())\n",
    "        self.metrics['network_connection_count'].set(connections)\n",
    "\n",
    "        # System Call Rate \n",
    "        self.metrics['system_call_rate'].set(psutil.cpu_stats().syscalls)\n",
    "    \n",
    "    def collect_container_metrics(self, container: Any):\n",
    "        try:\n",
    "            stats = self.docker_client.containers.get(container.container_id).stats(stream=False)\n",
    "            \n",
    "            # CPU Usage\n",
    "            cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - stats['precpu_stats']['cpu_usage']['total_usage']\n",
    "            system_delta = stats['cpu_stats']['system_cpu_usage'] - stats['precpu_stats']['system_cpu_usage']\n",
    "            cpu_usage = (cpu_delta / system_delta) * psutil.cpu_count() * 100.0\n",
    "            self.metrics['cpu_usage'].set(cpu_usage)\n",
    "\n",
    "            # Memory Usage\n",
    "            memory_usage = stats['memory_stats']['usage'] / stats['memory_stats']['limit'] * 100.0\n",
    "            self.metrics['memory_usage'].set(memory_usage)\n",
    "\n",
    "            # Network Throughput\n",
    "            if 'networks' in stats:\n",
    "                network_stats = stats['networks']['eth0']\n",
    "                throughput = (network_stats['rx_bytes'] + network_stats['tx_bytes']) / (1024 * 1024)  # Convert to MB\n",
    "                self.metrics['network_throughput'].set(throughput)\n",
    "\n",
    "            # Container Restart Count\n",
    "            self.metrics['container_restart_count'].set(stats['restart_count'])\n",
    "\n",
    "            # Error Rate and Response Time would typically come from application-level metrics\n",
    "            self.metrics['error_rate'].set(0)\n",
    "            self.metrics['response_time'].set(0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting metrics for container {container.container_id}: {str(e)}\")\n",
    "\n",
    "    def update_container_metrics(self, container):\n",
    "        try:\n",
    "            # Fetch container stats\n",
    "            stats = self.docker_client.containers.get(container.container_id).stats(stream=False)\n",
    "            \n",
    "            # CPU usage calculation\n",
    "            cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - stats['precpu_stats']['cpu_usage']['total_usage']\n",
    "            system_delta = stats['cpu_stats']['system_cpu_usage'] - stats['precpu_stats']['system_cpu_usage']\n",
    "            num_cpus = len(stats['cpu_stats']['cpu_usage']['percpu_usage'])\n",
    "            cpu_usage = (cpu_delta / system_delta) * num_cpus * 100.0\n",
    "            self.container_cpu.labels(container_id=container.container_id).set(cpu_usage)\n",
    "\n",
    "            # Memory usage calculation\n",
    "            memory_usage = stats['memory_stats']['usage'] / stats['memory_stats']['limit'] * 100.0\n",
    "            self.container_memory.labels(container_id=container.container_id).set(memory_usage)\n",
    "\n",
    "            # Network usage calculation\n",
    "            if 'networks' in stats:\n",
    "                network_stats = stats['networks']['eth0']\n",
    "                self.container_network_in.labels(container_id=container.container_id).set(network_stats['rx_bytes'])\n",
    "                self.container_network_out.labels(container_id=container.container_id).set(network_stats['tx_bytes'])\n",
    "\n",
    "            # Disk I/O calculation\n",
    "            if 'blkio_stats' in stats:\n",
    "                io_service_bytes_recursive = stats['blkio_stats']['io_service_bytes_recursive']\n",
    "                read_io = sum(item['value'] for item in io_service_bytes_recursive if item['op'] == 'Read')\n",
    "                write_io = sum(item['value'] for item in io_service_bytes_recursive if item['op'] == 'Write')\n",
    "                self.container_disk_io.labels(container_id=container.container_id, operation='read').set(read_io)\n",
    "                self.container_disk_io.labels(container_id=container.container_id, operation='write').set(write_io)\n",
    "\n",
    "        except docker.errors.NotFound:\n",
    "            print(f\"Container {container.container_id} not found for metrics update.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating metrics for container {container.container_id}: {str(e)}\")\n",
    "\n",
    "    def check_thresholds(self):\n",
    "        violations = []\n",
    "        for metric, threshold in self.thresholds.items():\n",
    "            if metric in self.metrics:\n",
    "                value = self.metrics[metric]._value.get()\n",
    "                if value > threshold:\n",
    "                    violations.append(f\"{metric}: {value} (threshold: {threshold})\")\n",
    "        return violations\n",
    "    \n",
    "    def collect_metrics(self, containers, interval=10):\n",
    "        while True:\n",
    "            self.collect_host_metrics()\n",
    "            for container in containers:\n",
    "                self.collect_container_metrics(container)\n",
    "            \n",
    "            violations = self.check_thresholds()\n",
    "            if violations:\n",
    "                print(\"Threshold violations detected:\")\n",
    "                for violation in violations:\n",
    "                    print(violation)\n",
    "            \n",
    "            time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResourceUtilizationAnalyzer\n",
    "\n",
    "class ResourceUtilizationAnalyzer:\n",
    "    def __init__(self, thresholds):\n",
    "        self.thresholds = thresholds  \n",
    "\n",
    "    def analyze_host_utilization(self, host):\n",
    "        cpu_usage = 100 - (host.available_cpu / host.total_cpu) * 100\n",
    "        memory_usage = 100 - (host.available_memory / host.total_memory) * 100\n",
    "        over_utilized = cpu_usage > self.thresholds['cpu_threshold'] or memory_usage > self.thresholds['memory_threshold']\n",
    "        under_utilized = cpu_usage < (self.thresholds['cpu_threshold'] / 2) and memory_usage < (self.thresholds['memory_threshold'] / 2)\n",
    "        return over_utilized, under_utilized\n",
    "\n",
    "    def analyze_container_utilization(self, container):\n",
    "        \n",
    "        cpu_usage = container.cpu_limit  \n",
    "        memory_usage = container.memory_limit  \n",
    "        needs_migration = cpu_usage > self.thresholds['cpu_threshold'] or memory_usage > self.thresholds['memory_threshold']\n",
    "        return needs_migration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Engine Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionEngine:\n",
    "    def __init__(self, workload_analyzer, resource_optimizer, migration_planner, policies, performance_metrics_collector):\n",
    "        self.workload_analyzer = workload_analyzer\n",
    "        self.resource_optimizer = resource_optimizer\n",
    "        self.migration_planner = migration_planner\n",
    "        self.policy_enforcer = PolicyEnforcer(policies)\n",
    "        self.policies = policies\n",
    "        self.performance_metrics_collector = performance_metrics_collector\n",
    "\n",
    "    def get_current_resource_usage(self, container: Any, host: Any) -> Dict[str, float]:\n",
    "\n",
    "        try:\n",
    "            # Fetch container-specific metrics\n",
    "            container_cpu = self.performance_metrics_collector.container_cpu.labels(container_id=container.container_id)._value.get()\n",
    "            container_memory = self.performance_metrics_collector.container_memory.labels(container_id=container.container_id)._value.get()\n",
    "            container_network_in = self.performance_metrics_collector.container_network_in.labels(container_id=container.container_id)._value.get()\n",
    "            container_network_out = self.performance_metrics_collector.container_network_out.labels(container_id=container.container_id)._value.get()\n",
    "            container_disk_read = self.performance_metrics_collector.container_disk_io.labels(container_id=container.container_id, operation='read')._value.get()\n",
    "            container_disk_write = self.performance_metrics_collector.container_disk_io.labels(container_id=container.container_id, operation='write')._value.get()\n",
    "\n",
    "            # Fetch host-specific metrics\n",
    "            host_cpu = self.performance_metrics_collector.cpu_usage._value.get()\n",
    "            host_memory = self.performance_metrics_collector.memory_usage._value.get()\n",
    "            host_network_in = self.performance_metrics_collector.network_in._value.get()\n",
    "            host_network_out = self.performance_metrics_collector.network_out._value.get()\n",
    "\n",
    "            # Calculate container's resource usage as a percentage of host's total resources\n",
    "            cpu_usage_percent = (container_cpu / host.total_cpu) * 100\n",
    "            memory_usage_percent = (container_memory / host.total_memory) * 100\n",
    "\n",
    "            # Calculate network utilization (bytes per second)\n",
    "            current_time = time.time()\n",
    "            time_diff = current_time - self.last_network_check_time if hasattr(self, 'last_network_check_time') else 1\n",
    "            network_in_rate = (container_network_in - self.last_network_in) / time_diff if hasattr(self, 'last_network_in') else 0\n",
    "            network_out_rate = (container_network_out - self.last_network_out) / time_diff if hasattr(self, 'last_network_out') else 0\n",
    "\n",
    "            # Update last checked values for next calculation\n",
    "            self.last_network_check_time = current_time\n",
    "            self.last_network_in = container_network_in\n",
    "            self.last_network_out = container_network_out\n",
    "\n",
    "            # Calculate disk I/O rates (bytes per second)\n",
    "            disk_read_rate = (container_disk_read - self.last_disk_read) / time_diff if hasattr(self, 'last_disk_read') else 0\n",
    "            disk_write_rate = (container_disk_write - self.last_disk_write) / time_diff if hasattr(self, 'last_disk_write') else 0\n",
    "\n",
    "            # Update last checked values for next calculation\n",
    "            self.last_disk_read = container_disk_read\n",
    "            self.last_disk_write = container_disk_write\n",
    "\n",
    "            return {\n",
    "                'cpu_usage': cpu_usage_percent,\n",
    "                'memory_usage': memory_usage_percent,\n",
    "                'network_in': network_in_rate,\n",
    "                'network_out': network_out_rate,\n",
    "                'disk_read': disk_read_rate,\n",
    "                'disk_write': disk_write_rate,\n",
    "                'host_cpu_usage': host_cpu,\n",
    "                'host_memory_usage': host_memory,\n",
    "                'host_network_in': host_network_in,\n",
    "                'host_network_out': host_network_out,\n",
    "                'time_of_day': int(time.strftime(\"%H\")),\n",
    "                'day_of_week': int(time.strftime(\"%w\"))\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching resource usage for container {container.container_id}: {str(e)}\")\n",
    "            # Return default values in case of an error\n",
    "            return {\n",
    "                'cpu_usage': 0,\n",
    "                'memory_usage': 0,\n",
    "                'network_in': 0,\n",
    "                'network_out': 0,\n",
    "                'disk_read': 0,\n",
    "                'disk_write': 0,\n",
    "                'host_cpu_usage': 0,\n",
    "                'host_memory_usage': 0,\n",
    "                'host_network_in': 0,\n",
    "                'host_network_out': 0,\n",
    "                'time_of_day': int(time.strftime(\"%H\")),\n",
    "                'day_of_week': int(time.strftime(\"%w\"))\n",
    "            }\n",
    "\n",
    "    def predict_future_resource_usage(self, container, current_usage):\n",
    "        features = np.array([list(current_usage.values())])\n",
    "        predicted_usage = self.workload_analyzer.predict_resource_usage(features)[0]\n",
    "        \n",
    "        return {\n",
    "            'predicted_cpu_usage': predicted_usage,\n",
    "            **current_usage  # Include current usage for other resources\n",
    "        }\n",
    "\n",
    "    def should_migrate(self, current_usage, future_usage):\n",
    "        # Define thresholds for migration\n",
    "        CPU_THRESHOLD = 80  # percent\n",
    "        MEMORY_THRESHOLD = 80  # percent\n",
    "        \n",
    "        return (future_usage['predicted_cpu_usage'] > CPU_THRESHOLD or \n",
    "                current_usage['memory_usage'] > MEMORY_THRESHOLD)\n",
    "\n",
    "    def find_best_host(self, container, future_usage, potential_hosts):\n",
    "        best_host = None\n",
    "        min_load = float('inf')\n",
    "        \n",
    "        for host in potential_hosts:\n",
    "            host_load = self.calculate_host_load(host, future_usage)\n",
    "            if host_load < min_load and self.can_host_accommodate(host, container, future_usage):\n",
    "                best_host = host\n",
    "                min_load = host_load\n",
    "        \n",
    "        return best_host\n",
    "\n",
    "    def calculate_host_load(self, host, future_usage):\n",
    "        # This should calculate the projected load on the host\n",
    "        # including the future usage of the container\n",
    "        return host.current_load + future_usage['predicted_cpu_usage']\n",
    "\n",
    "    def can_host_accommodate(self, host, container, future_usage):\n",
    "        # Check if the host has enough resources for the container\n",
    "        return (host.available_cpu >= future_usage['predicted_cpu_usage'] and\n",
    "                host.available_memory >= future_usage['memory_usage'])\n",
    "\n",
    "    def create_migration_request(self, container, source_host, destination_host):\n",
    "        return MigrationRequest(\n",
    "            container_id=container.id,\n",
    "            source_host=source_host,\n",
    "            destination_host=destination_host,\n",
    "            migration_type=self.select_migration_strategy(container, source_host, destination_host)\n",
    "        )\n",
    "\n",
    "    def select_migration_strategy(self, container, source_host, destination_host):\n",
    "        # This method should select the appropriate migration strategy\n",
    "        # based on the container, source and destination hosts\n",
    "        # For simplicity, we'll always choose live migration\n",
    "        return MigrationStrategy.LIVE_MIGRATION\n",
    "\n",
    "    def make_migration_decision(self, container, current_host, potential_hosts):\n",
    "        # Get current resource usage\n",
    "        current_usage = self.get_current_resource_usage(container, current_host)\n",
    "        \n",
    "        # Predict future resource usage\n",
    "        future_usage = self.predict_future_resource_usage(container, current_usage)\n",
    "        \n",
    "        # Check if migration is needed based on current and predicted usage\n",
    "        if self.should_migrate(current_usage, future_usage):\n",
    "            # Find the best host for migration\n",
    "            best_host = self.find_best_host(container, future_usage, potential_hosts)\n",
    "            \n",
    "            if best_host:\n",
    "                # Create a migration request\n",
    "                migration_request = self.create_migration_request(container, current_host, best_host)\n",
    "                \n",
    "                # Check if the migration request satisfies all policies\n",
    "                if self.policy_enforcer.enforce_policies(migration_request):\n",
    "                    return migration_request\n",
    "        \n",
    "        return None  # No migration needed or possible\n",
    "\n",
    "\n",
    "#WorkloadAnalyzer\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class WorkloadAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        self.lstm_model = None\n",
    "        self.scaler = None  # We'll use this to normalize data for LSTM\n",
    "        self.feature_columns = ['cpu_usage', 'memory_usage', 'network_in', 'network_out', 'time_of_day', 'day_of_week']\n",
    "        self.target_column = 'future_cpu_usage'\n",
    "        \n",
    "    def prepare_data(self, data):\n",
    "        # Assume data is a pandas DataFrame with columns matching self.feature_columns\n",
    "        # and a target column 'future_cpu_usage'\n",
    "        X = data[self.feature_columns]\n",
    "        y = data[self.target_column]\n",
    "        \n",
    "        # Normalize data for LSTM\n",
    "        self.scaler = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "            X.values, y.values, length=10, batch_size=32)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def train_rf_model(self, X, y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.rf_model.fit(X_train, y_train)\n",
    "        y_pred = self.rf_model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        logger.info(f\"Random Forest MSE: {mse}\")\n",
    "\n",
    "    def train_lstm_model(self, X, y):\n",
    "        # Reshape data for LSTM [samples, time steps, features]\n",
    "        X_reshaped = X.values.reshape((X.shape[0], 1, X.shape[1]))\n",
    "        \n",
    "        self.lstm_model = Sequential([\n",
    "            LSTM(50, activation='relu', input_shape=(1, X.shape[1])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        self.lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "        self.lstm_model.fit(X_reshaped, y, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "        \n",
    "        y_pred = self.lstm_model.predict(X_reshaped)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        logger.info(f\"LSTM MSE: {mse}\")\n",
    "\n",
    "    def predict_resource_usage(self, features):\n",
    "        # Ensure features is a 2D array\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(1, -1)\n",
    "        \n",
    "        rf_pred = self.rf_model.predict(features)\n",
    "        \n",
    "        # Reshape for LSTM prediction\n",
    "        lstm_features = features.reshape((features.shape[0], 1, features.shape[1]))\n",
    "        lstm_pred = self.lstm_model.predict(lstm_features)\n",
    "        \n",
    "        # Combine predictions (simple average)\n",
    "        combined_pred = (rf_pred + lstm_pred.flatten()) / 2\n",
    "        \n",
    "        return combined_pred\n",
    "\n",
    "    def get_feature_importance(self):\n",
    "        return dict(zip(self.feature_columns, self.rf_model.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource Optimizer\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class ResourceOptimizer:\n",
    "    def __init__(self, performance_metrics_collector, network_manager):\n",
    "        self.hosts = {}\n",
    "        self.containers = {}\n",
    "        self.performance_metrics_collector = performance_metrics_collector\n",
    "        self.network_manager = network_manager\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def optimize_allocation(self, hosts, containers):\n",
    "        allocation = {}\n",
    "        for container in containers:\n",
    "            best_host = self.select_best_host(hosts, container)\n",
    "            if best_host:\n",
    "                allocation[container.container_id] = best_host.host_id\n",
    "        return allocation\n",
    "\n",
    "    def select_best_host(self, hosts: List[Any], container: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Selects the best host for a given container based on multiple criteria.\n",
    "        \n",
    "        Args:\n",
    "            hosts: List of potential host objects.\n",
    "            container: Container object to be migrated.\n",
    "        \n",
    "        Returns:\n",
    "            The best host object for the container, or None if no suitable host is found.\n",
    "        \"\"\"\n",
    "        suitable_hosts = []\n",
    "        host_scores = []\n",
    "\n",
    "        for host in hosts:\n",
    "            if self.can_host_accommodate(host, container):\n",
    "                suitable_hosts.append(host)\n",
    "                host_scores.append(self.calculate_host_score(host, container))\n",
    "\n",
    "        if not suitable_hosts:\n",
    "            return None\n",
    "\n",
    "        best_host_index = np.argmax(host_scores)\n",
    "        return suitable_hosts[best_host_index]\n",
    "\n",
    "    def can_host_accommodate(self, host: Any, container: Any) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if a host can accommodate the container based on resource requirements.\n",
    "        \n",
    "        Args:\n",
    "            host: Host object to check.\n",
    "            container: Container object to be accommodated.\n",
    "        \n",
    "        Returns:\n",
    "            True if the host can accommodate the container, False otherwise.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            host.available_cpu >= container.cpu_limit and\n",
    "            host.available_memory >= container.memory_limit and\n",
    "            host.available_storage >= container.storage_limit\n",
    "        )\n",
    "\n",
    "    def calculate_host_score(self, host: Any, container: Any) -> float:\n",
    "        \"\"\"\n",
    "        Calculates a score for a host based on multiple criteria.\n",
    "        \n",
    "        Args:\n",
    "            host: Host object to score.\n",
    "            container: Container object to be migrated.\n",
    "        \n",
    "        Returns:\n",
    "            A float representing the host's score.\n",
    "        \"\"\"\n",
    "        # Fetch current resource usage\n",
    "        host_metrics = self.performance_metrics_collector.get_host_metrics(host.host_id)\n",
    "        \n",
    "        # Calculate resource utilization after potential migration\n",
    "        cpu_utilization = (host.total_cpu - host.available_cpu + container.cpu_limit) / host.total_cpu\n",
    "        memory_utilization = (host.total_memory - host.available_memory + container.memory_limit) / host.total_memory\n",
    "        storage_utilization = (host.total_storage - host.available_storage + container.storage_limit) / host.total_storage\n",
    "        \n",
    "        # Calculate network latency and bandwidth\n",
    "        network_metrics = self.network_manager.get_network_metrics(container.host, host.host_id)\n",
    "        latency = network_metrics['latency']\n",
    "        bandwidth = network_metrics['bandwidth']\n",
    "        \n",
    "        # Prepare feature vector\n",
    "        features = np.array([\n",
    "            cpu_utilization,\n",
    "            memory_utilization,\n",
    "            storage_utilization,\n",
    "            host_metrics['load_average'],\n",
    "            latency,\n",
    "            bandwidth,\n",
    "            host.power_efficiency,  # Assuming this is a property of the host\n",
    "            len(host.containers)  # Number of containers already on the host\n",
    "        ]).reshape(1, -1)\n",
    "        \n",
    "        # Normalize features\n",
    "        normalized_features = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Define weights for each criterion (adjust these based on your priorities)\n",
    "        weights = np.array([0.2, 0.2, 0.1, 0.1, 0.15, 0.15, 0.05, 0.05])\n",
    "        \n",
    "        # Calculate weighted score\n",
    "        score = np.dot(normalized_features, weights)[0]\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def update_resource_allocation(self, container: Any, old_host: Any, new_host: Any):\n",
    "        \"\"\"\n",
    "        Updates the resource allocation after a container migration.\n",
    "        \n",
    "        Args:\n",
    "            container: The migrated container object.\n",
    "            old_host: The host object from which the container was migrated.\n",
    "            new_host: The host object to which the container was migrated.\n",
    "        \"\"\"\n",
    "        # Update old host\n",
    "        old_host.available_cpu += container.cpu_limit\n",
    "        old_host.available_memory += container.memory_limit\n",
    "        old_host.available_storage += container.storage_limit\n",
    "        old_host.containers.remove(container)\n",
    "\n",
    "        # Update new host\n",
    "        new_host.available_cpu -= container.cpu_limit\n",
    "        new_host.available_memory -= container.memory_limit\n",
    "        new_host.available_storage -= container.storage_limit\n",
    "        new_host.containers.append(container)\n",
    "\n",
    "        # Update container's host\n",
    "        container.host = new_host.host_id\n",
    "\n",
    "    def rebalance_resources(self):\n",
    "        \"\"\"\n",
    "        Periodically rebalances resources across all hosts to optimize overall system performance.\n",
    "        \"\"\"\n",
    "        all_containers = list(self.containers.values())\n",
    "        all_hosts = list(self.hosts.values())\n",
    "        \n",
    "        for container in all_containers:\n",
    "            current_host = self.hosts[container.host]\n",
    "            best_host = self.select_best_host(all_hosts, container)\n",
    "            \n",
    "            if best_host and best_host != current_host:\n",
    "                self.update_resource_allocation(container, current_host, best_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migration Planner\n",
    "\n",
    "import networkx as nx\n",
    "from typing import Dict, Any\n",
    "from typing import List\n",
    "\n",
    "class MigrationPlanner:\n",
    "    def __init__(self, network_manager):\n",
    "        self.network_manager = network_manager\n",
    "\n",
    "    def calculate_migration_cost(self, container_id: str, source_host: str, target_host: str) -> float:\n",
    "        container = self.network_manager.get_container(container_id)\n",
    "        network_path = self.network_manager.compute_optimal_path(source_host, target_host)\n",
    "        \n",
    "        if not network_path:\n",
    "            return float('inf')  # No path available, migration is impossible\n",
    "        \n",
    "        # Calculate data transfer cost\n",
    "        data_size = container.memory_usage + container.disk_usage\n",
    "        bandwidth = min(self.network_manager.get_path_bandwidth(network_path))\n",
    "        transfer_time = data_size / bandwidth\n",
    "        \n",
    "        # Calculate resource usage cost\n",
    "        cpu_cost = container.cpu_usage * self.network_manager.get_cpu_cost(target_host)\n",
    "        memory_cost = container.memory_usage * self.network_manager.get_memory_cost(target_host)\n",
    "        \n",
    "        # Calculate downtime cost\n",
    "        downtime = self.estimate_downtime(container_id, source_host, target_host)\n",
    "        downtime_cost = downtime * container.importance_factor\n",
    "        \n",
    "        total_cost = transfer_time + cpu_cost + memory_cost + downtime_cost\n",
    "        return total_cost\n",
    "\n",
    "    def estimate_downtime(self, container_id: str, source_host: str, target_host: str) -> float:\n",
    "        container = self.network_manager.get_container(container_id)\n",
    "        network_path = self.network_manager.compute_optimal_path(source_host, target_host)\n",
    "        \n",
    "        if not network_path:\n",
    "            return float('inf')  # No path available, migration is impossible\n",
    "        \n",
    "        # Calculate network latency\n",
    "        latency = sum(self.network_manager.get_link_latency(link) for link in network_path)\n",
    "        \n",
    "        # Estimate time for final memory transfer\n",
    "        final_memory_transfer = container.memory_dirty_rate * latency\n",
    "        bandwidth = min(self.network_manager.get_path_bandwidth(network_path))\n",
    "        transfer_time = final_memory_transfer / bandwidth\n",
    "        \n",
    "        # Add time for container stop and start operations\n",
    "        stop_time = 0.1  # Assuming 100ms for container stop\n",
    "        start_time = 0.2  # Assuming 200ms for container start\n",
    "        \n",
    "        total_downtime = latency + transfer_time + stop_time + start_time\n",
    "        return total_downtime\n",
    "\n",
    "    def plan_migration(self, allocation: Dict[str, str], current_allocation: Dict[str, str]) -> List[Dict[str, Any]]:\n",
    "        migration_plan = []\n",
    "        for container_id, target_host_id in allocation.items():\n",
    "            if current_allocation.get(container_id) != target_host_id:\n",
    "                source_host_id = current_allocation.get(container_id)\n",
    "                migration_cost = self.calculate_migration_cost(container_id, source_host_id, target_host_id)\n",
    "                downtime = self.estimate_downtime(container_id, source_host_id, target_host_id)\n",
    "                migration_plan.append({\n",
    "                    'container_id': container_id,\n",
    "                    'source_host': source_host_id,\n",
    "                    'destination_host': target_host_id,\n",
    "                    'migration_cost': migration_cost,\n",
    "                    'downtime': downtime\n",
    "                })\n",
    "        # Sort by migration_cost ascending\n",
    "        migration_plan.sort(key=lambda x: x['migration_cost'])\n",
    "        return migration_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Container Manager Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime Controller\n",
    "\n",
    "import docker\n",
    "import psutil\n",
    "\n",
    "class ContainerRuntimeInterface:\n",
    "    def __init__(self):\n",
    "        self.supported_runtimes = {\n",
    "            'docker': docker.DockerClient,\n",
    "            'containerd': docker.DockerClient,  # Placeholder: Implement containerd client\n",
    "            'cri-o': docker.DockerClient       # Placeholder: Implement CRI-O client\n",
    "        }\n",
    "\n",
    "    def get_runtime_instance(self, runtime_type):\n",
    "        runtime_class = self.supported_runtimes.get(runtime_type.lower())\n",
    "        if runtime_class:\n",
    "            return runtime_class(base_url='tcp://127.0.0.1:2375')  # Example for Docker\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported runtime type: {runtime_type}\")\n",
    "\n",
    "    def create_container(self, runtime, container_spec):\n",
    "        return runtime.containers.create(**container_spec)\n",
    "\n",
    "    def start_container(self, container):\n",
    "        container.start()\n",
    "\n",
    "    def stop_container(self, container):\n",
    "        container.stop()\n",
    "\n",
    "    def checkpoint_container(self, container, checkpoint_name, checkpoint_dir):\n",
    "        subprocess.run(['docker', 'checkpoint', 'create', container.id, checkpoint_name, '--checkpoint-dir', checkpoint_dir], check=True)\n",
    "\n",
    "    def restore_container(self, runtime, container_spec, checkpoint_dir):\n",
    "        container = runtime.containers.run(**container_spec, checkpoint=checkpoint_dir, detach=True)\n",
    "        return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NestedContainerManager\n",
    "import docker\n",
    "import subprocess\n",
    "import os\n",
    "import tempfile\n",
    "from docker.errors import DockerException, APIError, ImageNotFound\n",
    "\n",
    "class NestedContainerManager:\n",
    "    def __init__(self, runtime_interface, state_synchronizer, network_manager):\n",
    "        self.runtime_interface = runtime_interface\n",
    "        self.state_synchronizer = state_synchronizer\n",
    "        self.network_manager = network_manager\n",
    "        self.docker_client = docker.from_env()\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    async def migrate_nested_container(self, nested_container: Dict[str, Any], source_host: Dict[str, Any], target_host: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Migrates a nested container from the source host to the target host.\n",
    "\n",
    "        Args:\n",
    "            nested_container: A dictionary containing information about the nested container.\n",
    "            source_host: Information about the source host.\n",
    "            target_host: Information about the target host.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if migration was successful, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Starting migration of nested container {nested_container['id']} from {source_host['id']} to {target_host['id']}\")\n",
    "\n",
    "            # Step 1: Prepare for migration\n",
    "            await self._prepare_for_migration(nested_container, source_host, target_host)\n",
    "\n",
    "            # Step 2: Checkpoint the outer container\n",
    "            outer_checkpoint = await self._checkpoint_outer_container(nested_container['outer_id'], source_host)\n",
    "\n",
    "            # Step 3: Checkpoint the inner container\n",
    "            inner_checkpoint = await self._checkpoint_inner_container(nested_container['inner_id'], source_host)\n",
    "\n",
    "            # Step 4: Transfer checkpoints to the target host\n",
    "            await self._transfer_checkpoints(outer_checkpoint, inner_checkpoint, source_host, target_host)\n",
    "\n",
    "            # Step 5: Restore the outer container on the target host\n",
    "            new_outer_id = await self._restore_outer_container(outer_checkpoint, target_host)\n",
    "\n",
    "            # Step 6: Restore the inner container on the target host\n",
    "            new_inner_id = await self._restore_inner_container(inner_checkpoint, new_outer_id, target_host)\n",
    "\n",
    "            # Step 7: Verify the restoration\n",
    "            if await self._verify_restoration(new_outer_id, new_inner_id, target_host):\n",
    "                # Step 8: Update network configuration\n",
    "                await self._update_network_configuration(nested_container, new_outer_id, new_inner_id, source_host, target_host)\n",
    "\n",
    "                # Step 9: Clean up source host\n",
    "                await self._cleanup_source(nested_container, source_host)\n",
    "\n",
    "                self.logger.info(f\"Successfully migrated nested container {nested_container['id']} to {target_host['id']}\")\n",
    "                return True\n",
    "            else:\n",
    "                raise Exception(\"Nested container restoration verification failed\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to migrate nested container {nested_container['id']}: {str(e)}\")\n",
    "            await self._rollback_migration(nested_container, source_host, target_host)\n",
    "            return False\n",
    "\n",
    "    async def _prepare_for_migration(self, nested_container: Dict[str, Any], source_host: Dict[str, Any], target_host: Dict[str, Any]):\n",
    "        \"\"\"Prepares the source and target hosts for migration.\"\"\"\n",
    "        # Ensure target host has necessary images\n",
    "        await self._ensure_images(nested_container, target_host)\n",
    "        \n",
    "        # Pre-create networks on target host\n",
    "        await self._pre_create_networks(nested_container, target_host)\n",
    "\n",
    "    async def _ensure_images(self, nested_container: Dict[str, Any], target_host: Dict[str, Any]):\n",
    "        \"\"\"Ensures that the target host has the necessary container images.\"\"\"\n",
    "        outer_image = nested_container['outer_image']\n",
    "        inner_image = nested_container['inner_image']\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for image in [outer_image, inner_image]:\n",
    "                async with session.post(f\"http://{target_host['address']}:2375/images/create\", params={'fromImage': image}) as response:\n",
    "                    if response.status != 200:\n",
    "                        raise Exception(f\"Failed to pull image {image} on target host\")\n",
    "\n",
    "    async def _pre_create_networks(self, nested_container: Dict[str, Any], target_host: Dict[str, Any]):\n",
    "        \"\"\"Pre-creates necessary networks on the target host.\"\"\"\n",
    "        networks = nested_container['networks']\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for network in networks:\n",
    "                async with session.post(f\"http://{target_host['address']}:2375/networks/create\", json=network) as response:\n",
    "                    if response.status != 201:\n",
    "                        raise Exception(f\"Failed to create network {network['Name']} on target host\")\n",
    "\n",
    "    async def _checkpoint_outer_container(self, outer_id: str, source_host: Dict[str, Any]) -> str:\n",
    "        \"\"\"Creates a checkpoint of the outer container.\"\"\"\n",
    "        checkpoint_dir = tempfile.mkdtemp(prefix=\"outer_checkpoint_\")\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(f\"http://{source_host['address']}:2375/containers/{outer_id}/checkpoint\", \n",
    "                                    json={\"CheckpointDir\": checkpoint_path}) as response:\n",
    "                if response.status != 201:\n",
    "                    raise Exception(f\"Failed to checkpoint outer container {outer_id}\")\n",
    "        \n",
    "        return checkpoint_dir\n",
    "\n",
    "    async def _checkpoint_inner_container(self, inner_id: str, source_host: Dict[str, Any]) -> str:\n",
    "        \"\"\"Creates a checkpoint of the inner container.\"\"\"\n",
    "        checkpoint_dir = tempfile.mkdtemp(prefix=\"inner_checkpoint_\")\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(f\"http://{source_host['address']}:2375/containers/{inner_id}/checkpoint\", \n",
    "                                    json={\"CheckpointDir\": checkpoint_path}) as response:\n",
    "                if response.status != 201:\n",
    "                    raise Exception(f\"Failed to checkpoint inner container {inner_id}\")\n",
    "        \n",
    "        return checkpoint_dir\n",
    "\n",
    "    async def _transfer_checkpoints(self, outer_checkpoint: str, inner_checkpoint: str, source_host: Dict[str, Any], target_host: Dict[str, Any]):\n",
    "        \"\"\"Transfers checkpoints from source host to target host.\"\"\"\n",
    "        for checkpoint in [outer_checkpoint, inner_checkpoint]:\n",
    "            tar_path = checkpoint + \".tar.gz\"\n",
    "            await self._create_tar(checkpoint, tar_path)\n",
    "            await self._scp_file(tar_path, source_host, target_host, \"/tmp/\")\n",
    "            await self._extract_tar(tar_path, checkpoint, target_host)\n",
    "\n",
    "    async def _create_tar(self, source_dir: str, tar_path: str):\n",
    "        \"\"\"Creates a tar archive of the checkpoint directory.\"\"\"\n",
    "        with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "            tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "    async def _scp_file(self, file_path: str, source_host: Dict[str, Any], target_host: Dict[str, Any], target_dir: str):\n",
    "        \"\"\"Copies a file from source host to target host using SCP.\"\"\"\n",
    "        cmd = f\"scp {file_path} {target_host['user']}@{target_host['address']}:{target_dir}\"\n",
    "        process = await asyncio.create_subprocess_shell(cmd)\n",
    "        await process.wait()\n",
    "\n",
    "    async def _extract_tar(self, tar_path: str, extract_path: str, host: Dict[str, Any]):\n",
    "        \"\"\"Extracts a tar archive on the specified host.\"\"\"\n",
    "        cmd = f\"ssh {host['user']}@{host['address']} 'tar -xzf {tar_path} -C {os.path.dirname(extract_path)}'\"\n",
    "        process = await asyncio.create_subprocess_shell(cmd)\n",
    "        await process.wait()\n",
    "\n",
    "    async def _restore_outer_container(self, checkpoint: str, target_host: Dict[str, Any]) -> str:\n",
    "        \"\"\"Restores the outer container on the target host.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(f\"http://{target_host['address']}:2375/containers/create\", \n",
    "                                    json={\"Image\": \"outer_image\", \"HostConfig\": {\"NetworkMode\": \"host\"}}) as response:\n",
    "                if response.status != 201:\n",
    "                    raise Exception(\"Failed to create outer container on target host\")\n",
    "                container_id = (await response.json())['Id']\n",
    "\n",
    "            async with session.post(f\"http://{target_host['address']}:2375/containers/{container_id}/start\") as response:\n",
    "                if response.status != 204:\n",
    "                    raise Exception(\"Failed to start outer container on target host\")\n",
    "\n",
    "            async with session.post(f\"http://{target_host['address']}:2375/containers/{container_id}/restore\", \n",
    "                                    json={\"CheckpointDir\": checkpoint}) as response:\n",
    "                if response.status != 204:\n",
    "                    raise Exception(\"Failed to restore outer container from checkpoint\")\n",
    "\n",
    "        return container_id\n",
    "\n",
    "    async def _restore_inner_container(self, checkpoint: str, outer_id: str, target_host: Dict[str, Any]) -> str:\n",
    "        \"\"\"Restores the inner container inside the outer container on the target host.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(f\"http://{target_host['address']}:2375/containers/create\", \n",
    "                                    json={\"Image\": \"inner_image\", \"HostConfig\": {\"NetworkMode\": f\"container:{outer_id}\"}}) as response:\n",
    "                if response.status != 201:\n",
    "                    raise Exception(\"Failed to create inner container on target host\")\n",
    "                container_id = (await response.json())['Id']\n",
    "\n",
    "            async with session.post(f\"http://{target_host['address']}:2375/containers/{container_id}/start\") as response:\n",
    "                if response.status != 204:\n",
    "                    raise Exception(\"Failed to start inner container on target host\")\n",
    "\n",
    "            async with session.post(f\"http://{target_host['address']}:2375/containers/{container_id}/restore\", \n",
    "                                    json={\"CheckpointDir\": checkpoint}) as response:\n",
    "                if response.status != 204:\n",
    "                    raise Exception(\"Failed to restore inner container from checkpoint\")\n",
    "\n",
    "        return container_id\n",
    "\n",
    "    async def _verify_restoration(self, outer_id: str, inner_id: str, target_host: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Verifies that both outer and inner containers are running correctly after restoration.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for container_id in [outer_id, inner_id]:\n",
    "                async with session.get(f\"http://{target_host['address']}:2375/containers/{container_id}/json\") as response:\n",
    "                    if response.status != 200:\n",
    "                        return False\n",
    "                    container_info = await response.json()\n",
    "                    if container_info['State']['Status'] != 'running':\n",
    "                        return False\n",
    "        return True\n",
    "\n",
    "    async def _update_network_configuration(self, nested_container: Dict[str, Any], new_outer_id: str, new_inner_id: str, source_host: Dict[str, Any], target_host: Dict[str, Any]):\n",
    "        \"\"\"Updates network configuration for the migrated nested container.\"\"\"\n",
    "        # Update DNS records\n",
    "        await self.network_manager.update_dns_records(nested_container['id'], target_host['address'])\n",
    "\n",
    "        # Update load balancer\n",
    "        await self.network_manager.update_load_balancer(nested_container['id'], target_host['address'])\n",
    "\n",
    "        # Update SDN flow rules\n",
    "        await self.network_manager.update_sdn_flow_rules(nested_container['id'], source_host['address'], target_host['address'])\n",
    "\n",
    "    async def _cleanup_source(self, nested_container: Dict[str, Any], source_host: Dict[str, Any]):\n",
    "        \"\"\"Cleans up resources on the source host after successful migration.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for container_id in [nested_container['outer_id'], nested_container['inner_id']]:\n",
    "                async with session.post(f\"http://{source_host['address']}:2375/containers/{container_id}/stop\") as response:\n",
    "                    if response.status != 204:\n",
    "                        self.logger.warning(f\"Failed to stop container {container_id} on source host\")\n",
    "\n",
    "                async with session.delete(f\"http://{source_host['address']}:2375/containers/{container_id}\") as response:\n",
    "                    if response.status != 204:\n",
    "                        self.logger.warning(f\"Failed to remove container {container_id} on source host\")\n",
    "\n",
    "    async def _rollback_migration(self, nested_container: Dict[str, Any], source_host: Dict[str, Any], target_host: Dict[str, Any]):\n",
    "        \"\"\"Rolls back the migration if any step fails.\"\"\"\n",
    "        self.logger.info(f\"Rolling back migration for nested container {nested_container['id']}\")\n",
    "\n",
    "        # Stop and remove any containers created on the target host\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for container_type in ['outer', 'inner']:\n",
    "                async with session.get(f\"http://{target_host['address']}:2375/containers/json?filters={json.dumps({'name': [f'{nested_container['id']}_{container_type}']})}\", ) as response:\n",
    "                    if response.status == 200:\n",
    "                        containers = await response.json()\n",
    "                        for container in containers:\n",
    "                            container_id = container['Id']\n",
    "                            await session.post(f\"http://{target_host['address']}:2375/containers/{container_id}/stop\")\n",
    "                            await session.delete(f\"http://{target_host['address']}:2375/containers/{container_id}\")\n",
    "\n",
    "        # Restart the original containers on the source host if they were stopped\n",
    "        for container_id in [nested_container['outer_id'], nested_container['inner_id']]:\n",
    "            await self.docker_client.containers.get(container_id).start()\n",
    "\n",
    "        # Revert network changes\n",
    "        await self.network_manager.revert_network_changes(nested_container['id'], source_host['address'], target_host['address'])\n",
    "\n",
    "        self.logger.info(f\"Rollback completed for nested container {nested_container['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageManager\n",
    "\n",
    "import docker\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "import hashlib\n",
    "\n",
    "import docker\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "import hashlib\n",
    "from docker.errors import DockerException, APIError, ImageNotFound\n",
    "\n",
    "class ImageManager:\n",
    "    def __init__(self, runtime_interface):\n",
    "        self.runtime_interface = runtime_interface\n",
    "        self.client = docker.from_env()\n",
    "        self.logger = logging.getLogger('ImageManager')\n",
    "\n",
    "    def pull_image(self, image_ref):\n",
    "        try:\n",
    "            self.logger.info(f\"Pulling image: {image_ref}\")\n",
    "            image = self.client.images.pull(image_ref)\n",
    "            self.logger.info(f\"Successfully pulled image: {image_ref}\")\n",
    "            return image\n",
    "        except docker.errors.APIError as e:\n",
    "            self.logger.error(f\"Failed to pull image {image_ref}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def push_image(self, image_ref, repository):\n",
    "        try:\n",
    "            self.logger.info(f\"Pushing image {image_ref} to repository {repository}\")\n",
    "            push_output = self.client.images.push(repository, tag=image_ref.split(':')[-1])\n",
    "            self.logger.info(f\"Successfully pushed image {image_ref} to repository {repository}\")\n",
    "            return push_output\n",
    "        except docker.errors.APIError as e:\n",
    "            self.logger.error(f\"Failed to push image {image_ref} to repository {repository}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def optimize_image_distribution(self, source_runtime, target_runtime, image_ref):\n",
    "        try:\n",
    "            self.logger.info(f\"Optimizing distribution of image {image_ref}\")\n",
    "            if self.image_exists(target_runtime, image_ref):\n",
    "                self.logger.debug(f\"Image {image_ref} exists on target, computing diff\")\n",
    "                diff_layers = self._compute_image_diff(source_runtime, target_runtime, image_ref)\n",
    "                self.logger.debug(f\"Transferring {len(diff_layers)} diff layers for {image_ref}\")\n",
    "                self._transfer_image_diff(diff_layers, target_runtime, image_ref)\n",
    "                self.logger.debug(f\"Applying image diff for {image_ref} on target\")\n",
    "                self._apply_image_diff(diff_layers, target_runtime, image_ref)\n",
    "            else:\n",
    "                self.logger.debug(f\"Image {image_ref} does not exist on target, pulling full image\")\n",
    "                self.pull_image(image_ref)\n",
    "            self.logger.info(f\"Successfully optimized distribution of image {image_ref}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to optimize distribution of image {image_ref}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def image_exists(self, runtime, image_ref):\n",
    "        try:\n",
    "            runtime.images.get(image_ref)\n",
    "            self.logger.debug(f\"Image {image_ref} exists\")\n",
    "            return True\n",
    "        except docker.errors.ImageNotFound:\n",
    "            self.logger.debug(f\"Image {image_ref} not found\")\n",
    "            return False\n",
    "\n",
    "    def _compute_image_diff(self, source_runtime, target_runtime, image_ref):\n",
    "        source_layers = self._get_image_layers(source_runtime, image_ref)\n",
    "        target_layers = self._get_image_layers(target_runtime, image_ref)\n",
    "        diff_layers = [layer for layer in source_layers if layer not in target_layers]\n",
    "        self.logger.debug(f\"Computed {len(diff_layers)} diff layers\")\n",
    "        return diff_layers\n",
    "\n",
    "    def _get_image_layers(self, runtime, image_ref):\n",
    "        image = runtime.images.get(image_ref)\n",
    "        return image.history()\n",
    "\n",
    "    def _transfer_image_diff(self, diff_layers, target_runtime, image_ref):\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Save diff layers to tar files\n",
    "            tar_files = []\n",
    "            for layer in diff_layers:\n",
    "                layer_id = layer['Id']\n",
    "                tar_path = os.path.join(temp_dir, f\"{layer_id}.tar\")\n",
    "                self.client.images.get(layer_id).save(tar_path)\n",
    "                tar_files.append(tar_path)\n",
    "            \n",
    "            # Transfer tar files to target runtime\n",
    "            for tar_file in tar_files:\n",
    "                with open(tar_file, 'rb') as f:\n",
    "                    target_runtime.images.load(f.read())\n",
    "\n",
    "    def _apply_image_diff(self, diff_layers, target_runtime, image_ref):\n",
    "        # Create a new image from the base image and apply diff layers\n",
    "        base_image = target_runtime.images.get(image_ref)\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            dockerfile = f\"FROM {image_ref}\\n\"\n",
    "            for layer in diff_layers:\n",
    "                layer_id = layer['Id']\n",
    "                dockerfile += f\"ADD {layer_id}.tar /\\n\"\n",
    "            \n",
    "            dockerfile_path = os.path.join(temp_dir, \"Dockerfile\")\n",
    "            with open(dockerfile_path, \"w\") as f:\n",
    "                f.write(dockerfile)\n",
    "            \n",
    "            new_image, _ = target_runtime.images.build(path=temp_dir, dockerfile=dockerfile_path, tag=image_ref)\n",
    "        \n",
    "        return new_image\n",
    "\n",
    "    def deduplicate_layers(self, image_ref):\n",
    "        try:\n",
    "            self.logger.info(f\"Deduplicating layers for image {image_ref}\")\n",
    "            image = self.client.images.get(image_ref)\n",
    "            layers = image.history()\n",
    "            unique_layers = {}\n",
    "            \n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                for layer in layers:\n",
    "                    layer_id = layer['Id']\n",
    "                    layer_content = self._get_layer_content(layer_id, temp_dir)\n",
    "                    layer_hash = self._compute_layer_hash(layer_content)\n",
    "                    if layer_hash not in unique_layers:\n",
    "                        unique_layers[layer_hash] = layer_id\n",
    "            \n",
    "            self.logger.debug(f\"Found {len(unique_layers)} unique layers out of {len(layers)} total layers\")\n",
    "            new_image = self._rebuild_image_with_unique_layers(image_ref, list(unique_layers.values()))\n",
    "            self.logger.info(f\"Successfully deduplicated layers for image {image_ref}\")\n",
    "            return new_image\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to deduplicate layers for image {image_ref}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _get_layer_content(self, layer_id, temp_dir):\n",
    "        layer_tar = os.path.join(temp_dir, f\"{layer_id}.tar\")\n",
    "        self.client.images.get(layer_id).save(layer_tar)\n",
    "        return layer_tar\n",
    "\n",
    "    def _compute_layer_hash(self, layer_tar):\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        with open(layer_tar, \"rb\") as f:\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "\n",
    "    def _rebuild_image_with_unique_layers(self, image_ref, unique_layer_ids):\n",
    "        self.logger.debug(f\"Rebuilding image {image_ref} with {len(unique_layer_ids)} unique layers\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Create a new Dockerfile\n",
    "            dockerfile = f\"FROM scratch\\n\"\n",
    "            for layer_id in unique_layer_ids:\n",
    "                dockerfile += f\"ADD {layer_id}.tar /\\n\"\n",
    "            \n",
    "            dockerfile_path = os.path.join(temp_dir, \"Dockerfile\")\n",
    "            with open(dockerfile_path, \"w\") as f:\n",
    "                f.write(dockerfile)\n",
    "            \n",
    "            # Save layer tars\n",
    "            for layer_id in unique_layer_ids:\n",
    "                layer_tar = os.path.join(temp_dir, f\"{layer_id}.tar\")\n",
    "                self.client.images.get(layer_id).save(layer_tar)\n",
    "            \n",
    "            # Build new image\n",
    "            new_image, _ = self.client.images.build(path=temp_dir, dockerfile=dockerfile_path, tag=image_ref)\n",
    "        \n",
    "        self.logger.debug(f\"Successfully rebuilt image {image_ref} with unique layers\")\n",
    "        return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Manager Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetworkManager\n",
    "import networkx as nx\n",
    "from typing import Tuple\n",
    "from os_ken.base import app_manager\n",
    "from os_ken.ofproto import ofproto_v1_3\n",
    "from os_ken.controller.handler import set_ev_cls\n",
    "from os_ken.controller import ofp_event\n",
    "from os_ken.controller.handler import MAIN_DISPATCHER, CONFIG_DISPATCHER\n",
    "from os_ken.lib.packet import packet, ethernet, arp, ipv4, tcp, udp\n",
    "from os_ken.topology import event, switches\n",
    "from os_ken.topology.api import get_switch, get_link\n",
    "from os_ken.lib.packet import packet\n",
    "from os_ken.lib.packet import ethernet\n",
    "from os_ken.lib.packet import ether_types\n",
    "\n",
    "class NetworkManager(app_manager.OSKenApp):\n",
    "    OFP_VERSIONS = [ofproto_v1_3.OFP_VERSION]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.network_graph = nx.Graph()\n",
    "        self.mac_to_port = {}\n",
    "        self.containers = {}\n",
    "        self.host_resources = {}\n",
    "        self.migration_manager = None\n",
    "        self.sdn_controller = SDNControllerConnector()\n",
    "        self.dns_manager = DNSControllerManager()\n",
    "        self.traffic_manager = NetworkTrafficManager()\n",
    "\n",
    "    def set_migration_manager(self, migration_manager):\n",
    "        self.migration_manager = migration_manager\n",
    "        \n",
    "    def get_container(self, container_id: str) -> Any:\n",
    "        return self.containers.get(container_id)\n",
    "\n",
    "    def get_cpu_cost(self, host_id: str) -> float:\n",
    "        return self.host_resources.get(host_id, {}).get('cpu_cost', 1.0)\n",
    "\n",
    "    def get_memory_cost(self, host_id: str) -> float:\n",
    "        return self.host_resources.get(host_id, {}).get('memory_cost', 1.0)\n",
    "\n",
    "    def get_link_latency(self, link: Tuple[str, str]) -> float:\n",
    "        return self.network_graph.edges[link].get('latency', 0.001)  # Default to 1ms if not set\n",
    "    def get_path_bandwidth(self, path: List[str]) -> List[float]:\n",
    "        return [self.network_graph.edges[link].get('bandwidth', float('inf')) for link in zip(path, path[1:])]\n",
    "    \n",
    "    @set_ev_cls(ofp_event.EventOFPSwitchFeatures, CONFIG_DISPATCHER)\n",
    "    def switch_features_handler(self, ev):\n",
    "        datapath = ev.msg.datapath\n",
    "        ofproto = datapath.ofproto\n",
    "        parser = datapath.ofproto_parser\n",
    "\n",
    "        # Install table-miss flow entry\n",
    "        match = parser.OFPMatch()\n",
    "        actions = [parser.OFPActionOutput(ofproto.OFPP_CONTROLLER,\n",
    "                                          ofproto.OFPCML_NO_BUFFER)]\n",
    "        self.add_flow(datapath, 0, match, actions)\n",
    "\n",
    "    @set_ev_cls(event.EventSwitchLeave)\n",
    "    def switch_leave_handler(self, ev):\n",
    "        switch = ev.switch\n",
    "        self.network_graph.remove_node(switch.dp.id)\n",
    "        self.logger.info(f\"Switch {switch.dp.id} removed from the network graph\")\n",
    "\n",
    "    @set_ev_cls(event.EventLinkAdd)\n",
    "    def link_add_handler(self, ev):\n",
    "        link = ev.link\n",
    "        src_dp = link.src.dpid\n",
    "        dst_dp = link.dst.dpid\n",
    "        self.network_graph.add_edge(src_dp, dst_dp, port=link.src.port_no)\n",
    "        self.network_graph.add_edge(dst_dp, src_dp, port=link.dst.port_no)\n",
    "        self.logger.info(f\"Link {link} added to the network graph\")\n",
    "\n",
    "    @set_ev_cls(event.EventLinkDelete)\n",
    "    def link_delete_handler(self, ev):\n",
    "        link = ev.link\n",
    "        src_dp = link.src.dpid\n",
    "        dst_dp = link.dst.dpid\n",
    "        self.network_graph.remove_edge(src_dp, dst_dp)\n",
    "        self.network_graph.remove_edge(dst_dp, src_dp)\n",
    "        self.logger.info(f\"Link {link} removed from the network graph\")\n",
    "\n",
    "    def compute_optimal_path(self, src_dp: str, dst_dp: str) -> List[str]:\n",
    "        try:\n",
    "            # Use Dijkstra's algorithm to find the shortest path based on latency\n",
    "            path = nx.shortest_path(self.network_graph, src_dp, dst_dp, weight='latency')\n",
    "            self.logger.info(f\"Optimal path computed: {path}\")\n",
    "            return path\n",
    "        except nx.NetworkXNoPath:\n",
    "            self.logger.error(f\"No path found between {src_dp} and {dst_dp}\")\n",
    "            return None\n",
    "\n",
    "    def update_flow_tables(self, path: List[str], bandwidth: float):\n",
    "        for i in range(len(path) - 1):\n",
    "            src_dp = path[i]\n",
    "            dst_dp = path[i+1]\n",
    "            out_port = self.network_graph[src_dp][dst_dp]['port']\n",
    "            self.add_flow(src_dp, dst_dp, out_port, bandwidth)\n",
    "\n",
    "    def add_flow(self, src_dp: str, dst_dp: str, out_port: int, bandwidth: float):\n",
    "        datapath = self.datapaths[src_dp]\n",
    "        ofproto = datapath.ofproto\n",
    "        parser = datapath.ofproto_parser\n",
    "\n",
    "        match = parser.OFPMatch(in_port=1, eth_dst=dst_dp)\n",
    "        actions = [parser.OFPActionOutput(out_port)]\n",
    "        inst = [parser.OFPInstructionActions(ofproto.OFPIT_APPLY_ACTIONS, actions)]\n",
    "        \n",
    "        mod = parser.OFPFlowMod(\n",
    "            datapath=datapath,\n",
    "            priority=1,\n",
    "            match=match,\n",
    "            instructions=inst,\n",
    "            hard_timeout=300,  # Flow expires after 5 minutes\n",
    "            flags=ofproto.OFPFF_SEND_FLOW_REM  # Send flow removed message when flow expires\n",
    "        )\n",
    "        datapath.send_msg(mod)\n",
    "\n",
    "        # Apply QoS for bandwidth limitation\n",
    "        queue_id = self.create_queue(datapath, out_port, bandwidth)\n",
    "        match = parser.OFPMatch(in_port=1, eth_dst=dst_dp)\n",
    "        actions = [parser.OFPActionSetQueue(queue_id), parser.OFPActionOutput(out_port)]\n",
    "        inst = [parser.OFPInstructionActions(ofproto.OFPIT_APPLY_ACTIONS, actions)]\n",
    "        mod = parser.OFPFlowMod(\n",
    "            datapath=datapath,\n",
    "            priority=2,\n",
    "            match=match,\n",
    "            instructions=inst,\n",
    "            hard_timeout=300,\n",
    "            flags=ofproto.OFPFF_SEND_FLOW_REM\n",
    "        )\n",
    "        datapath.send_msg(mod)\n",
    "\n",
    "\n",
    "    async def handle_migration_request(self, migration_request):\n",
    "        src_host = migration_request.source_host\n",
    "        dst_host = migration_request.destination_host\n",
    "        container = self.migration_manager.decision_engine.resource_optimizer.containers.get(migration_request.container_id)\n",
    "        # Initiate traffic redirection\n",
    "        self.traffic_redirector.initiate_traffic_redirection(\n",
    "            migration_request.container_id, src_host, dst_host\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from typing import Dict, List\n",
    "\n",
    "class SDNControllerConnector:\n",
    "    def __init__(self):\n",
    "        self.network_graph = nx.Graph()\n",
    "        self.flow_rules = {}\n",
    "        self.prepared_changes = {}\n",
    "\n",
    "    def update_flow_rules(self, new_rules: Dict[str, Dict]):\n",
    "        \"\"\"Implement dynamic flow rule management using two-phase commit.\"\"\"\n",
    "        try:\n",
    "            # Phase 1: Prepare\n",
    "            self._prepare_flow_rules(new_rules)\n",
    "            \n",
    "            # Phase 2: Commit\n",
    "            self._commit_flow_rules()\n",
    "            \n",
    "            print(\"Flow rules updated successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating flow rules: {str(e)}\")\n",
    "            self._rollback_flow_rules()\n",
    "\n",
    "    def _prepare_flow_rules(self, new_rules: Dict[str, Dict]):\n",
    "        \"\"\"Prepare flow rule changes on all affected switches.\"\"\"\n",
    "        for switch_id, rules in new_rules.items():\n",
    "            if switch_id not in self.prepared_changes:\n",
    "                self.prepared_changes[switch_id] = {}\n",
    "            for rule_id, rule in rules.items():\n",
    "                self.prepared_changes[switch_id][rule_id] = rule\n",
    "\n",
    "    def _commit_flow_rules(self):\n",
    "        \"\"\"Commit prepared flow rule changes atomically.\"\"\"\n",
    "        for switch_id, rules in self.prepared_changes.items():\n",
    "            if switch_id not in self.flow_rules:\n",
    "                self.flow_rules[switch_id] = {}\n",
    "            self.flow_rules[switch_id].update(rules)\n",
    "        self._apply_flow_rules_to_network()\n",
    "        self.prepared_changes.clear()\n",
    "\n",
    "    def _rollback_flow_rules(self):\n",
    "        \"\"\"Rollback prepared changes in case of failure.\"\"\"\n",
    "        self.prepared_changes.clear()\n",
    "        print(\"Flow rule changes rolled back.\")\n",
    "\n",
    "    def _apply_flow_rules_to_network(self):\n",
    "        \"\"\"Apply committed flow rules to the network.\"\"\"\n",
    "        for switch_id, rules in self.flow_rules.items():\n",
    "            for rule_id, rule in rules.items():\n",
    "                # Here we would interact with the actual SDN controller API\n",
    "                # to apply the rules to the network switches\n",
    "                print(f\"Applying rule {rule_id} to switch {switch_id}\")\n",
    "\n",
    "    def get_optimal_path(self, source: str, destination: str) -> List[str]:\n",
    "        \"\"\"Compute optimal path using network graph.\"\"\"\n",
    "        try:\n",
    "            return nx.shortest_path(self.network_graph, source, destination, weight='latency')\n",
    "        except nx.NetworkXNoPath:\n",
    "            print(f\"No path found between {source} and {destination}\")\n",
    "            return []\n",
    "\n",
    "    def update_network_topology(self, nodes: List[str], edges: List[tuple]):\n",
    "        \"\"\"Update the network topology graph.\"\"\"\n",
    "        self.network_graph.clear()\n",
    "        self.network_graph.add_nodes_from(nodes)\n",
    "        self.network_graph.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNS Controller Manager\n",
    "import dns.update\n",
    "import dns.query\n",
    "import dns.tsigkeyring\n",
    "import dns.resolver\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "\n",
    "class DNSControllerManager:\n",
    "    def __init__(self, dns_server: str, zone: str, ttl: int = 60, key_name: str = None, key_secret: str = None):\n",
    "        self.dns_server = dns_server\n",
    "        self.zone = zone\n",
    "        self.ttl = ttl\n",
    "        self.resolver = dns.resolver.Resolver(nameservers=[dns_server])\n",
    "        self.keyring = None\n",
    "        if key_name and key_secret:\n",
    "            self.keyring = dns.tsigkeyring.from_text({key_name: key_secret})\n",
    "        self.migration_records = {}\n",
    "\n",
    "    def update_dns_record(self, container_id: str, new_ip: str, hostname: str) -> bool:\n",
    "        try:\n",
    "            # Prepare the DNS update message\n",
    "            update = dns.update.Update(self.zone, keyring=self.keyring)\n",
    "            \n",
    "            # Remove the old record if it exists\n",
    "            update.delete(hostname)\n",
    "            \n",
    "            # Add the new A record\n",
    "            update.add(hostname, self.ttl, 'A', new_ip)\n",
    "            \n",
    "            # Send the update to the DNS server\n",
    "            response = dns.query.tcp(update, self.dns_server)\n",
    "            \n",
    "            if response.rcode() == 0:\n",
    "                print(f\"DNS record updated successfully for {hostname}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Failed to update DNS record for {hostname}. RCODE: {response.rcode()}\")\n",
    "                return False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating DNS record: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def prepare_dns_migration(self, container_id: str, source_ip: str, target_ip: str) -> str:\n",
    "        try:\n",
    "            # Get the original hostname for the container\n",
    "            original_hostname = self._get_hostname_for_ip(source_ip)\n",
    "            if not original_hostname:\n",
    "                raise Exception(f\"No hostname found for IP {source_ip}\")\n",
    "\n",
    "            # Create a temporary CNAME\n",
    "            temp_hostname = f\"{container_id}-temp.{self.zone}\"\n",
    "            \n",
    "            # Create CNAME record pointing to the original hostname\n",
    "            update = dns.update.Update(self.zone, keyring=self.keyring)\n",
    "            update.add(temp_hostname, self.ttl, 'CNAME', original_hostname)\n",
    "            \n",
    "            response = dns.query.tcp(update, self.dns_server)\n",
    "            \n",
    "            if response.rcode() == 0:\n",
    "                print(f\"Temporary CNAME {temp_hostname} created successfully\")\n",
    "                \n",
    "                # Store migration information\n",
    "                self.migration_records[container_id] = {\n",
    "                    'original_hostname': original_hostname,\n",
    "                    'temp_hostname': temp_hostname,\n",
    "                    'new_ip': target_ip\n",
    "                }\n",
    "                \n",
    "                return temp_hostname\n",
    "            else:\n",
    "                raise Exception(f\"Failed to create temporary CNAME. RCODE: {response.rcode()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing DNS migration: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def finalize_dns_migration(self, container_id: str) -> bool:\n",
    "        try:\n",
    "            if container_id not in self.migration_records:\n",
    "                raise Exception(f\"No migration record found for container {container_id}\")\n",
    "\n",
    "            migration_info = self.migration_records[container_id]\n",
    "            original_hostname = migration_info['original_hostname']\n",
    "            temp_hostname = migration_info['temp_hostname']\n",
    "            new_ip = migration_info['new_ip']\n",
    "\n",
    "            # Update the original hostname to point to the new IP\n",
    "            update = dns.update.Update(self.zone, keyring=self.keyring)\n",
    "            update.delete(original_hostname, 'A')\n",
    "            update.add(original_hostname, self.ttl, 'A', new_ip)\n",
    "            \n",
    "            # Remove the temporary CNAME\n",
    "            update.delete(temp_hostname, 'CNAME')\n",
    "\n",
    "            response = dns.query.tcp(update, self.dns_server)\n",
    "            \n",
    "            if response.rcode() == 0:\n",
    "                print(f\"DNS migration finalized successfully for {original_hostname}\")\n",
    "                del self.migration_records[container_id]\n",
    "                return True\n",
    "            else:\n",
    "                raise Exception(f\"Failed to finalize DNS migration. RCODE: {response.rcode()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error finalizing DNS migration: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _get_hostname_for_ip(self, ip: str) -> str:\n",
    "        try:\n",
    "            answers = self.resolver.query(dns.reversename.from_address(ip), \"PTR\")\n",
    "            return str(answers[0])\n",
    "        except dns.resolver.NXDOMAIN:\n",
    "            return None\n",
    "\n",
    "    def adjust_ttl(self, hostname: str, new_ttl: int) -> bool:\n",
    "        try:\n",
    "            update = dns.update.Update(self.zone, keyring=self.keyring)\n",
    "            update.replace(hostname, new_ttl, 'A')\n",
    "            \n",
    "            response = dns.query.tcp(update, self.dns_server)\n",
    "            \n",
    "            if response.rcode() == 0:\n",
    "                print(f\"TTL adjusted successfully for {hostname}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Failed to adjust TTL for {hostname}. RCODE: {response.rcode()}\")\n",
    "                return False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adjusting TTL: {str(e)}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import networkx as nx\n",
    "\n",
    "class NetworkTrafficManager:\n",
    "    def __init__(self, network_graph, prediction_window=10, features=4):\n",
    "        self.network_graph = network_graph\n",
    "        self.prediction_window = prediction_window\n",
    "        self.features = features\n",
    "        self.model = self._build_lstm_model()\n",
    "        self.traffic_history = {}\n",
    "        for edge in self.network_graph.edges():\n",
    "            self.traffic_history[edge] = deque(maxlen=prediction_window)\n",
    "\n",
    "    def _build_lstm_model(self):\n",
    "        model = Sequential([\n",
    "            LSTM(64, activation='relu', input_shape=(self.prediction_window, self.features), return_sequences=True),\n",
    "            LSTM(32, activation='relu'),\n",
    "            Dense(self.features)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update_traffic_data(self, edge, traffic_data):\n",
    "        \"\"\"Update traffic data for a specific edge.\"\"\"\n",
    "        self.traffic_history[edge].append(traffic_data)\n",
    "        if len(self.traffic_history[edge]) == self.prediction_window:\n",
    "            self._train_model(edge)\n",
    "\n",
    "    def _train_model(self, edge):\n",
    "        \"\"\"Train the LSTM model with the latest traffic data.\"\"\"\n",
    "        X = np.array(self.traffic_history[edge])\n",
    "        y = X[-1]  # Use the last data point as the target\n",
    "        X = X.reshape((1, self.prediction_window, self.features))\n",
    "        y = y.reshape((1, self.features))\n",
    "        self.model.fit(X, y, epochs=1, verbose=0)\n",
    "\n",
    "    def predict_traffic(self, edge):\n",
    "        \"\"\"Predict traffic for a specific edge.\"\"\"\n",
    "        if len(self.traffic_history[edge]) < self.prediction_window:\n",
    "            return None\n",
    "        X = np.array(self.traffic_history[edge])\n",
    "        X = X.reshape((1, self.prediction_window, self.features))\n",
    "        return self.model.predict(X)[0]\n",
    "\n",
    "    def redirect_traffic(self, container_id, old_ip, new_ip):\n",
    "        \"\"\"Redirect traffic based on predicted network conditions.\"\"\"\n",
    "        old_node = self._get_node_for_ip(old_ip)\n",
    "        new_node = self._get_node_for_ip(new_ip)\n",
    "        \n",
    "        # Find the optimal path based on predicted traffic\n",
    "        path = self._find_optimal_path(old_node, new_node)\n",
    "        \n",
    "        # Update flow rules for traffic redirection\n",
    "        self._update_flow_rules(container_id, path)\n",
    "        \n",
    "        print(f\"Traffic redirected for container {container_id} from {old_ip} to {new_ip}\")\n",
    "\n",
    "    def _get_node_for_ip(self, ip):\n",
    "        \"\"\"Get the network node corresponding to an IP address.\"\"\"\n",
    "        \n",
    "        return ip\n",
    "\n",
    "    def _find_optimal_path(self, source, target):\n",
    "        \"\"\"Find the optimal path based on predicted traffic conditions.\"\"\"\n",
    "        def traffic_weight(u, v, attrs):\n",
    "            edge = (u, v)\n",
    "            predicted_traffic = self.predict_traffic(edge)\n",
    "            if predicted_traffic is None:\n",
    "                return 1  # Default weight if no prediction is available\n",
    "            return np.mean(predicted_traffic)  # Use mean of predicted features as weight\n",
    "\n",
    "        return nx.shortest_path(self.network_graph, source, target, weight=traffic_weight)\n",
    "\n",
    "    async def _update_flow_rules(self, container_id: str, path: List[str]):\n",
    "\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                for i in range(len(path) - 1):\n",
    "                    src_switch = path[i]\n",
    "                    dst_switch = path[i + 1]\n",
    "                    \n",
    "                    # Get the output port for the next switch in the path\n",
    "                    out_port = await self._get_output_port(src_switch, dst_switch)\n",
    "                    \n",
    "                    # Prepare the flow rule\n",
    "                    flow_rule = {\n",
    "                        \"switch\": src_switch,\n",
    "                        \"name\": f\"container_migration_{container_id}_{i}\",\n",
    "                        \"cookie\": \"0\",\n",
    "                        \"priority\": \"32768\",\n",
    "                        \"in_port\": \"any\",\n",
    "                        \"eth_type\": \"0x0800\",  # IPv4\n",
    "                        \"ipv4_dst\": await self._get_container_ip(container_id),\n",
    "                        \"active\": \"true\",\n",
    "                        \"actions\": f\"output={out_port}\"\n",
    "                    }\n",
    "\n",
    "                    # Send the flow rule to the SDN controller\n",
    "                    url = f\"{self.sdn_controller_url}/wm/staticflowpusher/json\"\n",
    "                    async with session.post(url, json=flow_rule) as response:\n",
    "                        if response.status != 200:\n",
    "                            raise Exception(f\"Failed to update flow rule for switch {src_switch}\")\n",
    "                        \n",
    "                print(f\"Successfully updated flow rules for container {container_id} along path: {path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating flow rules for container {container_id}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def _get_output_port(self, src_switch: str, dst_switch: str) -> str:\n",
    "\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                url = f\"{self.sdn_controller_url}/wm/topology/links/json\"\n",
    "                async with session.get(url) as response:\n",
    "                    if response.status != 200:\n",
    "                        raise Exception(\"Failed to retrieve topology information\")\n",
    "                    \n",
    "                    links = await response.json()\n",
    "                    \n",
    "                    for link in links:\n",
    "                        if link['src-switch'] == src_switch and link['dst-switch'] == dst_switch:\n",
    "                            return link['src-port']\n",
    "                    \n",
    "                    raise Exception(f\"No link found between switches {src_switch} and {dst_switch}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting output port: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "\n",
    "    def get_network_metrics(self, source, destination):\n",
    "        \"\"\"Get current network metrics between source and destination.\"\"\"\n",
    "        path = nx.shortest_path(self.network_graph, source, destination)\n",
    "        metrics = {\n",
    "            'latency': 0,\n",
    "            'bandwidth': float('inf'),\n",
    "            'packet_loss': 0\n",
    "        }\n",
    "        \n",
    "        for i in range(len(path) - 1):\n",
    "            edge = (path[i], path[i+1])\n",
    "            edge_metrics = self.network_graph.edges[edge].get('metrics', {})\n",
    "            metrics['latency'] += edge_metrics.get('latency', 0)\n",
    "            metrics['bandwidth'] = min(metrics['bandwidth'], edge_metrics.get('bandwidth', float('inf')))\n",
    "            metrics['packet_loss'] = max(metrics['packet_loss'], edge_metrics.get('packet_loss', 0))\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Synchronizer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import zlib\n",
    "import hashlib\n",
    "import tempfile\n",
    "from typing import Dict, Any\n",
    "import asyncio\n",
    "import aiofiles\n",
    "import docker\n",
    "\n",
    "class CheckpointController:\n",
    "    def __init__(self, container_runtime, checkpoint_dir, compression_level=6):\n",
    "        self.container_runtime = container_runtime\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.compression_level = compression_level\n",
    "        self.docker_client = docker.from_env()\n",
    "\n",
    "    async def create_checkpoint(self, container: Any, checkpoint_type: str = 'full') -> str:\n",
    "        \"\"\"\n",
    "        Creates a checkpoint for the given container.\n",
    "        \n",
    "        Args:\n",
    "            container: The container object to checkpoint.\n",
    "            checkpoint_type: Type of checkpoint ('full' or 'incremental').\n",
    "        \n",
    "        Returns:\n",
    "            The ID of the created checkpoint.\n",
    "        \n",
    "        Raises:\n",
    "            Exception: If checkpoint creation fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            container_id = container.container_id\n",
    "            checkpoint_id = f\"{container_id}_{checkpoint_type}_{int(time.time())}\"\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_id)\n",
    "\n",
    "            # Ensure checkpoint directory exists\n",
    "            os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "            # Prepare checkpoint options\n",
    "            checkpoint_options = [\n",
    "                \"--checkpoint-dir\", checkpoint_path,\n",
    "                \"--leave-running\",  # Keep the container running during checkpoint\n",
    "            ]\n",
    "\n",
    "            if checkpoint_type == 'incremental':\n",
    "                checkpoint_options.append(\"--previous-checkpoint\")\n",
    "                checkpoint_options.append(self._get_latest_checkpoint(container_id))\n",
    "\n",
    "            # Create checkpoint using container runtime\n",
    "            if self.container_runtime == 'docker':\n",
    "                await self._docker_checkpoint(container_id, checkpoint_id, checkpoint_options)\n",
    "            elif self.container_runtime == 'criu':\n",
    "                await self._criu_checkpoint(container_id, checkpoint_path)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported container runtime: {self.container_runtime}\")\n",
    "\n",
    "            # Collect and store container metadata\n",
    "            metadata = await self._collect_container_metadata(container)\n",
    "            await self._store_metadata(checkpoint_path, metadata)\n",
    "\n",
    "            # Implement Copy-on-Write mechanism\n",
    "            await self._implement_cow(checkpoint_path)\n",
    "\n",
    "            # Perform memory page deduplication\n",
    "            await self._deduplicate_memory_pages(checkpoint_path)\n",
    "\n",
    "            print(f\"Checkpoint {checkpoint_id} created successfully\")\n",
    "            return checkpoint_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating checkpoint for container {container.container_id}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def _docker_checkpoint(self, container_id: str, checkpoint_id: str, options: List[str]):\n",
    "        \"\"\"\n",
    "        Creates a checkpoint using Docker.\n",
    "        \"\"\"\n",
    "        cmd = [\"docker\", \"checkpoint\", \"create\"] + options + [container_id, checkpoint_id]\n",
    "        process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n",
    "        stdout, stderr = await process.communicate()\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            raise Exception(f\"Docker checkpoint creation failed: {stderr.decode()}\")\n",
    "\n",
    "    async def _criu_checkpoint(self, container_id: str, checkpoint_path: str):\n",
    "        \"\"\"\n",
    "        Creates a checkpoint using CRIU.\n",
    "        \"\"\"\n",
    "        cmd = [\n",
    "            \"criu\", \"dump\",\n",
    "            \"-t\", container_id,\n",
    "            \"-D\", checkpoint_path,\n",
    "            \"--shell-job\",\n",
    "            \"--leave-running\",\n",
    "            \"--manage-cgroups\"\n",
    "        ]\n",
    "        process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n",
    "        stdout, stderr = await process.communicate()\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            raise Exception(f\"CRIU checkpoint creation failed: {stderr.decode()}\")\n",
    "\n",
    "    async def _collect_container_metadata(self, container: Any) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Collects metadata about the container.\n",
    "        \"\"\"\n",
    "        inspect_data = await self.docker_client.api.inspect_container(container.container_id)\n",
    "        return {\n",
    "            \"id\": container.container_id,\n",
    "            \"name\": inspect_data[\"Name\"],\n",
    "            \"image\": inspect_data[\"Config\"][\"Image\"],\n",
    "            \"env\": inspect_data[\"Config\"][\"Env\"],\n",
    "            \"cmd\": inspect_data[\"Config\"][\"Cmd\"],\n",
    "            \"volumes\": inspect_data[\"Mounts\"],\n",
    "            \"network_settings\": inspect_data[\"NetworkSettings\"],\n",
    "        }\n",
    "\n",
    "    async def _store_metadata(self, checkpoint_path: str, metadata: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Stores container metadata in the checkpoint directory.\n",
    "        \"\"\"\n",
    "        metadata_path = os.path.join(checkpoint_path, \"metadata.json\")\n",
    "        async with aiofiles.open(metadata_path, 'w') as f:\n",
    "            await f.write(json.dumps(metadata, indent=2))\n",
    "\n",
    "    async def _implement_cow(self, checkpoint_path: str):\n",
    "        \"\"\"\n",
    "        Implements Copy-on-Write mechanism for checkpoint files.\n",
    "        \"\"\"\n",
    "        cow_dir = os.path.join(checkpoint_path, \"cow\")\n",
    "        os.makedirs(cow_dir, exist_ok=True)\n",
    "\n",
    "        for root, _, files in os.walk(checkpoint_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(file_path, checkpoint_path)\n",
    "                cow_file_path = os.path.join(cow_dir, relative_path)\n",
    "\n",
    "                # Create hard link to original file\n",
    "                os.link(file_path, cow_file_path)\n",
    "\n",
    "                # Replace original file with copy-on-write version\n",
    "                os.rename(cow_file_path, file_path)\n",
    "\n",
    "    async def _deduplicate_memory_pages(self, checkpoint_path: str):\n",
    "        \"\"\"\n",
    "        Performs memory page deduplication on the checkpoint.\n",
    "        \"\"\"\n",
    "        memory_dump_path = os.path.join(checkpoint_path, \"pages.img\")\n",
    "        if not os.path.exists(memory_dump_path):\n",
    "            return\n",
    "\n",
    "        page_size = 4096  # Assuming 4KB page size\n",
    "        page_hashes = {}\n",
    "\n",
    "        async with aiofiles.open(memory_dump_path, 'rb') as f:\n",
    "            while True:\n",
    "                page = await f.read(page_size)\n",
    "                if not page:\n",
    "                    break\n",
    "                \n",
    "                page_hash = hashlib.md5(page).hexdigest()\n",
    "                if page_hash in page_hashes:\n",
    "                    # Duplicate page found, replace with reference to existing page\n",
    "                    await f.seek(-page_size, 1)  # Move back one page\n",
    "                    await f.write(page_hashes[page_hash].to_bytes(8, byteorder='little'))\n",
    "                else:\n",
    "                    page_hashes[page_hash] = await f.tell() - page_size\n",
    "\n",
    "        print(f\"Memory page deduplication completed. Unique pages: {len(page_hashes)}\")\n",
    "\n",
    "    def _get_latest_checkpoint(self, container_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves the latest checkpoint for a given container.\n",
    "        \"\"\"\n",
    "        checkpoints = [cp for cp in os.listdir(self.checkpoint_dir) if cp.startswith(container_id)]\n",
    "        if not checkpoints:\n",
    "            raise Exception(f\"No previous checkpoints found for container {container_id}\")\n",
    "        return max(checkpoints, key=lambda cp: os.path.getctime(os.path.join(self.checkpoint_dir, cp)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta Tracker\n",
    "\n",
    "import time\n",
    "import mmap\n",
    "import ctypes\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import docker\n",
    "import psutil\n",
    "import struct\n",
    "\n",
    "class DeltaTracker:\n",
    "    def __init__(self):\n",
    "        self.deltas = defaultdict(list)\n",
    "        self.checkpoints = {}\n",
    "        self.memory_maps = {}\n",
    "        self.page_size = mmap.PAGESIZE\n",
    "        self.libc = ctypes.CDLL('libc.so.6')\n",
    "        self.docker_client = docker.from_env()\n",
    "        self.pagemap_fds = {}\n",
    "        self.kpageflags_fd = os.open(\"/proc/kpageflags\", os.O_RDONLY)\n",
    "\n",
    "    def initialize_delta_tracking(self, container_id: str):\n",
    "        \"\"\"Initialize delta tracking for a container.\"\"\"\n",
    "        pid = self._get_container_pid(container_id)\n",
    "        if pid:\n",
    "            self._setup_memory_tracking(container_id, pid)\n",
    "        else:\n",
    "            print(f\"Failed to get PID for container {container_id}\")\n",
    "\n",
    "    def _get_container_pid(self, container_id: str) -> int:\n",
    "        \"\"\"Get the PID of the container's main process using Docker API.\"\"\"\n",
    "        try:\n",
    "            container = self.docker_client.containers.get(container_id)\n",
    "            container_info = container.attrs\n",
    "            pid = container_info['State']['Pid']\n",
    "            return pid\n",
    "        except docker.errors.NotFound:\n",
    "            print(f\"Container {container_id} not found\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting PID for container {container_id}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _setup_memory_tracking(self, container_id: str, pid: int):\n",
    "        \"\"\"Set up memory tracking for a container.\"\"\"\n",
    "        try:\n",
    "            process = psutil.Process(pid)\n",
    "            with process.oneshot():\n",
    "                for mmap_info in process.memory_maps(grouped=False):\n",
    "                    if mmap_info.path == '[anon]':  # Track anonymous memory\n",
    "                        mem_file = f\"/proc/{pid}/mem\"\n",
    "                        fd = os.open(mem_file, os.O_RDONLY)\n",
    "                        mem_map = mmap.mmap(fd, mmap_info.size, mmap.MAP_PRIVATE, mmap.PROT_READ, offset=mmap_info.addr)\n",
    "                        self.memory_maps[container_id] = (mem_map, mmap_info.addr, mmap_info.size)\n",
    "                        os.close(fd)\n",
    "                        break\n",
    "\n",
    "            # Open pagemap file for the process\n",
    "            pagemap_file = f\"/proc/{pid}/pagemap\"\n",
    "            self.pagemap_fds[container_id] = os.open(pagemap_file, os.O_RDONLY)\n",
    "        except psutil.NoSuchProcess:\n",
    "            print(f\"Process {pid} for container {container_id} not found\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up memory tracking for container {container_id}: {str(e)}\")\n",
    "\n",
    "    def track_changes(self, container_id: str):\n",
    "        \"\"\"Track changes in real-time for a container.\"\"\"\n",
    "        if container_id not in self.memory_maps:\n",
    "            print(f\"Memory tracking not set up for container {container_id}\")\n",
    "            return\n",
    "\n",
    "        mem_map, start_addr, size = self.memory_maps[container_id]\n",
    "        dirty_pages = self._get_dirty_pages(container_id, start_addr, size)\n",
    "\n",
    "        for page_addr in dirty_pages:\n",
    "            page_content = mem_map[page_addr - start_addr:page_addr - start_addr + self.page_size]\n",
    "            self.record_delta(container_id, ('memory', page_addr, page_content))\n",
    "\n",
    "    def _get_dirty_pages(self, container_id: str, start_addr: int, size: int) -> List[int]:\n",
    "        \"\"\"Get the list of dirty memory pages for a container using pagemap.\"\"\"\n",
    "        dirty_pages = []\n",
    "        pagemap_fd = self.pagemap_fds[container_id]\n",
    "        \n",
    "        for offset in range(0, size, self.page_size):\n",
    "            virt_addr = start_addr + offset\n",
    "            pagemap_offset = (virt_addr // self.page_size) * 8\n",
    "            os.lseek(pagemap_fd, pagemap_offset, os.SEEK_SET)\n",
    "            pagemap_entry = struct.unpack('Q', os.read(pagemap_fd, 8))[0]\n",
    "            \n",
    "            if pagemap_entry & (1 << 55):  # Check if page is present\n",
    "                pfn = pagemap_entry & ((1 << 55) - 1)  # Get page frame number\n",
    "                kpageflags_offset = pfn * 8\n",
    "                os.lseek(self.kpageflags_fd, kpageflags_offset, os.SEEK_SET)\n",
    "                kpageflags = struct.unpack('Q', os.read(self.kpageflags_fd, 8))[0]\n",
    "                \n",
    "                if kpageflags & (1 << 4):  # Check if page is dirty\n",
    "                    dirty_pages.append(virt_addr)\n",
    "\n",
    "        return dirty_pages\n",
    "\n",
    "    def record_delta(self, container_id: str, delta: Tuple):\n",
    "        \"\"\"Record a delta for a container.\"\"\"\n",
    "        timestamp = time.time()\n",
    "        self.deltas[container_id].append((timestamp, delta))\n",
    "\n",
    "    def create_checkpoint(self, container_id: str) -> str:\n",
    "        \"\"\"Create a checkpoint for a container.\"\"\"\n",
    "        checkpoint_id = f\"{container_id}_{time.time()}\"\n",
    "        self.checkpoints[checkpoint_id] = time.time()\n",
    "        return checkpoint_id\n",
    "\n",
    "    def get_deltas_since_checkpoint(self, container_id: str, checkpoint_id: str) -> List[Tuple]:\n",
    "        \"\"\"Get all deltas for a container since a specific checkpoint.\"\"\"\n",
    "        if checkpoint_id not in self.checkpoints:\n",
    "            raise ValueError(f\"Checkpoint {checkpoint_id} not found\")\n",
    "        \n",
    "        checkpoint_time = self.checkpoints[checkpoint_id]\n",
    "        return [delta for timestamp, delta in self.deltas[container_id] if timestamp > checkpoint_time]\n",
    "\n",
    "    def clear_old_deltas(self, container_id: str, checkpoint_id: str):\n",
    "        \"\"\"Clear deltas older than the specified checkpoint.\"\"\"\n",
    "        if checkpoint_id not in self.checkpoints:\n",
    "            raise ValueError(f\"Checkpoint {checkpoint_id} not found\")\n",
    "        \n",
    "        checkpoint_time = self.checkpoints[checkpoint_id]\n",
    "        self.deltas[container_id] = [(t, d) for t, d in self.deltas[container_id] if t > checkpoint_time]\n",
    "\n",
    "    def remove_checkpoint(self, checkpoint_id: str):\n",
    "        \"\"\"Remove a checkpoint.\"\"\"\n",
    "        if checkpoint_id in self.checkpoints:\n",
    "            del self.checkpoints[checkpoint_id]\n",
    "\n",
    "    def cleanup(self, container_id: str):\n",
    "        \"\"\"Clean up resources for a container.\"\"\"\n",
    "        if container_id in self.memory_maps:\n",
    "            self.memory_maps[container_id][0].close()\n",
    "            del self.memory_maps[container_id]\n",
    "        if container_id in self.pagemap_fds:\n",
    "            os.close(self.pagemap_fds[container_id])\n",
    "            del self.pagemap_fds[container_id]\n",
    "        if container_id in self.deltas:\n",
    "            del self.deltas[container_id]\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup when the object is destroyed.\"\"\"\n",
    "        os.close(self.kpageflags_fd)\n",
    "        for fd in self.pagemap_fds.values():\n",
    "            os.close(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Restoration\n",
    "import shutil\n",
    "\n",
    "import asyncio\n",
    "import aiofiles\n",
    "import os\n",
    "import json\n",
    "import mmap\n",
    "import ctypes\n",
    "import struct\n",
    "import tarfile\n",
    "import docker\n",
    "import resource\n",
    "import fcntl\n",
    "import errno\n",
    "import signal\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from ctypes.util import find_library\n",
    "\n",
    "# Load the C library\n",
    "libc = ctypes.CDLL(find_library('c'), use_errno=True)\n",
    "\n",
    "# Define necessary constants\n",
    "PTRACE_ATTACH = 16\n",
    "PTRACE_DETACH = 17\n",
    "PTRACE_GETREGS = 12\n",
    "PTRACE_SETREGS = 13\n",
    "PTRACE_POKEDATA = 5\n",
    "MAP_SHARED = 0x01\n",
    "PROT_READ = 0x1\n",
    "PROT_WRITE = 0x2\n",
    "MADV_DONTFORK = 10\n",
    "MADV_DOFORK = 11\n",
    "\n",
    "# Define necessary structures\n",
    "class iovec(ctypes.Structure):\n",
    "    _fields_ = [(\"iov_base\", ctypes.c_void_p),\n",
    "                (\"iov_len\", ctypes.c_size_t)]\n",
    "\n",
    "class user_regs_struct(ctypes.Structure):\n",
    "    _fields_ = [\n",
    "        (\"r15\", ctypes.c_ulonglong),\n",
    "        (\"r14\", ctypes.c_ulonglong),\n",
    "        (\"r13\", ctypes.c_ulonglong),\n",
    "        (\"r12\", ctypes.c_ulonglong),\n",
    "        (\"rbp\", ctypes.c_ulonglong),\n",
    "        (\"rbx\", ctypes.c_ulonglong),\n",
    "        (\"r11\", ctypes.c_ulonglong),\n",
    "        (\"r10\", ctypes.c_ulonglong),\n",
    "        (\"r9\", ctypes.c_ulonglong),\n",
    "        (\"r8\", ctypes.c_ulonglong),\n",
    "        (\"rax\", ctypes.c_ulonglong),\n",
    "        (\"rcx\", ctypes.c_ulonglong),\n",
    "        (\"rdx\", ctypes.c_ulonglong),\n",
    "        (\"rsi\", ctypes.c_ulonglong),\n",
    "        (\"rdi\", ctypes.c_ulonglong),\n",
    "        (\"orig_rax\", ctypes.c_ulonglong),\n",
    "        (\"rip\", ctypes.c_ulonglong),\n",
    "        (\"cs\", ctypes.c_ulonglong),\n",
    "        (\"eflags\", ctypes.c_ulonglong),\n",
    "        (\"rsp\", ctypes.c_ulonglong),\n",
    "        (\"ss\", ctypes.c_ulonglong),\n",
    "        (\"fs_base\", ctypes.c_ulonglong),\n",
    "        (\"gs_base\", ctypes.c_ulonglong),\n",
    "        (\"ds\", ctypes.c_ulonglong),\n",
    "        (\"es\", ctypes.c_ulonglong),\n",
    "        (\"fs\", ctypes.c_ulonglong),\n",
    "        (\"gs\", ctypes.c_ulonglong),\n",
    "    ]\n",
    "\n",
    "class StateRestorationModule:\n",
    "    def __init__(self, container_runtime: str, checkpoint_dir: str, max_workers: int = 4):\n",
    "        self.container_runtime = container_runtime\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.docker_client = docker.from_env()\n",
    "        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n",
    "\n",
    "    async def restore_state(self, container_id: str, checkpoint_id: str, destination_host: Any) -> bool:\n",
    "        try:\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_id)\n",
    "            \n",
    "            if not await self._verify_checkpoint(checkpoint_path):\n",
    "                raise Exception(f\"Checkpoint {checkpoint_id} failed integrity check\")\n",
    "\n",
    "            metadata = await self._load_metadata(checkpoint_path)\n",
    "            await self._prepare_container_environment(metadata, destination_host)\n",
    "\n",
    "            if self.container_runtime == 'docker':\n",
    "                await self._docker_restore(container_id, checkpoint_path, metadata)\n",
    "            elif self.container_runtime == 'criu':\n",
    "                await self._criu_restore(container_id, checkpoint_path, metadata)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported container runtime: {self.container_runtime}\")\n",
    "\n",
    "            if not await self._verify_restored_container(container_id, metadata):\n",
    "                raise Exception(f\"Restored container {container_id} failed verification\")\n",
    "\n",
    "            print(f\"Container {container_id} restored successfully from checkpoint {checkpoint_id}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error restoring container {container_id} from checkpoint {checkpoint_id}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    async def _verify_checkpoint(self, checkpoint_path: str) -> bool:\n",
    "        try:\n",
    "            essential_files = ['metadata.json', 'memory.img', 'fs.tar', 'fds.json', 'cpu.regs']\n",
    "            for file in essential_files:\n",
    "                if not os.path.exists(os.path.join(checkpoint_path, file)):\n",
    "                    print(f\"Missing essential file: {file}\")\n",
    "                    return False\n",
    "\n",
    "            metadata = await self._load_metadata(checkpoint_path)\n",
    "            if not all(key in metadata for key in ['id', 'name', 'image', 'env', 'cmd', 'volumes', 'network_settings']):\n",
    "                print(\"Incomplete metadata in checkpoint\")\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error during checkpoint verification: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    async def _load_metadata(self, checkpoint_path: str) -> Dict[str, Any]:\n",
    "        async with aiofiles.open(os.path.join(checkpoint_path, \"metadata.json\"), 'r') as f:\n",
    "            return json.loads(await f.read())\n",
    "\n",
    "    async def _prepare_container_environment(self, metadata: Dict[str, Any], destination_host: Any):\n",
    "        # Ensure the required image is available\n",
    "        await self._ensure_image(metadata['image'], destination_host)\n",
    "\n",
    "        # Set up volumes\n",
    "        volume_tasks = [self._setup_volume(volume, destination_host) for volume in metadata['volumes']]\n",
    "        await asyncio.gather(*volume_tasks)\n",
    "\n",
    "        # Configure network\n",
    "        await self._configure_network(metadata['network_settings'], destination_host)\n",
    "\n",
    "    async def _ensure_image(self, image: str, destination_host: Any):\n",
    "        try:\n",
    "            await self.docker_client.images.pull(image)\n",
    "        except docker.errors.ImageNotFound:\n",
    "            print(f\"Image {image} not found. Attempting to pull...\")\n",
    "            await self.docker_client.images.pull(image)\n",
    "\n",
    "    async def _setup_volume(self, volume: Dict[str, Any], destination_host: Any):\n",
    "        volume_name = volume['Name']\n",
    "        if not await self._volume_exists(volume_name, destination_host):\n",
    "            await self._create_volume(volume_name, destination_host)\n",
    "\n",
    "    async def _volume_exists(self, volume_name: str, destination_host: Any) -> bool:\n",
    "        volumes = await self.docker_client.volumes.list()\n",
    "        return any(v.name == volume_name for v in volumes)\n",
    "\n",
    "    async def _create_volume(self, volume_name: str, destination_host: Any):\n",
    "        await self.docker_client.volumes.create(name=volume_name)\n",
    "\n",
    "    async def _configure_network(self, network_settings: Dict[str, Any], destination_host: Any):\n",
    "        network_name = list(network_settings['Networks'].keys())[0]\n",
    "        if not await self._network_exists(network_name, destination_host):\n",
    "            await self._create_network(network_name, destination_host)\n",
    "\n",
    "    async def _network_exists(self, network_name: str, destination_host: Any) -> bool:\n",
    "        networks = await self.docker_client.networks.list()\n",
    "        return any(n.name == network_name for n in networks)\n",
    "\n",
    "    async def _create_network(self, network_name: str, destination_host: Any):\n",
    "        await self.docker_client.networks.create(name=network_name)\n",
    "\n",
    "    async def _docker_restore(self, container_id: str, checkpoint_path: str, metadata: Dict[str, Any]):\n",
    "        container = await self.docker_client.containers.create(\n",
    "            image=metadata['image'],\n",
    "            command=metadata['cmd'],\n",
    "            environment=metadata['env'],\n",
    "            volumes=metadata['volumes'],\n",
    "            name=metadata['name'],\n",
    "            network=list(metadata['network_settings']['Networks'].keys())[0]\n",
    "        )\n",
    "        await container.start()\n",
    "\n",
    "        pid = await self._get_container_pid(container.id)\n",
    "        if not pid:\n",
    "            raise Exception(f\"Failed to get PID for container {container.id}\")\n",
    "\n",
    "        await self._restore_process_state(pid, checkpoint_path)\n",
    "\n",
    "    async def _criu_restore(self, container_id: str, checkpoint_path: str, metadata: Dict[str, Any]):\n",
    "        cmd = [\n",
    "            \"criu\", \"restore\",\n",
    "            \"--shell-job\",\n",
    "            \"--manage-cgroups\",\n",
    "            \"--restore-detached\",\n",
    "            \"--restore-sibling\",\n",
    "            \"--inherit-fd\", f\"fd[1]:{checkpoint_path}/criu.work\",\n",
    "            \"-D\", checkpoint_path\n",
    "        ]\n",
    "        process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n",
    "        stdout, stderr = await process.communicate()\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            raise Exception(f\"CRIU restore failed: {stderr.decode()}\")\n",
    "\n",
    "        pid = int(stdout.decode().strip())\n",
    "        await self._restore_process_state(pid, checkpoint_path)\n",
    "\n",
    "    async def _restore_process_state(self, pid: int, checkpoint_path: str):\n",
    "        memory_map = await self._map_process_memory(pid)\n",
    "        \n",
    "        tasks = [\n",
    "            self._restore_memory_pages(pid, memory_map, checkpoint_path),\n",
    "            self._restore_filesystem(pid, checkpoint_path),\n",
    "            self._restore_file_descriptors(pid, checkpoint_path)\n",
    "        ]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "        await self._restore_cpu_state(pid, checkpoint_path)\n",
    "\n",
    "    async def _map_process_memory(self, pid: int) -> List[Tuple[int, int, str, str]]:\n",
    "        maps_file = f\"/proc/{pid}/maps\"\n",
    "        async with aiofiles.open(maps_file, 'r') as f:\n",
    "            content = await f.read()\n",
    "        \n",
    "        memory_map = []\n",
    "        for line in content.splitlines():\n",
    "            fields = line.split()\n",
    "            addr_range, perms, offset, dev, inode = fields[:5]\n",
    "            path = fields[5] if len(fields) > 5 else \"\"\n",
    "            \n",
    "            start, end = map(lambda x: int(x, 16), addr_range.split('-'))\n",
    "            memory_map.append((start, end, perms, path))\n",
    "        \n",
    "        return memory_map\n",
    "\n",
    "    async def _restore_memory_pages(self, pid: int, memory_map: List[Tuple[int, int, str, str]], checkpoint_path: str):\n",
    "        pages_file = os.path.join(checkpoint_path, \"memory.img\")\n",
    "        \n",
    "        async with aiofiles.open(pages_file, 'rb') as f:\n",
    "            while True:\n",
    "                header = await f.read(24)  # Assuming 24-byte header\n",
    "                if not header:\n",
    "                    break\n",
    "                vaddr, size = struct.unpack(\"QQ\", header[8:24])\n",
    "                \n",
    "                page_data = await f.read(size)\n",
    "                \n",
    "                for start, end, perms, path in memory_map:\n",
    "                    if start <= vaddr < end:\n",
    "                        if 'w' in perms:\n",
    "                            await self._write_process_memory(pid, vaddr, page_data)\n",
    "                        break\n",
    "\n",
    "    async def _write_process_memory(self, pid: int, address: int, data: bytes):\n",
    "        iov = iovec(ctypes.cast(ctypes.c_char_p(data), ctypes.c_void_p), len(data))\n",
    "        remote_iov = iovec(ctypes.c_void_p(address), len(data))\n",
    "        \n",
    "        libc.process_vm_writev.argtypes = [\n",
    "            ctypes.c_int, ctypes.POINTER(iovec), ctypes.c_ulong,\n",
    "            ctypes.POINTER(iovec), ctypes.c_ulong, ctypes.c_ulong\n",
    "        ]\n",
    "        \n",
    "        result = libc.process_vm_writev(pid, ctypes.byref(iov), 1, ctypes.byref(remote_iov), 1, 0)\n",
    "        if result == -1:\n",
    "            errno_ = ctypes.get_errno()\n",
    "            raise OSError(errno_, f\"Failed to write process memory: {os.strerror(errno_)}\")\n",
    "\n",
    "    async def _restore_filesystem(self, pid: int, checkpoint_path: str):\n",
    "        fs_dump_path = os.path.join(checkpoint_path, \"fs.tar\")\n",
    "        if not os.path.exists(fs_dump_path):\n",
    "            return\n",
    "\n",
    "        root_fd = os.open(f\"/proc/{pid}/root\", os.O_RDONLY)\n",
    "        os.fchdir(root_fd)\n",
    "        os.chroot(\".\")\n",
    "\n",
    "        try:\n",
    "            with tarfile.open(fs_dump_path, 'r') as tar:\n",
    "                tar.extractall(path=\"/\")\n",
    "        finally:\n",
    "            os.fchdir(root_fd)\n",
    "            os.chroot(\".\")\n",
    "            os.close(root_fd)\n",
    "\n",
    "    async def _restore_file_descriptors(self, pid: int, checkpoint_path: str):\n",
    "        fds_file = os.path.join(checkpoint_path, \"fds.json\")\n",
    "        if not os.path.exists(fds_file):\n",
    "            return\n",
    "\n",
    "        async with aiofiles.open(fds_file, 'r') as f:\n",
    "            fds_data = json.loads(await f.read())\n",
    "\n",
    "        for fd, fd_info in fds_data.items():\n",
    "            fd = int(fd)\n",
    "            path = fd_info['path']\n",
    "            flags = fd_info['flags']\n",
    "            \n",
    "            new_fd = os.open(path, flags)\n",
    "            os.dup2(new_fd, fd)\n",
    "            os.close(new_fd)\n",
    "\n",
    "    async def _restore_cpu_state(self, pid: int, checkpoint_path: str):\n",
    "        regs_file = os.path.join(checkpoint_path, \"cpu.regs\")\n",
    "        if not os.path.exists(regs_file):\n",
    "            return\n",
    "\n",
    "        async with aiofiles.open(regs_file, 'rb') as f:\n",
    "            regs_data = await f.read()\n",
    "\n",
    "        regs = user_regs_struct()\n",
    "        ctypes.memmove(ctypes.pointer(regs), regs_data, ctypes.sizeof(regs))\n",
    "\n",
    "        if libc.ptrace(PTRACE_ATTACH, pid, None, None) == -1:\n",
    "            raise Exception(f\"Failed to attach to process {pid}\")\n",
    "\n",
    "        try:\n",
    "            _, status = os.waitpid(pid, 0)\n",
    "            if os.WIFSTOPPED(status):\n",
    "                if libc.ptrace(PTRACE_SETREGS, pid, None, ctypes.byref(regs)) == -1:\n",
    "                    raise Exception(f\"Failed to set registers for process {pid}\")\n",
    "        finally:\n",
    "            libc.ptrace(PTRACE_DETACH, pid, None, None)\n",
    "\n",
    "    async def _get_container_pid(self, container_id: str) -> int:\n",
    "        container = self.docker_client.containers.get(container_id)\n",
    "        return container.attrs['State']['Pid']\n",
    "\n",
    "    async def _verify_restored_container(self, container_id: str, metadata: Dict[str, Any]) -> bool:\n",
    "        try:\n",
    "            container = self.docker_client.containers.get(container_id)\n",
    "            \n",
    "            if container.status != 'running':\n",
    "                print(f\"Restored container is not running. Current status: {container.status}\")\n",
    "                return False\n",
    "\n",
    "            inspect_data = container.attrs\n",
    "\n",
    "            if inspect_data['Config']['Image'] != metadata['image']:\n",
    "                print(f\"Image mismatch. Expected: {metadata['image']}, Actual: {inspect_data['Config']['Image']}\")\n",
    "                return False\n",
    "\n",
    "            if inspect_data['Config']['Cmd'] != metadata['cmd']:\n",
    "                print(f\"Command mismatch. Expected: {metadata['cmd']}, Actual: {inspect_data['Config']['Cmd']}\")\n",
    "                return False\n",
    "\n",
    "            if set(inspect_data['Config']['Env']) != set(metadata['env']):\n",
    "                print(\"Environment variables mismatch\")\n",
    "                return False\n",
    "\n",
    "            if len(inspect_data['Mounts']) != len(metadata['volumes']):\n",
    "                print(\"Volume configuration mismatch\")\n",
    "                return False\n",
    "\n",
    "            if inspect_data['NetworkSettings']['Networks'].keys() != metadata['network_settings']['Networks'].keys():\n",
    "                print(\"Network configuration mismatch\")\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error verifying restored container: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# Helper function to run the restoration process\n",
    "async def restore_container(container_id: str, checkpoint_id: str, container_runtime: str, checkpoint_dir: str):\n",
    "    restorer = StateRestorationModule(container_runtime, checkpoint_dir)\n",
    "    success = await restorer.restore_state(container_id, checkpoint_id, None)  # Assuming local restoration\n",
    "    if success:\n",
    "        print(f\"Container {container_id} restored successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to restore container {container_id}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinates the migration process, integrating all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlexiMigrate Framework\n",
    "## Integrates all components into a cohesive framework.\n",
    "\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "class FlexiMigrate:\n",
    "    def __init__(self, policies):\n",
    "        self.performance_metrics_collector = PerformanceMetricsCollector()\n",
    "        self.resource_utilization_analyzer = ResourceUtilizationAnalyzer(thresholds={\n",
    "            'cpu_threshold': 80,\n",
    "            'memory_threshold': 70\n",
    "        })\n",
    "        self.policy_enforcer = PolicyEnforcer(policies)\n",
    "        self.workload_analyzer = WorkloadAnalyzer()\n",
    "        self.resource_optimizer = ResourceOptimizer()\n",
    "        self.network_manager = NetworkManager()\n",
    "        self.migration_planner = MigrationPlanner(self.network_manager)\n",
    "        self.decision_engine = DecisionEngine(\n",
    "            workload_analyzer=self.workload_analyzer,\n",
    "            resource_optimizer=self.resource_optimizer,\n",
    "            migration_planner=self.migration_planner,\n",
    "            policies=policies,\n",
    "            performance_metrics_collector=self.performance_metrics_collector\n",
    "        )\n",
    "        self.checkpointing_module = CheckpointingModule(container_runtime='docker', checkpoint_dir='/tmp/checkpoints')\n",
    "        self.delta_tracker = DeltaTracker()\n",
    "        self.state_restoration_module = StateRestorationModule(container_runtime='docker', checkpoint_dir='/tmp/checkpoints')\n",
    "        self.state_synchronizer = StateSynchronizer(\n",
    "            checkpointing_module=self.checkpointing_module,\n",
    "            delta_tracker=self.delta_tracker,\n",
    "            state_restoration_module=self.state_restoration_module,\n",
    "            network_manager=self.network_manager,\n",
    "            resource_monitor=self.performance_metrics_collector\n",
    "        )\n",
    "        self.logging_monitoring = LoggingAndMonitoringModule()\n",
    "        self.migration_strategy_selector = MigrationStrategySelector(\n",
    "            performance_metrics_collector=self.performance_metrics_collector,\n",
    "            network_manager=self.network_manager\n",
    "        )\n",
    "        self.migration_manager = MigrationManager(\n",
    "            decision_engine=self.decision_engine,\n",
    "            state_synchronizer=self.state_synchronizer,\n",
    "            migration_strategy_selector=self.migration_strategy_selector,\n",
    "            logging_monitoring=self.logging_monitoring,\n",
    "            network_manager=self.network_manager\n",
    "        )\n",
    "        self.container_runtime_interface = ContainerRuntimeInterface()\n",
    "        self.nested_container_manager = NestedContainerManager(\n",
    "            runtime_interface=self.container_runtime_interface,\n",
    "            state_synchronizer=self.state_synchronizer,\n",
    "            network_manager=self.network_manager\n",
    "        )\n",
    "        self.image_manager = ImageManager(self.container_runtime_interface)\n",
    "        self.network_manager.set_migration_manager(self.migration_manager)\n",
    "        self.network_orchestrator = NetworkOrchestrator(migration_coordinator=self.migration_manager)\n",
    "\n",
    "    async def run(self):\n",
    "        # Start Prometheus metrics server\n",
    "        start_http_server(8000)\n",
    "        self.logging_monitoring.log(\"FlexiMigrate monitoring started\")\n",
    "\n",
    "        # Start Network Orchestrator\n",
    "        await self.network_orchestrator.start()\n",
    "\n",
    "        # Start monitoring and migration loops\n",
    "        monitoring_task = asyncio.create_task(self._monitoring_loop())\n",
    "        migration_task = asyncio.create_task(self._migration_loop())\n",
    "\n",
    "        await asyncio.gather(monitoring_task, migration_task)\n",
    "\n",
    "    async def _monitoring_loop(self):\n",
    "        while True:\n",
    "            # Update Host Metrics\n",
    "            for host in self.decision_engine.resource_optimizer.hosts.values():\n",
    "                await self.performance_metrics_collector.update_host_metrics(host)\n",
    "                over, under = await self.resource_utilization_analyzer.analyze_host_utilization(host)\n",
    "                if over:\n",
    "                    self.logging_monitoring.log(f\"Host {host.host_id} is overutilized\")\n",
    "                if under:\n",
    "                    self.logging_monitoring.log(f\"Host {host.host_id} is underutilized\")\n",
    "\n",
    "            # Update Container Metrics\n",
    "            for host in self.decision_engine.resource_optimizer.hosts.values():\n",
    "                for container in host.containers:\n",
    "                    await self.performance_metrics_collector.update_container_metrics(container)\n",
    "                    needs_migration = await self.resource_utilization_analyzer.analyze_container_utilization(container)\n",
    "                    if needs_migration:\n",
    "                        best_host = await self.resource_optimizer.select_best_host(self.resource_optimizer.hosts, container)\n",
    "                        if best_host:\n",
    "                            migration_request = MigrationRequest(\n",
    "                                container_id=container.container_id,\n",
    "                                source_host=host,\n",
    "                                destination_host=best_host,\n",
    "                                migration_type=await self.migration_strategy_selector.select_strategy(\n",
    "                                    container,\n",
    "                                    host,\n",
    "                                    best_host\n",
    "                                )\n",
    "                            )\n",
    "                            if await self.decision_engine.policy_enforcer.enforce_policies(migration_request):\n",
    "                                await self.migration_manager.add_migration_request(migration_request)\n",
    "                                await self.network_orchestrator.handle_migration_request(migration_request)\n",
    "            await asyncio.sleep(10)  # Adjust the monitoring interval as needed\n",
    "\n",
    "    async def _migration_loop(self):\n",
    "        while True:\n",
    "            await self.migration_manager.process_migrations()\n",
    "            await asyncio.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [\n",
    "    {\n",
    "        'policy_name': 'adaptive_load_balancing',\n",
    "        'CONTEXT': ['source_cpu_utilization', 'destination_cpu_utilization', 'time_of_day', 'network_congestion_prob', 'service_type'],\n",
    "        'CONDITIONS': '(source_cpu_utilization > 80 and destination_cpu_utilization < 50) or '\n",
    "                      '(time_of_day >= 18 and time_of_day <= 22 and service_type == \"critical\") or '\n",
    "                      '(network_congestion_prob < 0.2)',\n",
    "        'ACTIONS': ['allow_migration', 'set_priority(\"high\")', 'trigger_load_balancer_reconfiguration'],\n",
    "        'CONSTRAINTS': {'max_concurrent_migrations': 5, 'migration_duration': 300},\n",
    "        'PRIORITY': 2\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    policies = [\n",
    "        {\n",
    "            'policy_name': 'adaptive_load_balancing',\n",
    "            'CONTEXT': ['source_cpu_utilization', 'destination_cpu_utilization', 'time_of_day', 'network_congestion_prob', 'service_type'],\n",
    "            'CONDITIONS': '(source_cpu_utilization > 80 and destination_cpu_utilization < 50) or '\n",
    "                          '(time_of_day >= 18 and time_of_day <= 22 and service_type == \"critical\") or '\n",
    "                          '(network_congestion_prob < 0.2)',\n",
    "            'ACTIONS': ['allow_migration', 'set_priority(\"high\")', 'trigger_load_balancer_reconfiguration'],\n",
    "            'CONSTRAINTS': {'max_concurrent_migrations': 5, 'migration_duration': 300},\n",
    "            'PRIORITY': 2\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    flexi_migrate = FlexiMigrate(policies=policies)\n",
    "    asyncio.run(flexi_migrate.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "host1 = Host(host_id='host1', total_cpu=16, total_memory=32768, total_storage=1000)\n",
    "host2 = Host(host_id='host2', total_cpu=16, total_memory=32768, total_storage=1000)\n",
    "\n",
    "flexi_migrate.decision_engine.resource_optimizer.hosts = {\n",
    "    host1.host_id: host1,\n",
    "    host2.host_id: host2\n",
    "}\n",
    "\n",
    "container1 = Container(container_id='container1', image='nginx:latest', cpu_limit=4, memory_limit=2048, storage_limit=50)\n",
    "container1.host = 'host1'\n",
    "host1.containers.append(container1)\n",
    "\n",
    "flexi_migrate.decision_engine.resource_optimizer.containers = {\n",
    "    container1.container_id: container1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MigrationRequest:\n",
    "    def __init__(self, container_id, source_host, destination_host, migration_type, priority=1):\n",
    "        self.container_id = container_id\n",
    "        self.source_host = source_host\n",
    "        self.destination_host = destination_host\n",
    "        self.migration_type = migration_type\n",
    "        self.priority = priority\n",
    "        self.state = MigrationState.PENDING\n",
    "\n",
    "migration_request = MigrationRequest(\n",
    "    container_id='container1',\n",
    "    source_host=flexi_migrate.resource_optimizer.hosts['host1'],\n",
    "    destination_host=flexi_migrate.resource_optimizer.hosts['host2'],\n",
    "    migration_type=MigrationStrategy.LIVE_MIGRATION\n",
    ")\n",
    "\n",
    "if flexi_migrate.decision_engine.policy_enforcer.enforce_policies(migration_request):\n",
    "    flexi_migrate.migration_manager.add_migration_request(migration_request)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
