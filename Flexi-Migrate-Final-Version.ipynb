{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "FlexiMigrate is a sophisticated framework designed to facilitate efficient and seamless live container migration across diverse cloud and edge computing environments. It addresses the limitations of existing migration schemes by introducing a modular, orchestrator-agnostic architecture, enhanced state management, adaptive migration strategies, and robust network orchestration.\n",
    "\n",
    "This implementation closely follows the architecture and design principles outlined in the research paper \"Enhancing Cloud and Edge Computing with FlexiMigrate: A Seamless Approach to Live Container Migration,\" ensuring fidelity to the proposed structure and functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Architecture\n",
    "Below is a high-level diagram representing the architecture of the FlexiMigrate framework, showcasing the interactions between various components:\n",
    "\n",
    "# FlexiMigrate Framework\n",
    "- Migration Manager\n",
    "  - Migration Coordinator\n",
    "  - Policy Enforcer\n",
    "  - Resource Allocator\n",
    "  - Network Orchestrator\n",
    "  - State Management Interface\n",
    "  - Logging & Monitoring\n",
    "  - Migration Strategy Selector\n",
    "- Resource Monitor\n",
    "  - Performance Metrics Collector\n",
    "  - Resource Utilization Analyzer\n",
    "- Decision Engine\n",
    "  - Workload Analyzer\n",
    "  - Resource Optimizer\n",
    "  - Migration Planner\n",
    "- Container Manager\n",
    "  - Runtime Controller\n",
    "  - Nested Container Manager\n",
    "  - Image Manager\n",
    "- Network Manager\n",
    "  - SDN Controller Interface\n",
    "  - DNS Manager\n",
    "  - Traffic Redirector\n",
    "- State Synchronizer\n",
    "  - Checkpointing Module\n",
    "  - Delta Transfer\n",
    "  - State Restoration Module\n",
    "\n",
    "Each component is meticulously implemented to fulfill its designated role within the framework, ensuring seamless integration and efficient operation during container migrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Framework\n",
    "\n",
    "To run the FlexiMigrate framework, follow these steps:\n",
    "\n",
    "1. **Install Required Dependencies**: Ensure that all necessary Python packages are installed. You can install them using pip:\n",
    "\n",
    "   `pip install elasticsearch os-ken docker prometheus_client  bsdiff4 networkx grpcio tensorflow`\n",
    "\n",
    "2. **Configure Container Runtimes and SDN Controllers**:\n",
    "   - Docker: Ensure Docker is installed and running on all host machines.\n",
    "   - SDN Controllers: Deploy and configure SDN controllers like os_ken.\n",
    "\n",
    "3. **Define Migration Policies**: Define policies that govern migration behaviors.\n",
    "\n",
    "   `policies = [`\n",
    "     `{`\n",
    "       `'policy_name': 'adaptive_load_balancing',`\n",
    "       `'CONTEXT': ['source_cpu_utilization', 'destination_cpu_utilization', 'time_of_day', 'network_congestion_prob', 'service_type'],`\n",
    "       `'CONDITIONS': '(source_cpu_utilization > 80 and destination_cpu_utilization < 50) or '`\n",
    "                     `'(time_of_day >= 18 and time_of_day <= 22 and service_type == \"critical\") or '`\n",
    "                     `'(network_congestion_prob < 0.2)',`\n",
    "       `'ACTIONS': ['allow_migration', 'set_priority(\"high\")', 'trigger_load_balancer_reconfiguration'],`\n",
    "       `'CONSTRAINTS': {'max_concurrent_migrations': 5, 'migration_duration': 300},`\n",
    "       `'PRIORITY': 2`\n",
    "     `}`\n",
    "   `]`\n",
    "\n",
    "4. **Initialize and Run FlexiMigrate**:\n",
    "\n",
    "   `if __name__ == '__main__':`\n",
    "       `flexi_migrate = FlexiMigrate(policies=policies)`\n",
    "       `flexi_migrate.run()`\n",
    "\n",
    "5. **Interact with the Framework**:\n",
    "   \n",
    "   - Adding Hosts and Containers:\n",
    "\n",
    "     `host1 = Host(host_id='host1', total_cpu=16, total_memory=32768, total_storage=1000)`\n",
    "     `host2 = Host(host_id='host2', total_cpu=16, total_memory=32768, total_storage=1000)`\n",
    "     `flexi_migrate.migration_engine.resource_optimizer.hosts = {`\n",
    "         `host1.host_id: host1,`\n",
    "         `host2.host_id: host2`\n",
    "     `}`\n",
    "\n",
    "     `container1 = Container(container_id='container1', image='nginx:latest', cpu_limit=4, memory_limit=2048, storage_limit=50)`\n",
    "     `container1.host = 'host1'`\n",
    "     `host1.containers.append(container1)`\n",
    "     `flexi_migrate.migration_engine.resource_optimizer.containers = {`\n",
    "         `container1.container_id: container1`\n",
    "     `}`\n",
    "\n",
    "   - Requesting Migrations:\n",
    "\n",
    "     `migration_request = MigrationRequest(`\n",
    "         `container_id='container1',`\n",
    "         `source_host=flexi_migrate.resource_optimizer.hosts['host1'],`\n",
    "         `destination_host=flexi_migrate.resource_optimizer.hosts['host2'],`\n",
    "         `migration_type=MigrationStrategy.LIVE_MIGRATION`\n",
    "     `)`\n",
    "     `if flexi_migrate.decision_engine.enforce_policies(migration_request):`\n",
    "         `flexi_migrate.migration_manager.add_migration_request(migration_request)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enums for Migration States and Strategies\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class MigrationState(Enum):\n",
    "    PENDING = 1\n",
    "    PLANNING = 2\n",
    "    PREPARATION = 3\n",
    "    EXECUTION = 4\n",
    "    VERIFICATION = 5\n",
    "    COMPLETED = 6\n",
    "    FAILED = 7\n",
    "    ROLLBACK = 8\n",
    "\n",
    "class MigrationStrategy(Enum):\n",
    "    COLD_MIGRATION = 1\n",
    "    PRE_COPY = 2\n",
    "    POST_COPY = 3\n",
    "    LIVE_MIGRATION = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Classes\n",
    "\n",
    "class Container:\n",
    "    def __init__(self, container_id, image, cpu_limit, memory_limit, storage_limit):\n",
    "        self.container_id = container_id\n",
    "        self.image = image\n",
    "        self.cpu_limit = cpu_limit\n",
    "        self.memory_limit = memory_limit\n",
    "        self.storage_limit = storage_limit\n",
    "        self.state = None\n",
    "        self.host = None\n",
    "\n",
    "class Host:\n",
    "    def __init__(self, host_id, total_cpu, total_memory, total_storage):\n",
    "        self.host_id = host_id\n",
    "        self.total_cpu = total_cpu\n",
    "        self.total_memory = total_memory\n",
    "        self.total_storage = total_storage\n",
    "        self.available_cpu = total_cpu\n",
    "        self.available_memory = total_memory\n",
    "        self.available_storage = total_storage\n",
    "        self.containers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migration Manager Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MigrationManager:\n",
    "    def __init__(self, decision_engine, state_synchronizer, migration_strategy_selector, logging_monitoring, network_manager):\n",
    "        self.decision_engine = decision_engine\n",
    "        self.state_synchronizer = state_synchronizer\n",
    "        self.migration_strategy_selector = migration_strategy_selector\n",
    "        self.logging_monitoring = logging_monitoring\n",
    "        self.network_manager = network_manager\n",
    "        self.migration_requests = []\n",
    "        self.docker_client = docker.from_env()\n",
    "        \n",
    "    \n",
    "        # Initialize a graph to represent the cluster topology\n",
    "        self.cluster_topology = nx.Graph()\n",
    "        \n",
    "        # Cache to store host information\n",
    "        self.host_cache = {}\n",
    "        \n",
    "        # Threshold for host selection (in percentage)\n",
    "        self.cpu_threshold = 80\n",
    "        self.memory_threshold = 80\n",
    "        self.network_threshold = 70\n",
    "\n",
    "    async def get_current_host(self, container: Any) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Determines the current host of a given container.\n",
    "\n",
    "        Args:\n",
    "            container: The container object.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing information about the current host.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the current host cannot be determined.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First, try to get information from Docker\n",
    "            container_info = self.docker_client.containers.get(container.id)\n",
    "            node_id = container_info.attrs['Node']['ID']\n",
    "            \n",
    "            if node_id in self.host_cache:\n",
    "                return self.host_cache[node_id]\n",
    "            \n",
    "            node_info = self.docker_client.nodes.get(node_id)\n",
    "            host_info = {\n",
    "                'id': node_id,\n",
    "                'name': node_info.attrs['Description']['Hostname'],\n",
    "                'address': node_info.attrs['Status']['Addr'],\n",
    "                'architecture': node_info.attrs['Description']['Platform']['Architecture'],\n",
    "                'os': node_info.attrs['Description']['Platform']['OS'],\n",
    "                'resources': node_info.attrs['Description']['Resources'],\n",
    "            }\n",
    "            \n",
    "            # Cache the host information\n",
    "            self.host_cache[node_id] = host_info\n",
    "            \n",
    "            return host_info\n",
    "\n",
    "        except docker.errors.NotFound:\n",
    "\n",
    "                raise Exception(f\"Unable to determine current host for container {container.id}\")\n",
    "\n",
    "    async def get_potential_hosts(self, container: Any) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Identifies potential hosts for migrating a given container.\n",
    "\n",
    "        Args:\n",
    "            container: The container object to be migrated.\n",
    "\n",
    "        Returns:\n",
    "            A list of dictionaries, each containing information about a potential host.\n",
    "        \"\"\"\n",
    "        potential_hosts = []\n",
    "        current_host = await self.get_current_host(container)\n",
    "        \n",
    "        # Get all nodes in the cluster\n",
    "        try:\n",
    "            nodes = self.docker_client.nodes.list()\n",
    "        except client.exceptions.ApiException:\n",
    "            nodes = self.docker_client.nodes.list()\n",
    "        \n",
    "        for node in nodes:\n",
    "            # Skip the current host\n",
    "            if self._is_same_host(node, current_host):\n",
    "                continue\n",
    "            \n",
    "            host_info = self._get_host_info(node)\n",
    "            \n",
    "            # Check if the host meets the resource requirements\n",
    "            if self._meets_resource_requirements(host_info, container):\n",
    "                # Check network conditions\n",
    "                network_suitable = await self._check_network_conditions(current_host, host_info)\n",
    "                \n",
    "                if network_suitable:\n",
    "                    potential_hosts.append(host_info)\n",
    "        \n",
    "        # Sort potential hosts based on suitability\n",
    "        sorted_hosts = await self._sort_hosts_by_suitability(potential_hosts, container)\n",
    "        \n",
    "        return sorted_hosts\n",
    "\n",
    "    def _is_same_host(self, node: Any, current_host: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if a given node is the same as the current host.\n",
    "        \"\"\"\n",
    "        if isinstance(node, client.models.v1_node.V1Node):\n",
    "            return node.metadata.uid == current_host['id']\n",
    "        else:  # Docker node\n",
    "            return node.id == current_host['id']\n",
    "\n",
    "    def _get_host_info(self, node: Any) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extracts relevant information from a node object.\n",
    "        \"\"\"\n",
    "        if isinstance(node, client.models.v1_node.V1Node):\n",
    "            return {\n",
    "                'id': node.metadata.uid,\n",
    "                'name': node.metadata.name,\n",
    "                'address': node.status.addresses[0].address,\n",
    "                'architecture': node.status.node_info.architecture,\n",
    "                'os': node.status.node_info.os_image,\n",
    "                'resources': {\n",
    "                    'NanoCPUs': int(node.status.capacity['cpu']) * 1e9,\n",
    "                    'MemoryBytes': int(node.status.capacity['memory'].rstrip('Ki')) * 1024,\n",
    "                },\n",
    "            }\n",
    "        else:  # Docker node\n",
    "            return {\n",
    "                'id': node.id,\n",
    "                'name': node.attrs['Description']['Hostname'],\n",
    "                'address': node.attrs['Status']['Addr'],\n",
    "                'architecture': node.attrs['Description']['Platform']['Architecture'],\n",
    "                'os': node.attrs['Description']['Platform']['OS'],\n",
    "                'resources': node.attrs['Description']['Resources'],\n",
    "            }\n",
    "\n",
    "    def _meets_resource_requirements(self, host: Dict[str, Any], container: Any) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if a host meets the resource requirements of a container.\n",
    "        \"\"\"\n",
    "        # Get container resource requirements\n",
    "        container_info = self.docker_client.containers.get(container.id)\n",
    "        container_cpu = container_info.attrs['HostConfig']['NanoCpus']\n",
    "        container_memory = container_info.attrs['HostConfig']['Memory']\n",
    "        \n",
    "        # Check available resources on the host\n",
    "        host_cpu_available = host['resources']['NanoCPUs'] * (100 - self.cpu_threshold) / 100\n",
    "        host_memory_available = host['resources']['MemoryBytes'] * (100 - self.memory_threshold) / 100\n",
    "        \n",
    "        return container_cpu <= host_cpu_available and container_memory <= host_memory_available\n",
    "\n",
    "    async def _check_network_conditions(self, source_host: Dict[str, Any], destination_host: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if network conditions are suitable for migration between two hosts.\n",
    "        \"\"\"\n",
    "        network_metrics = await self.network_manager.get_network_metrics(source_host['id'], destination_host['id'])\n",
    "        \n",
    "        # Check if bandwidth utilization is below threshold\n",
    "        bandwidth_utilization = network_metrics['bandwidth_utilization']\n",
    "        return bandwidth_utilization <= self.network_threshold\n",
    "\n",
    "    async def _sort_hosts_by_suitability(self, hosts: List[Dict[str, Any]], container: Any) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Sorts potential hosts based on their suitability for the given container.\n",
    "        \"\"\"\n",
    "        host_scores = []\n",
    "        \n",
    "        for host in hosts:\n",
    "            score = await self._calculate_host_suitability(host, container)\n",
    "            host_scores.append((host, score))\n",
    "        \n",
    "        # Sort hosts by score in descending order\n",
    "        sorted_hosts = [host for host, score in sorted(host_scores, key=lambda x: x[1], reverse=True)]\n",
    "        \n",
    "        return sorted_hosts\n",
    "\n",
    "    async def _calculate_host_suitability(self, host: Dict[str, Any], container: Any) -> float:\n",
    "        \"\"\"\n",
    "        Calculates a suitability score for a host with respect to a container.\n",
    "        \"\"\"\n",
    "        # Get container resource requirements\n",
    "        container_info = self.docker_client.containers.get(container.id)\n",
    "        container_cpu = container_info.attrs['HostConfig']['NanoCpus']\n",
    "        container_memory = container_info.attrs['HostConfig']['Memory']\n",
    "        \n",
    "        # Calculate resource availability scores\n",
    "        cpu_score = (host['resources']['NanoCPUs'] - container_cpu) / host['resources']['NanoCPUs']\n",
    "        memory_score = (host['resources']['MemoryBytes'] - container_memory) / host['resources']['MemoryBytes']\n",
    "        \n",
    "        # Get network conditions\n",
    "        network_metrics = await self.network_manager.get_network_metrics(container.attrs['Node']['ID'], host['id'])\n",
    "        network_score = 1 - (network_metrics['bandwidth_utilization'] / 100)\n",
    "        \n",
    "        # Calculate overall score (you can adjust weights as needed)\n",
    "        overall_score = 0.4 * cpu_score + 0.4 * memory_score + 0.2 * network_score\n",
    "        \n",
    "        return overall_score\n",
    "    \n",
    "    async def process_migration(self, migration_request):\n",
    "        try:\n",
    "            migration_request.state = MigrationState.PLANNING\n",
    "            plan = await self.decision_engine.plan_migration(migration_request)\n",
    "            \n",
    "            migration_request.state = MigrationState.PREPARATION\n",
    "            await self.state_synchronizer.prepare_migration(migration_request, plan)\n",
    "            \n",
    "            migration_request.state = MigrationState.EXECUTION\n",
    "            success = await self.state_synchronizer.perform_migration(migration_request, plan)\n",
    "            \n",
    "            if success:\n",
    "                migration_request.state = MigrationState.VERIFICATION\n",
    "                verified = await self.state_synchronizer.verify_migration(migration_request)\n",
    "                \n",
    "                if verified:\n",
    "                    migration_request.state = MigrationState.COMPLETED\n",
    "                else:\n",
    "                    raise Exception(\"Migration verification failed\")\n",
    "            else:\n",
    "                raise Exception(\"Migration execution failed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            migration_request.state = MigrationState.FAILED\n",
    "            await self.handle_migration_failure(migration_request, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Policy Enforcer\n",
    "import time\n",
    "from prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n",
    "\n",
    "class PolicyEnforcer:\n",
    "    def __init__(self, policies):\n",
    "        self.policies = policies  # List of policy dictionaries\n",
    "\n",
    "    def enforce_policies(self, migration_request):\n",
    "        for policy in self.policies:\n",
    "            context = policy['CONTEXT']\n",
    "            conditions = policy['CONDITIONS']\n",
    "            actions = policy['ACTIONS']\n",
    "            constraints = policy['CONSTRAINTS']\n",
    "            priority = policy['PRIORITY']\n",
    "            # Evaluate conditions based on context\n",
    "            condition_met = self.evaluate_conditions(conditions, migration_request)\n",
    "            if condition_met:\n",
    "                self.execute_actions(actions, migration_request)\n",
    "                if 'rollback' in actions:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def evaluate_conditions(self, conditions: str, migration_request: Any) -> bool:\n",
    "\n",
    "        env = self.get_evaluation_environment(migration_request)\n",
    "        try:\n",
    "            return eval(conditions, {}, env)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating conditions: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_evaluation_environment(self, migration_request: Any) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'source_cpu_utilization': self.get_cpu_utilization(migration_request.source_host),\n",
    "            'destination_cpu_utilization': self.get_cpu_utilization(migration_request.destination_host),\n",
    "            'time_of_day': int(time.strftime(\"%H\")),\n",
    "            'network_congestion_prob': self.get_network_congestion_probability(migration_request),\n",
    "            'service_type': self.get_service_type(migration_request.container_id)\n",
    "        }\n",
    "\n",
    "    def get_cpu_utilization(self, host: Any) -> float:\n",
    "        return (host.total_cpu - host.available_cpu) / host.total_cpu * 100\n",
    "\n",
    "    def get_network_congestion_probability(self, migration_request: Any) -> float:\n",
    "        source_host = migration_request.source_host\n",
    "        destination_host = migration_request.destination_host\n",
    "\n",
    "        # Get network metrics from the network manager\n",
    "        bandwidth_usage = self.network_manager.get_bandwidth_usage(source_host, destination_host)\n",
    "        packet_loss = self.network_manager.get_packet_loss(source_host, destination_host)\n",
    "        latency = self.network_manager.get_latency(source_host, destination_host)\n",
    "\n",
    "\n",
    "        congestion_prob = (bandwidth_usage / 100 + packet_loss + latency / 1000) / 3\n",
    "\n",
    "        # Ensure the probability is between 0 and 1\n",
    "        congestion_prob = max(0, min(1, congestion_prob))\n",
    "\n",
    "        # Update Prometheus metric\n",
    "        self.network_congestion_gauge.set(congestion_prob)\n",
    "        push_to_gateway(self.prometheus_gateway, job='network_congestion', registry=self.registry)\n",
    "\n",
    "        return congestion_prob\n",
    "\n",
    "    def execute_actions(self, actions, migration_request):\n",
    "        for action in actions:\n",
    "            \n",
    "            if action == 'allow_migration':\n",
    "                migration_request.state = MigrationState.PLANNING\n",
    "                continue  # No action needed; migration is allowed\n",
    "            elif action.startswith('set_priority'):\n",
    "                # Extract priority level\n",
    "                priority = action.split('(')[1].rstrip(')')\n",
    "                migration_request.priority = priority\n",
    "            elif action.startswith('trigger_load_balancer_reconfiguration'):\n",
    "                self.trigger_load_balancer_reconfiguration(migration_request)\n",
    "            elif action == 'rollback':\n",
    "                migration_request.state = MigrationState.ROLLBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoggingAndMonitoring\n",
    "import json\n",
    "import logging\n",
    "from elasticsearch import Elasticsearch\n",
    "from logstash import TCPLogstashHandler\n",
    "from prometheus_client import start_http_server, Gauge\n",
    "import time\n",
    "import requests\n",
    "\n",
    "class LoggingAndMonitoringModule:\n",
    "    def __init__(self, elasticsearch_host='localhost', elasticsearch_port=9200, \n",
    "                 logstash_host='localhost', logstash_port=5000,\n",
    "                 kibana_host='localhost', kibana_port=5601,\n",
    "                 grafana_host='localhost', grafana_port=3000,\n",
    "                 prometheus_port=9090):\n",
    "        # Initialize Elasticsearch client\n",
    "        self.es = Elasticsearch([f'http://{elasticsearch_host}:{elasticsearch_port}'])\n",
    "        \n",
    "        # Initialize Logger\n",
    "        self.logger = logging.getLogger('FlexiMigrate')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Add Logstash handler\n",
    "        logstash_handler = TCPLogstashHandler(logstash_host, logstash_port, version=1)\n",
    "        self.logger.addHandler(logstash_handler)\n",
    "        \n",
    "        # Initialize Prometheus metrics\n",
    "        self.init_prometheus_metrics(prometheus_port)\n",
    "        \n",
    "        # Store visualization tool URLs\n",
    "        self.kibana_url = f'http://{kibana_host}:{kibana_port}'\n",
    "        self.grafana_url = f'http://{grafana_host}:{grafana_port}'\n",
    "\n",
    "    def init_prometheus_metrics(self, prometheus_port):\n",
    "        # Start Prometheus HTTP server\n",
    "        start_http_server(prometheus_port)\n",
    "        \n",
    "        # Define Prometheus metrics\n",
    "        self.migration_count = Gauge('fleximigrate_migration_count', 'Number of migrations')\n",
    "        self.migration_duration = Gauge('fleximigrate_migration_duration', 'Duration of migrations')\n",
    "        self.resource_utilization = Gauge('fleximigrate_resource_utilization', 'Resource utilization', ['resource_type'])\n",
    "\n",
    "    def log_event(self, event_type, message, additional_data=None):\n",
    "        log_entry = {\n",
    "            'timestamp': time.time(),\n",
    "            'event_type': event_type,\n",
    "            'message': message,\n",
    "            'additional_data': additional_data or {}\n",
    "        }\n",
    "        \n",
    "        # Log to Elasticsearch\n",
    "        self.es.index(index='fleximigrate-logs', body=log_entry)\n",
    "        \n",
    "        # Log using Python logger (which will send to Logstash)\n",
    "        self.logger.info(json.dumps(log_entry))\n",
    "\n",
    "    def update_prometheus_metrics(self, metric_name, value, labels=None):\n",
    "        if metric_name == 'migration_count':\n",
    "            self.migration_count.inc(value)\n",
    "        elif metric_name == 'migration_duration':\n",
    "            self.migration_duration.set(value)\n",
    "        elif metric_name == 'resource_utilization':\n",
    "            self.resource_utilization.labels(resource_type=labels['resource_type']).set(value)\n",
    "\n",
    "    def create_kibana_dashboard(self, dashboard_name):\n",
    "        dashboard_url = f'{self.kibana_url}/app/kibana#/dashboard/{dashboard_name}'\n",
    "        print(f\"Kibana dashboard created: {dashboard_url}\")\n",
    "\n",
    "    def create_grafana_dashboard(self, dashboard_name):\n",
    "        dashboard_url = f'{self.grafana_url}/d/{dashboard_name}'\n",
    "        print(f\"Grafana dashboard created: {dashboard_url}\")\n",
    "\n",
    "    def analyze_logs(self, query):\n",
    "        # Perform a search query on Elasticsearch\n",
    "        results = self.es.search(index='fleximigrate-logs', body=query)\n",
    "        return results['hits']['hits']\n",
    "\n",
    "    def get_metric_data(self, metric_name, time_range):\n",
    "        query = f'{metric_name}[{time_range}]'\n",
    "        response = requests.get(f'http://localhost:9090/api/v1/query', params={'query': query})\n",
    "        return response.json()['data']['result']\n",
    "\n",
    "\n",
    "    def generate_report(self, start_time, end_time):\n",
    "        log_data = self.analyze_logs({\n",
    "            \"query\": {\n",
    "                \"range\": {\n",
    "                    \"timestamp\": {\n",
    "                        \"gte\": start_time,\n",
    "                        \"lte\": end_time\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        metric_data = self.get_metric_data('fleximigrate_migration_count', f'{end_time - start_time}s')\n",
    "        \n",
    "        report = {\n",
    "            'log_summary': log_data,\n",
    "            'metric_summary': metric_data\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Usage example:\n",
    "monitoring = LoggingAndMonitoringModule()\n",
    "\n",
    "# Log an event\n",
    "monitoring.log_event('migration_started', 'Container migration initiated', {'container_id': 'abc123'})\n",
    "\n",
    "# Update a Prometheus metric\n",
    "monitoring.update_prometheus_metrics('migration_count', 1)\n",
    "\n",
    "# Create dashboards\n",
    "monitoring.create_kibana_dashboard('fleximigrate-overview')\n",
    "monitoring.create_grafana_dashboard('fleximigrate-metrics')\n",
    "\n",
    "# Analyze logs\n",
    "log_analysis = monitoring.analyze_logs({\"query\": {\"match\": {\"event_type\": \"migration_started\"}}})\n",
    "\n",
    "# Get metric data\n",
    "metric_data = monitoring.get_metric_data('fleximigrate_migration_count', '1h')\n",
    "\n",
    "# Set up an alert\n",
    "monitoring.alert_on_condition(monitoring.migration_count.get() > 100, \"High number of migrations detected\")\n",
    "\n",
    "# Generate a report\n",
    "report = monitoring.generate_report(time.time() - 3600, time.time())  # Last hour\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Manager\n",
    "class StateSynchronizer:\n",
    "    def __init__(self, checkpointing_module, delta_tracker, state_restoration_module, network_manager, resource_monitor):\n",
    "        self.checkpointing_module = checkpointing_module\n",
    "        self.delta_tracker = delta_tracker\n",
    "        self.state_restoration_module = state_restoration_module\n",
    "        self.network_manager = network_manager\n",
    "        self.resource_monitor = resource_monitor\n",
    "        self.migration_readiness_threshold = 0.8  # Threshold for migration readiness (0-1)\n",
    "        self.max_retry_attempts = 5\n",
    "        self.retry_delay = 5  # seconds\n",
    "\n",
    "    def perform_migration(self, container, source_host, destination_host, strategy):\n",
    "        try:\n",
    "            # Step 1: Create an initial checkpoint\n",
    "            checkpoint_id = self.checkpointing_module.create_checkpoint(container, checkpoint_type='full')\n",
    "            print(f\"Created initial checkpoint: {checkpoint_id}\")\n",
    "\n",
    "            # Step 2: Transfer the initial checkpoint to the destination\n",
    "            self.checkpointing_module.optimize_checkpoint(checkpoint_id)\n",
    "            self.transfer_checkpoint(checkpoint_id, source_host, destination_host)\n",
    "\n",
    "            # Step 3: Start delta tracking\n",
    "            self.delta_tracker.initialize_delta_tracking(container.container_id)\n",
    "\n",
    "            # Step 4: Capture and transfer deltas iteratively\n",
    "            retry_count = 0\n",
    "            while not self._migration_is_ready(container, source_host, destination_host):\n",
    "                if retry_count >= self.max_retry_attempts:\n",
    "                    raise Exception(\"Maximum retry attempts reached. Migration aborted.\")\n",
    "                \n",
    "                deltas = self.delta_tracker.get_deltas_since_checkpoint(container.container_id, checkpoint_id)\n",
    "                if deltas:\n",
    "                    self.transfer_deltas(deltas, source_host, destination_host)\n",
    "                \n",
    "                retry_count += 1\n",
    "                time.sleep(self.retry_delay)\n",
    "\n",
    "            # Step 5: Final checkpoint and transfer\n",
    "            final_checkpoint_id = self.checkpointing_module.create_checkpoint(container, checkpoint_type='incremental')\n",
    "            self.checkpointing_module.optimize_checkpoint(final_checkpoint_id)\n",
    "            self.transfer_checkpoint(final_checkpoint_id, source_host, destination_host)\n",
    "\n",
    "            # Step 6: Restore the container at the destination\n",
    "            success = self.state_restoration_module.restore_state(container, final_checkpoint_id, destination_host)\n",
    "            if success:\n",
    "                print(f\"Container {container.container_id} restored successfully on {destination_host.host_id}.\")\n",
    "                container.state = MigrationState.COMPLETED\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Failed to restore container {container.container_id} on {destination_host.host_id}.\")\n",
    "                container.state = MigrationState.FAILED\n",
    "                return False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Migration failed: {e}\")\n",
    "            container.state = MigrationState.FAILED\n",
    "            return False\n",
    "\n",
    "    def _migration_is_ready(self, container: Any, source_host: Any, destination_host: Any) -> bool:\n",
    "        if container.state != MigrationState.PREPARATION:\n",
    "            return False\n",
    "        \n",
    "        \"\"\"\n",
    "        Determines if the migration is ready to proceed based on various factors.\n",
    "        \n",
    "        Args:\n",
    "            container: The container being migrated.\n",
    "            source_host: The source host of the migration.\n",
    "            destination_host: The destination host of the migration.\n",
    "        \n",
    "        Returns:\n",
    "            True if the migration is ready to proceed, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check network conditions\n",
    "            network_metrics = self.network_manager.get_network_metrics(source_host.host_id, destination_host.host_id)\n",
    "            bandwidth_utilization = network_metrics['current_bandwidth'] / network_metrics['max_bandwidth']\n",
    "            latency = network_metrics['latency']\n",
    "\n",
    "            # Check resource availability on destination host\n",
    "            dest_resources = self.resource_monitor.get_host_resources(destination_host.host_id)\n",
    "            cpu_availability = dest_resources['available_cpu'] / dest_resources['total_cpu']\n",
    "            memory_availability = dest_resources['available_memory'] / dest_resources['total_memory']\n",
    "\n",
    "            # Check container state\n",
    "            container_state = self.checkpointing_module.get_container_state(container.container_id)\n",
    "            delta_size = self.delta_tracker.get_total_delta_size(container.container_id)\n",
    "            \n",
    "            # Calculate migration readiness score\n",
    "            readiness_score = self._calculate_readiness_score(\n",
    "                bandwidth_utilization,\n",
    "                latency,\n",
    "                cpu_availability,\n",
    "                memory_availability,\n",
    "                container_state,\n",
    "                delta_size\n",
    "            )\n",
    "\n",
    "            print(f\"Migration readiness score: {readiness_score}\")\n",
    "\n",
    "            return readiness_score >= self.migration_readiness_threshold\n",
    "        \n",
    "        \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in migration readiness check: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _calculate_readiness_score(self, \n",
    "                                   bandwidth_utilization: float, \n",
    "                                   latency: float, \n",
    "                                   cpu_availability: float, \n",
    "                                   memory_availability: float, \n",
    "                                   container_state: str, \n",
    "                                   delta_size: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculates a readiness score based on various metrics.\n",
    "        \n",
    "        Args:\n",
    "            bandwidth_utilization: Current bandwidth utilization (0-1).\n",
    "            latency: Network latency in milliseconds.\n",
    "            cpu_availability: Available CPU on destination host (0-1).\n",
    "            memory_availability: Available memory on destination host (0-1).\n",
    "            container_state: Current state of the container.\n",
    "            delta_size: Size of accumulated deltas since last checkpoint.\n",
    "        \n",
    "        Returns:\n",
    "            A float representing the readiness score (0-1).\n",
    "        \"\"\"\n",
    "        # Define weights for each factor\n",
    "        weights = {\n",
    "            'bandwidth': 0.25,\n",
    "            'latency': 0.2,\n",
    "            'cpu': 0.15,\n",
    "            'memory': 0.15,\n",
    "            'container_state': 0.15,\n",
    "            'delta_size': 0.1\n",
    "        }\n",
    "\n",
    "        # Normalize inputs\n",
    "        normalized_bandwidth = 1 - bandwidth_utilization  # Higher is better\n",
    "        normalized_latency = 1 / (1 + latency / 100)  # Transform to 0-1 range, lower latency is better\n",
    "        normalized_delta_size = 1 / (1 + delta_size / 1e6)  # Transform to 0-1 range, smaller delta is better\n",
    "\n",
    "        # Score container state\n",
    "        state_scores = {'running': 1.0, 'paused': 0.8, 'restarting': 0.5, 'exited': 0.2}\n",
    "        container_state_score = state_scores.get(container_state, 0)\n",
    "\n",
    "        # Calculate weighted score\n",
    "        score = (\n",
    "            weights['bandwidth'] * normalized_bandwidth +\n",
    "            weights['latency'] * normalized_latency +\n",
    "            weights['cpu'] * cpu_availability +\n",
    "            weights['memory'] * memory_availability +\n",
    "            weights['container_state'] * container_state_score +\n",
    "            weights['delta_size'] * normalized_delta_size\n",
    "        )\n",
    "\n",
    "        return score\n",
    "\n",
    "    async def transfer_checkpoint(self, checkpoint_id: str, source_host: Any, destination_host: Any):\n",
    "        \"\"\"\n",
    "        Transfers a checkpoint from the source host to the destination host asynchronously.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_id: The ID of the checkpoint to transfer.\n",
    "            source_host: The source host object.\n",
    "            destination_host: The destination host object.\n",
    "        \n",
    "        Raises:\n",
    "            Exception: If there's an error during the transfer process.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            checkpoint_path = os.path.join(source_host.checkpoint_dir, checkpoint_id)\n",
    "            destination_path = os.path.join(destination_host.checkpoint_dir, checkpoint_id)\n",
    "\n",
    "            # Ensure the destination directory exists\n",
    "            os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "\n",
    "            # Calculate the total size of the checkpoint\n",
    "            total_size = os.path.getsize(checkpoint_path)\n",
    "\n",
    "            # Create a progress bar\n",
    "            progress_bar = tqdm(total=total_size, unit='B', unit_scale=True, desc=f\"Transferring checkpoint {checkpoint_id}\")\n",
    "\n",
    "            # Use aiofiles for asynchronous file I/O\n",
    "            async with aiofiles.open(checkpoint_path, 'rb') as source_file:\n",
    "                async with aiofiles.open(destination_path, 'wb') as dest_file:\n",
    "                    while True:\n",
    "                        chunk = await source_file.read(8192)  # 8KB chunks\n",
    "                        if not chunk:\n",
    "                            break\n",
    "                        await dest_file.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "\n",
    "            progress_bar.close()\n",
    "\n",
    "            # Verify the transfer\n",
    "            if not await self._verify_transfer(checkpoint_path, destination_path):\n",
    "                raise Exception(\"Checkpoint transfer verification failed\")\n",
    "\n",
    "            print(f\"Checkpoint {checkpoint_id} transferred successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error transferring checkpoint {checkpoint_id}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def transfer_deltas(self, deltas: List[Any], source_host: Any, destination_host: Any):\n",
    "        \"\"\"\n",
    "        Transfers delta changes from the source host to the destination host asynchronously.\n",
    "        \n",
    "        Args:\n",
    "            deltas: List of delta objects to transfer.\n",
    "            source_host: The source host object.\n",
    "            destination_host: The destination host object.\n",
    "        \n",
    "        Raises:\n",
    "            Exception: If there's an error during the transfer process.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            total_delta_size = sum(delta.size for delta in deltas)\n",
    "            progress_bar = tqdm(total=total_delta_size, unit='B', unit_scale=True, desc=\"Transferring deltas\")\n",
    "\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                for delta in deltas:\n",
    "                    delta_path = os.path.join(source_host.delta_dir, delta.id)\n",
    "                    destination_url = f\"http://{destination_host.address}:{destination_host.port}/receive_delta\"\n",
    "\n",
    "                    async with aiofiles.open(delta_path, 'rb') as delta_file:\n",
    "                        delta_data = await delta_file.read()\n",
    "\n",
    "                    async with session.post(destination_url, data=delta_data) as response:\n",
    "                        if response.status != 200:\n",
    "                            raise Exception(f\"Failed to transfer delta {delta.id}: {await response.text()}\")\n",
    "\n",
    "                    progress_bar.update(delta.size)\n",
    "\n",
    "            progress_bar.close()\n",
    "\n",
    "            print(f\"All deltas transferred successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error transferring deltas: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def _verify_transfer(self, source_path: str, destination_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Verifies the integrity of the transferred file using SHA256 hash.\n",
    "        \n",
    "        Args:\n",
    "            source_path: Path to the source file.\n",
    "            destination_path: Path to the destination file.\n",
    "        \n",
    "        Returns:\n",
    "            True if the transfer is verified, False otherwise.\n",
    "        \"\"\"\n",
    "        async def calculate_hash(file_path):\n",
    "            hash_sha256 = hashlib.sha256()\n",
    "            async with aiofiles.open(file_path, 'rb') as f:\n",
    "                while chunk := await f.read(8192):\n",
    "                    hash_sha256.update(chunk)\n",
    "            return hash_sha256.hexdigest()\n",
    "\n",
    "        source_hash = await calculate_hash(source_path)\n",
    "        destination_hash = await calculate_hash(destination_path)\n",
    "\n",
    "        return source_hash == destination_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MigrationStrategySelector\n",
    "\n",
    "class MigrationStrategySelector:\n",
    "    def __init__(self, performance_metrics_collector, network_manager):\n",
    "        self.performance_metrics_collector = performance_metrics_collector\n",
    "        self.network_manager = network_manager\n",
    "        self.strategy_weights = {\n",
    "            'memory_usage': 0.3,\n",
    "            'cpu_usage': 0.2,\n",
    "            'network_bandwidth': 0.2,\n",
    "            'disk_io': 0.1,\n",
    "            'container_size': 0.1,\n",
    "            'downtime_tolerance': 0.1\n",
    "        }\n",
    "\n",
    "    def select_strategy(self, container: Any, source_host: Any, destination_host: Any) -> MigrationStrategy:\n",
    "        \"\"\"\n",
    "        Selects the most appropriate migration strategy based on container and environment characteristics.\n",
    "\n",
    "        Args:\n",
    "            container: The container object to be migrated.\n",
    "            source_host: The source host object.\n",
    "            destination_host: The destination host object.\n",
    "\n",
    "        Returns:\n",
    "            MigrationStrategy: The selected migration strategy.\n",
    "        \"\"\"\n",
    "        # Collect relevant metrics\n",
    "        container_metrics = self.performance_metrics_collector.get_container_metrics(container.id)\n",
    "        network_metrics = self.network_manager.get_network_metrics(source_host.id, destination_host.id)\n",
    "        \n",
    "        # Calculate scores for each strategy\n",
    "        cold_score = self._calculate_cold_migration_score(container_metrics, network_metrics)\n",
    "        pre_copy_score = self._calculate_pre_copy_score(container_metrics, network_metrics)\n",
    "        post_copy_score = self._calculate_post_copy_score(container_metrics, network_metrics)\n",
    "        live_score = self._calculate_hybrid_score(container_metrics, network_metrics)\n",
    "\n",
    "        # Select the strategy with the highest score\n",
    "        scores = {\n",
    "            MigrationStrategy.COLD_MIGRATION: cold_score,\n",
    "            MigrationStrategy.PRE_COPY: pre_copy_score,\n",
    "            MigrationStrategy.POST_COPY: post_copy_score,\n",
    "            MigrationStrategy.LIVE_MIGRATION: live_score\n",
    "        }\n",
    "\n",
    "        selected_strategy = max(scores, key=scores.get)\n",
    "        \n",
    "        self._log_strategy_selection(container.id, scores, selected_strategy)\n",
    "        \n",
    "        return selected_strategy\n",
    "\n",
    "    def _calculate_cold_migration_score(self, container_metrics: Dict[str, float], network_metrics: Dict[str, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the score for cold migration strategy.\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        score += (1 - container_metrics['memory_usage']) * self.strategy_weights['memory_usage']\n",
    "        score += (1 - container_metrics['cpu_usage']) * self.strategy_weights['cpu_usage']\n",
    "        score += network_metrics['bandwidth'] / 1000 * self.strategy_weights['network_bandwidth']  # Assuming bandwidth is in Mbps\n",
    "        score += (1 - container_metrics['disk_io']) * self.strategy_weights['disk_io']\n",
    "        score += (1 - container_metrics['container_size'] / 10000) * self.strategy_weights['container_size']  # Assuming size is in MB\n",
    "        score += container_metrics['downtime_tolerance'] * self.strategy_weights['downtime_tolerance']\n",
    "        return score\n",
    "\n",
    "    def _calculate_pre_copy_score(self, container_metrics: Dict[str, float], network_metrics: Dict[str, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the score for pre-copy migration strategy.\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        score += container_metrics['memory_usage'] * self.strategy_weights['memory_usage']\n",
    "        score += (1 - container_metrics['cpu_usage']) * self.strategy_weights['cpu_usage']\n",
    "        score += network_metrics['bandwidth'] / 1000 * self.strategy_weights['network_bandwidth']\n",
    "        score += (1 - container_metrics['disk_io']) * self.strategy_weights['disk_io']\n",
    "        score += (1 - container_metrics['container_size'] / 10000) * self.strategy_weights['container_size']\n",
    "        score += (1 - container_metrics['downtime_tolerance']) * self.strategy_weights['downtime_tolerance']\n",
    "        return score\n",
    "\n",
    "    def _calculate_post_copy_score(self, container_metrics: Dict[str, float], network_metrics: Dict[str, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the score for post-copy migration strategy.\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        score += (1 - container_metrics['memory_usage']) * self.strategy_weights['memory_usage']\n",
    "        score += container_metrics['cpu_usage'] * self.strategy_weights['cpu_usage']\n",
    "        score += network_metrics['bandwidth'] / 1000 * self.strategy_weights['network_bandwidth']\n",
    "        score += container_metrics['disk_io'] * self.strategy_weights['disk_io']\n",
    "        score += container_metrics['container_size'] / 10000 * self.strategy_weights['container_size']\n",
    "        score += (1 - container_metrics['downtime_tolerance']) * self.strategy_weights['downtime_tolerance']\n",
    "        return score\n",
    "\n",
    "    def _calculate_hybrid_score(self, container_metrics: Dict[str, float], network_metrics: Dict[str, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the score for Live migration strategy.\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        score += container_metrics['memory_usage'] * self.strategy_weights['memory_usage']\n",
    "        score += container_metrics['cpu_usage'] * self.strategy_weights['cpu_usage']\n",
    "        score += network_metrics['bandwidth'] / 1000 * self.strategy_weights['network_bandwidth']\n",
    "        score += container_metrics['disk_io'] * self.strategy_weights['disk_io']\n",
    "        score += container_metrics['container_size'] / 10000 * self.strategy_weights['container_size']\n",
    "        score += (1 - container_metrics['downtime_tolerance']) * self.strategy_weights['downtime_tolerance']\n",
    "        return score\n",
    "\n",
    "    def _log_strategy_selection(self, container_id: str, scores: Dict[MigrationStrategy, float], selected_strategy: MigrationStrategy):\n",
    "        \"\"\"\n",
    "        Logs the strategy selection process for analysis and debugging.\n",
    "        \"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': time.time(),\n",
    "            'container_id': container_id,\n",
    "            'scores': {str(strategy): score for strategy, score in scores.items()},\n",
    "            'selected_strategy': str(selected_strategy)\n",
    "        }\n",
    "      \n",
    "        print(f\"Strategy Selection Log: {log_entry}\")\n",
    "\n",
    "    def update_strategy_weights(self, new_weights: Dict[str, float]):\n",
    "        \"\"\"\n",
    "        Updates the weights used for strategy selection.\n",
    "        \"\"\"\n",
    "        if sum(new_weights.values()) != 1.0:\n",
    "            raise ValueError(\"The sum of weights must be 1.0\")\n",
    "        self.strategy_weights.update(new_weights)\n",
    "\n",
    "    def analyze_migration_performance(self, migration_history: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Analyzes past migration performance to potentially adjust strategy selection.\n",
    "        \"\"\"\n",
    "        strategy_performance = {strategy: [] for strategy in MigrationStrategy}\n",
    "        \n",
    "        for migration in migration_history:\n",
    "            strategy = migration['strategy']\n",
    "            performance = migration['performance_score']\n",
    "            strategy_performance[strategy].append(performance)\n",
    "        \n",
    "        for strategy, performances in strategy_performance.items():\n",
    "            if performances:\n",
    "                avg_performance = np.mean(performances)\n",
    "                print(f\"Average performance for {strategy}: {avg_performance}\")\n",
    "                \n",
    "                # Adjust weights based on performance\n",
    "                if avg_performance < 0.5:  # Assuming performance score is between 0 and 1\n",
    "                    self._decrease_strategy_weight(strategy)\n",
    "                elif avg_performance > 0.8:\n",
    "                    self._increase_strategy_weight(strategy)\n",
    "\n",
    "    def _decrease_strategy_weight(self, strategy: MigrationStrategy):\n",
    "        \"\"\"\n",
    "        Decreases the weight of factors favoring the given strategy.\n",
    "        \"\"\"\n",
    "        if strategy == MigrationStrategy.COLD_MIGRATION:\n",
    "            self.strategy_weights['downtime_tolerance'] *= 0.9\n",
    "        elif strategy == MigrationStrategy.PRE_COPY:\n",
    "            self.strategy_weights['memory_usage'] *= 0.9\n",
    "        elif strategy == MigrationStrategy.POST_COPY:\n",
    "            self.strategy_weights['cpu_usage'] *= 0.9\n",
    "        elif strategy == MigrationStrategy.LIVE_MIGRATION:\n",
    "            self.strategy_weights['network_bandwidth'] *= 0.9\n",
    "        \n",
    "        self._normalize_weights()\n",
    "\n",
    "    def _increase_strategy_weight(self, strategy: MigrationStrategy):\n",
    "        \"\"\"\n",
    "        Increases the weight of factors favoring the given strategy.\n",
    "        \"\"\"\n",
    "        if strategy == MigrationStrategy.COLD_MIGRATION:\n",
    "            self.strategy_weights['downtime_tolerance'] *= 1.1\n",
    "        elif strategy == MigrationStrategy.PRE_COPY:\n",
    "            self.strategy_weights['memory_usage'] *= 1.1\n",
    "        elif strategy == MigrationStrategy.POST_COPY:\n",
    "            self.strategy_weights['cpu_usage'] *= 1.1\n",
    "        elif strategy == MigrationStrategy.LIVE_MIGRATION:\n",
    "            self.strategy_weights['network_bandwidth'] *= 1.1\n",
    "        \n",
    "        self._normalize_weights()\n",
    "\n",
    "    def _normalize_weights(self):\n",
    "        \"\"\"\n",
    "        Normalizes the strategy weights to ensure they sum to 1.\n",
    "        \"\"\"\n",
    "        total_weight = sum(self.strategy_weights.values())\n",
    "        self.strategy_weights = {k: v / total_weight for k, v in self.strategy_weights.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetworkOrchestrator\n",
    "\n",
    "import asyncio\n",
    "import ipaddress\n",
    "from typing import Dict, Any\n",
    "import docker\n",
    "from os_ken.app.simple_switch_13 import SimpleSwitch13\n",
    "from os_ken.controller import ofp_event\n",
    "from os_ken.controller.handler import MAIN_DISPATCHER, set_ev_cls\n",
    "from os_ken.ofproto import ofproto_v1_3\n",
    "from os_ken.lib.packet import packet, ethernet, arp, ipv4\n",
    "\n",
    "class NetworkOrchestrator(SimpleSwitch13):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NetworkOrchestrator, self).__init__(*args, **kwargs)\n",
    "        self.migration_coordinator = kwargs.get('migration_coordinator')\n",
    "        self.active_migrations = {}\n",
    "        self.docker_client = docker.from_env()\n",
    "        config.load_kube_config()\n",
    "        self.network_policies = {}\n",
    "\n",
    "    async def handle_network_changes(self, migration_request: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Handles network changes during container migration.\n",
    "\n",
    "        Args:\n",
    "            migration_request: A dictionary containing migration details.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if network changes were handled successfully, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            container_id = migration_request['container_id']\n",
    "            source_host = migration_request['source_host']\n",
    "            destination_host = migration_request['destination_host']\n",
    "\n",
    "            # Step 1: Prepare the network on the destination host\n",
    "            await self._prepare_destination_network(container_id, destination_host)\n",
    "\n",
    "            # Step 2: Set up tunneling between source and destination hosts\n",
    "            tunnel_id = await self._setup_tunnel(source_host, destination_host)\n",
    "\n",
    "            # Step 3: Update SDN flow rules for traffic redirection\n",
    "            await self._update_sdn_flow_rules(container_id, source_host, destination_host)\n",
    "\n",
    "            # Step 4: Update DNS records\n",
    "            await self._update_dns_records(container_id, destination_host)\n",
    "\n",
    "            # Step 5: Apply network policies on the destination host\n",
    "            await self._apply_network_policies(container_id, destination_host)\n",
    "\n",
    "            # Step 6: Handle service mesh configuration (if applicable)\n",
    "            await self._update_service_mesh_config(container_id, source_host, destination_host)\n",
    "\n",
    "            # Store migration details for cleanup\n",
    "            self.active_migrations[container_id] = {\n",
    "                'source_host': source_host,\n",
    "                'destination_host': destination_host,\n",
    "                'tunnel_id': tunnel_id\n",
    "            }\n",
    "\n",
    "            self.logger.info(f\"Network changes handled successfully for container {container_id}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error handling network changes for container {container_id}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    async def _prepare_destination_network(self, container_id: str, destination_host: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Prepares the network on the destination host for the migrating container.\n",
    "        \"\"\"\n",
    "        container = self.docker_client.containers.get(container_id)\n",
    "        network_settings = container.attrs['NetworkSettings']\n",
    "\n",
    "        for network_name, network_config in network_settings['Networks'].items():\n",
    "            # Create the network on the destination host if it doesn't exist\n",
    "            try:\n",
    "                self.docker_client.networks.get(network_name)\n",
    "            except docker.errors.NotFound:\n",
    "                self.docker_client.networks.create(\n",
    "                    name=network_name,\n",
    "                    driver=network_config.get('Driver', 'bridge'),\n",
    "                    ipam=network_config.get('IPAM', None)\n",
    "                )\n",
    "\n",
    "            # Reserve the same IP address for the container on the destination host\n",
    "            if 'IPAddress' in network_config:\n",
    "                await self._reserve_ip_address(network_name, network_config['IPAddress'], destination_host)\n",
    "\n",
    "    async def _setup_tunnel(self, source_host: Dict[str, Any], destination_host: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Sets up a network tunnel between source and destination hosts.\n",
    "        \"\"\"\n",
    "        tunnel_id = f\"mig_tunnel_{source_host['id']}_{destination_host['id']}\"\n",
    "        \n",
    "        # Set up VXLAN tunnel\n",
    "        cmd = f\"ip link add {tunnel_id} type vxlan id 100 remote {destination_host['ip']} dstport 4789 dev {source_host['interface']}\"\n",
    "        await self._run_ssh_command(source_host, cmd)\n",
    "        \n",
    "        cmd = f\"ip link set {tunnel_id} up\"\n",
    "        await self._run_ssh_command(source_host, cmd)\n",
    "        \n",
    "        # Set up the receiving end on the destination host\n",
    "        cmd = f\"ip link add {tunnel_id} type vxlan id 100 remote {source_host['ip']} dstport 4789 dev {destination_host['interface']}\"\n",
    "        await self._run_ssh_command(destination_host, cmd)\n",
    "        \n",
    "        cmd = f\"ip link set {tunnel_id} up\"\n",
    "        await self._run_ssh_command(destination_host, cmd)\n",
    "\n",
    "        return tunnel_id\n",
    "\n",
    "    async def _update_sdn_flow_rules(self, container_id: str, source_host: Dict[str, Any], destination_host: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Updates SDN flow rules to redirect traffic for the migrating container.\n",
    "        \"\"\"\n",
    "        container = self.docker_client.containers.get(container_id)\n",
    "        container_ip = container.attrs['NetworkSettings']['IPAddress']\n",
    "        \n",
    "        # Add flow rules to redirect traffic to the tunnel\n",
    "        for switch in self.switches:\n",
    "            datapath = switch.dp\n",
    "            ofproto = datapath.ofproto\n",
    "            parser = datapath.ofproto_parser\n",
    "\n",
    "            # Redirect incoming traffic to the container via the tunnel\n",
    "            match = parser.OFPMatch(eth_type=0x0800, ipv4_dst=container_ip)\n",
    "            actions = [parser.OFPActionSetField(ipv4_dst=destination_host['ip']),\n",
    "                       parser.OFPActionOutput(self._get_tunnel_port(datapath, destination_host))]\n",
    "            self.add_flow(datapath, 10, match, actions)\n",
    "\n",
    "            # Redirect outgoing traffic from the container via the tunnel\n",
    "            match = parser.OFPMatch(eth_type=0x0800, ipv4_src=container_ip)\n",
    "            actions = [parser.OFPActionSetField(ipv4_src=source_host['ip']),\n",
    "                       parser.OFPActionOutput(self._get_tunnel_port(datapath, source_host))]\n",
    "            self.add_flow(datapath, 10, match, actions)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    async def _reserve_ip_address(self, network_name: str, ip_address: str, host: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Reserves an IP address in the specified network on the given host.\n",
    "        \"\"\"\n",
    "        cmd = f\"docker network connect --ip {ip_address} {network_name} {host['id']}\"\n",
    "        await self._run_ssh_command(host, cmd)\n",
    "\n",
    "    async def _run_ssh_command(self, host: Dict[str, Any], command: str):\n",
    "        \"\"\"\n",
    "        Runs a command on a remote host via SSH.\n",
    "        \"\"\"\n",
    "        ssh_command = f\"ssh {host['user']}@{host['ip']} '{command}'\"\n",
    "        process = await asyncio.create_subprocess_shell(\n",
    "            ssh_command,\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE\n",
    "        )\n",
    "        stdout, stderr = await process.communicate()\n",
    "        if process.returncode != 0:\n",
    "            raise Exception(f\"SSH command failed: {stderr.decode()}\")\n",
    "        return stdout.decode()\n",
    "\n",
    "    def _get_tunnel_port(self, datapath, host):\n",
    "        \"\"\"\n",
    "        Gets the port number for the tunnel interface on the given datapath.\n",
    "        \"\"\"\n",
    "        # This is a placeholder. In a real implementation, you'd need to maintain\n",
    "        # a mapping of tunnel interfaces to switch ports.\n",
    "        return 1  # Assuming port 1 is always the tunnel port for simplicity\n",
    "\n",
    "    def _policy_applies_to_container(self, policy, container_labels):\n",
    "        \"\"\"\n",
    "        Checks if a network policy applies to a container based on its labels.\n",
    "        \"\"\"\n",
    "        selector = policy.spec.pod_selector.match_labels\n",
    "        return all(container_labels.get(k) == v for k, v in selector.items())\n",
    "\n",
    "    @set_ev_cls(ofp_event.EventOFPPacketIn, MAIN_DISPATCHER)\n",
    "    def _packet_in_handler(self, ev):\n",
    "        msg = ev.msg\n",
    "        datapath = msg.datapath\n",
    "        ofproto = datapath.ofproto\n",
    "        parser = datapath.ofproto_parser\n",
    "        in_port = msg.match['in_port']\n",
    "\n",
    "        pkt = packet.Packet(msg.data)\n",
    "        eth = pkt.get_protocols(ethernet.ethernet)[0]\n",
    "\n",
    "        if eth.ethertype == ether_types.ETH_TYPE_LLDP:\n",
    "            # Ignore LLDP packets\n",
    "            return\n",
    "\n",
    "        dst = eth.dst\n",
    "        src = eth.src\n",
    "\n",
    "        dpid = datapath.id\n",
    "        self.mac_to_port.setdefault(dpid, {})\n",
    "\n",
    "        # Learn a mac address to avoid FLOOD next time.\n",
    "        self.mac_to_port[dpid][src] = in_port\n",
    "\n",
    "        if dst in self.mac_to_port[dpid]:\n",
    "            out_port = self.mac_to_port[dpid][dst]\n",
    "        else:\n",
    "            out_port = ofproto.OFPP_FLOOD\n",
    "\n",
    "        actions = [parser.OFPActionOutput(out_port)]\n",
    "\n",
    "        # Install a flow to avoid packet_in next time\n",
    "        if out_port != ofproto.OFPP_FLOOD:\n",
    "            match = parser.OFPMatch(in_port=in_port, eth_dst=dst, eth_src=src)\n",
    "            # Verify if we have a valid buffer_id, if yes avoid to send both\n",
    "            # flow_mod & packet_out\n",
    "            if msg.buffer_id != ofproto.OFP_NO_BUFFER:\n",
    "                self.add_flow(datapath, 1, match, actions, msg.buffer_id)\n",
    "                return\n",
    "            else:\n",
    "                self.add_flow(datapath, 1, match, actions)\n",
    "\n",
    "        data = None\n",
    "        if msg.buffer_id == ofproto.OFP_NO_BUFFER:\n",
    "            data = msg.data\n",
    "\n",
    "        out = parser.OFPPacketOut(datapath=datapath, buffer_id=msg.buffer_id,\n",
    "                                  in_port=in_port, actions=actions, data=data)\n",
    "        datapath.send_msg(out)\n",
    "\n",
    "    def add_flow(self, datapath, priority, match, actions, buffer_id=None):\n",
    "        ofproto = datapath.ofproto\n",
    "        parser = datapath.ofproto_parser\n",
    "\n",
    "        inst = [parser.OFPInstructionActions(ofproto.OFPIT_APPLY_ACTIONS,\n",
    "                                             actions)]\n",
    "        if buffer_id:\n",
    "            mod = parser.OFPFlowMod(datapath=datapath, buffer_id=buffer_id,\n",
    "                                    priority=priority, match=match,\n",
    "                                    instructions=inst)\n",
    "        else:\n",
    "            mod = parser.OFPFlowMod(datapath=datapath, priority=priority,\n",
    "                                    match=match, instructions=inst)\n",
    "        datapath.send_msg(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Monitor Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PerformanceMetricsCollector\n",
    "import time\n",
    "import psutil\n",
    "import docker\n",
    "from prometheus_client import start_http_server, Gauge, CollectorRegistry\n",
    "from typing import Dict, Any\n",
    "\n",
    "class PerformanceMetricsCollector:\n",
    "    def __init__(self):\n",
    "        self.registry = CollectorRegistry()\n",
    "        self.docker_client = docker.from_env()\n",
    "        self.metrics = self._initialize_metrics()\n",
    "        self.thresholds = self._set_thresholds()\n",
    "\n",
    "    def _initialize_metrics(self):\n",
    "        metrics = {}\n",
    "        metric_definitions = [\n",
    "            ('cpu_usage', 'CPU usage percentage'),\n",
    "            ('memory_usage', 'Memory usage percentage'),\n",
    "            ('disk_io', 'Disk I/O operations per second'),\n",
    "            ('network_throughput', 'Network throughput in MB/min'),\n",
    "            ('container_startup_time', 'Container startup time in seconds'),\n",
    "            ('response_time', 'Response time in milliseconds'),\n",
    "            ('error_rate', 'Error rate percentage'),\n",
    "            ('network_latency', 'Network latency in milliseconds'),\n",
    "            ('cpu_load_average', 'CPU load average', ['interval']),\n",
    "            ('memory_page_faults', 'Memory page faults per minute'),\n",
    "            ('network_packet_loss', 'Network packet loss percentage'),\n",
    "            ('disk_queue_length', 'Disk queue length'),\n",
    "            ('container_restart_count', 'Container restart count'),\n",
    "            ('network_connection_count', 'Network connection count'),\n",
    "            ('system_call_rate', 'System call rate per second')\n",
    "        ]\n",
    "        \n",
    "        for name, description, *labels in metric_definitions:\n",
    "            metrics[name] = Gauge(name, description, labels or None, registry=self.registry)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def _set_thresholds(self):\n",
    "        return {\n",
    "            'cpu_usage': 80,\n",
    "            'memory_usage': 70,\n",
    "            'disk_io': 100,\n",
    "            'network_throughput': 500,\n",
    "            'container_startup_time': 5,\n",
    "            'response_time': 200,\n",
    "            'error_rate': 1,\n",
    "            'network_latency': 100,\n",
    "            'cpu_load_average': 2.0,\n",
    "            'memory_page_faults': 50,\n",
    "            'network_packet_loss': 1,\n",
    "            'disk_queue_length': 5,\n",
    "            'container_restart_count': 3,\n",
    "            'network_connection_count': 100,\n",
    "            'system_call_rate': 1000\n",
    "        }\n",
    "\n",
    "    def update_host_metrics(self, host):\n",
    "        self.cpu_usage.set(psutil.cpu_percent())\n",
    "        self.memory_usage.set(psutil.virtual_memory().percent)\n",
    "        net_io = psutil.net_io_counters()\n",
    "        self.network_in.set(net_io.bytes_recv)\n",
    "        self.network_out.set(net_io.bytes_sent)\n",
    "    \n",
    "    def collect_host_metrics(self):\n",
    "        # CPU Usage\n",
    "        self.metrics['cpu_usage'].set(psutil.cpu_percent())\n",
    "\n",
    "        # Memory Usage\n",
    "        mem = psutil.virtual_memory()\n",
    "        self.metrics['memory_usage'].set(mem.percent)\n",
    "\n",
    "        # Disk I/O\n",
    "        disk_io = psutil.disk_io_counters()\n",
    "        self.metrics['disk_io'].set(disk_io.read_count + disk_io.write_count)\n",
    "\n",
    "        # Network Throughput\n",
    "        net_io = psutil.net_io_counters()\n",
    "        throughput = (net_io.bytes_sent + net_io.bytes_recv) / (1024 * 1024)  # Convert to MB\n",
    "        self.metrics['network_throughput'].set(throughput)\n",
    "\n",
    "        # CPU Load Average\n",
    "        load1, load5, load15 = psutil.getloadavg()\n",
    "        self.metrics['cpu_load_average'].labels('1min').set(load1)\n",
    "        self.metrics['cpu_load_average'].labels('5min').set(load5)\n",
    "        self.metrics['cpu_load_average'].labels('15min').set(load15)\n",
    "\n",
    "        # Memory Page Faults\n",
    "        self.metrics['memory_page_faults'].set(mem.pgfault)\n",
    "\n",
    "        # Disk Queue Length\n",
    "        disk_usage = psutil.disk_usage('/')\n",
    "        self.metrics['disk_queue_length'].set(disk_usage.used)\n",
    "\n",
    "        # Network Connection Count\n",
    "        connections = len(psutil.net_connections())\n",
    "        self.metrics['network_connection_count'].set(connections)\n",
    "\n",
    "        # System Call Rate \n",
    "        self.metrics['system_call_rate'].set(psutil.cpu_stats().syscalls)\n",
    "    \n",
    "    def collect_container_metrics(self, container: Any):\n",
    "        try:\n",
    "            stats = self.docker_client.containers.get(container.container_id).stats(stream=False)\n",
    "            \n",
    "            # CPU Usage\n",
    "            cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - stats['precpu_stats']['cpu_usage']['total_usage']\n",
    "            system_delta = stats['cpu_stats']['system_cpu_usage'] - stats['precpu_stats']['system_cpu_usage']\n",
    "            cpu_usage = (cpu_delta / system_delta) * psutil.cpu_count() * 100.0\n",
    "            self.metrics['cpu_usage'].set(cpu_usage)\n",
    "\n",
    "            # Memory Usage\n",
    "            memory_usage = stats['memory_stats']['usage'] / stats['memory_stats']['limit'] * 100.0\n",
    "            self.metrics['memory_usage'].set(memory_usage)\n",
    "\n",
    "            # Network Throughput\n",
    "            if 'networks' in stats:\n",
    "                network_stats = stats['networks']['eth0']\n",
    "                throughput = (network_stats['rx_bytes'] + network_stats['tx_bytes']) / (1024 * 1024)  # Convert to MB\n",
    "                self.metrics['network_throughput'].set(throughput)\n",
    "\n",
    "            # Container Restart Count\n",
    "            self.metrics['container_restart_count'].set(stats['restart_count'])\n",
    "\n",
    "            # Error Rate and Response Time would typically come from application-level metrics\n",
    "            self.metrics['error_rate'].set(0)\n",
    "            self.metrics['response_time'].set(0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting metrics for container {container.container_id}: {str(e)}\")\n",
    "\n",
    "    def update_container_metrics(self, container):\n",
    "        try:\n",
    "            # Fetch container stats\n",
    "            stats = self.docker_client.containers.get(container.container_id).stats(stream=False)\n",
    "            \n",
    "            # CPU usage calculation\n",
    "            cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - stats['precpu_stats']['cpu_usage']['total_usage']\n",
    "            system_delta = stats['cpu_stats']['system_cpu_usage'] - stats['precpu_stats']['system_cpu_usage']\n",
    "            num_cpus = len(stats['cpu_stats']['cpu_usage']['percpu_usage'])\n",
    "            cpu_usage = (cpu_delta / system_delta) * num_cpus * 100.0\n",
    "            self.container_cpu.labels(container_id=container.container_id).set(cpu_usage)\n",
    "\n",
    "            # Memory usage calculation\n",
    "            memory_usage = stats['memory_stats']['usage'] / stats['memory_stats']['limit'] * 100.0\n",
    "            self.container_memory.labels(container_id=container.container_id).set(memory_usage)\n",
    "\n",
    "            # Network usage calculation\n",
    "            if 'networks' in stats:\n",
    "                network_stats = stats['networks']['eth0']\n",
    "                self.container_network_in.labels(container_id=container.container_id).set(network_stats['rx_bytes'])\n",
    "                self.container_network_out.labels(container_id=container.container_id).set(network_stats['tx_bytes'])\n",
    "\n",
    "            # Disk I/O calculation\n",
    "            if 'blkio_stats' in stats:\n",
    "                io_service_bytes_recursive = stats['blkio_stats']['io_service_bytes_recursive']\n",
    "                read_io = sum(item['value'] for item in io_service_bytes_recursive if item['op'] == 'Read')\n",
    "                write_io = sum(item['value'] for item in io_service_bytes_recursive if item['op'] == 'Write')\n",
    "                self.container_disk_io.labels(container_id=container.container_id, operation='read').set(read_io)\n",
    "                self.container_disk_io.labels(container_id=container.container_id, operation='write').set(write_io)\n",
    "\n",
    "        except docker.errors.NotFound:\n",
    "            print(f\"Container {container.container_id} not found for metrics update.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating metrics for container {container.container_id}: {str(e)}\")\n",
    "\n",
    "    def check_thresholds(self):\n",
    "        violations = []\n",
    "        for metric, threshold in self.thresholds.items():\n",
    "            if metric in self.metrics:\n",
    "                value = self.metrics[metric]._value.get()\n",
    "                if value > threshold:\n",
    "                    violations.append(f\"{metric}: {value} (threshold: {threshold})\")\n",
    "        return violations\n",
    "    \n",
    "    def collect_metrics(self, containers, interval=10):\n",
    "        while True:\n",
    "            self.collect_host_metrics()\n",
    "            for container in containers:\n",
    "                self.collect_container_metrics(container)\n",
    "            \n",
    "            violations = self.check_thresholds()\n",
    "            if violations:\n",
    "                print(\"Threshold violations detected:\")\n",
    "                for violation in violations:\n",
    "                    print(violation)\n",
    "            \n",
    "            time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResourceUtilizationAnalyzer\n",
    "\n",
    "class ResourceUtilizationAnalyzer:\n",
    "    def __init__(self, thresholds):\n",
    "        self.thresholds = thresholds  \n",
    "\n",
    "    def analyze_host_utilization(self, host):\n",
    "        cpu_usage = 100 - (host.available_cpu / host.total_cpu) * 100\n",
    "        memory_usage = 100 - (host.available_memory / host.total_memory) * 100\n",
    "        over_utilized = cpu_usage > self.thresholds['cpu_threshold'] or memory_usage > self.thresholds['memory_threshold']\n",
    "        under_utilized = cpu_usage < (self.thresholds['cpu_threshold'] / 2) and memory_usage < (self.thresholds['memory_threshold'] / 2)\n",
    "        return over_utilized, under_utilized\n",
    "\n",
    "    def analyze_container_utilization(self, container):\n",
    "        \n",
    "        cpu_usage = container.cpu_limit  \n",
    "        memory_usage = container.memory_limit  \n",
    "        needs_migration = cpu_usage > self.thresholds['cpu_threshold'] or memory_usage > self.thresholds['memory_threshold']\n",
    "        return needs_migration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Engine Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionEngine:\n",
    "    def __init__(self, workload_analyzer, resource_optimizer, migration_planner, policies, performance_metrics_collector):\n",
    "        self.workload_analyzer = workload_analyzer\n",
    "        self.resource_optimizer = resource_optimizer\n",
    "        self.migration_planner = migration_planner\n",
    "        self.policy_enforcer = PolicyEnforcer(policies)\n",
    "        self.policies = policies\n",
    "        self.performance_metrics_collector = performance_metrics_collector\n",
    "\n",
    "    def get_current_resource_usage(self, container: Any, host: Any) -> Dict[str, float]:\n",
    "\n",
    "        try:\n",
    "            # Fetch container-specific metrics\n",
    "            container_cpu = self.performance_metrics_collector.container_cpu.labels(container_id=container.container_id)._value.get()\n",
    "            container_memory = self.performance_metrics_collector.container_memory.labels(container_id=container.container_id)._value.get()\n",
    "            container_network_in = self.performance_metrics_collector.container_network_in.labels(container_id=container.container_id)._value.get()\n",
    "            container_network_out = self.performance_metrics_collector.container_network_out.labels(container_id=container.container_id)._value.get()\n",
    "            container_disk_read = self.performance_metrics_collector.container_disk_io.labels(container_id=container.container_id, operation='read')._value.get()\n",
    "            container_disk_write = self.performance_metrics_collector.container_disk_io.labels(container_id=container.container_id, operation='write')._value.get()\n",
    "\n",
    "            # Fetch host-specific metrics\n",
    "            host_cpu = self.performance_metrics_collector.cpu_usage._value.get()\n",
    "            host_memory = self.performance_metrics_collector.memory_usage._value.get()\n",
    "            host_network_in = self.performance_metrics_collector.network_in._value.get()\n",
    "            host_network_out = self.performance_metrics_collector.network_out._value.get()\n",
    "\n",
    "            # Calculate container's resource usage as a percentage of host's total resources\n",
    "            cpu_usage_percent = (container_cpu / host.total_cpu) * 100\n",
    "            memory_usage_percent = (container_memory / host.total_memory) * 100\n",
    "\n",
    "            # Calculate network utilization (bytes per second)\n",
    "            current_time = time.time()\n",
    "            time_diff = current_time - self.last_network_check_time if hasattr(self, 'last_network_check_time') else 1\n",
    "            network_in_rate = (container_network_in - self.last_network_in) / time_diff if hasattr(self, 'last_network_in') else 0\n",
    "            network_out_rate = (container_network_out - self.last_network_out) / time_diff if hasattr(self, 'last_network_out') else 0\n",
    "\n",
    "            # Update last checked values for next calculation\n",
    "            self.last_network_check_time = current_time\n",
    "            self.last_network_in = container_network_in\n",
    "            self.last_network_out = container_network_out\n",
    "\n",
    "            # Calculate disk I/O rates (bytes per second)\n",
    "            disk_read_rate = (container_disk_read - self.last_disk_read) / time_diff if hasattr(self, 'last_disk_read') else 0\n",
    "            disk_write_rate = (container_disk_write - self.last_disk_write) / time_diff if hasattr(self, 'last_disk_write') else 0\n",
    "\n",
    "            # Update last checked values for next calculation\n",
    "            self.last_disk_read = container_disk_read\n",
    "            self.last_disk_write = container_disk_write\n",
    "\n",
    "            return {\n",
    "                'cpu_usage': cpu_usage_percent,\n",
    "                'memory_usage': memory_usage_percent,\n",
    "                'network_in': network_in_rate,\n",
    "                'network_out': network_out_rate,\n",
    "                'disk_read': disk_read_rate,\n",
    "                'disk_write': disk_write_rate,\n",
    "                'host_cpu_usage': host_cpu,\n",
    "                'host_memory_usage': host_memory,\n",
    "                'host_network_in': host_network_in,\n",
    "                'host_network_out': host_network_out,\n",
    "                'time_of_day': int(time.strftime(\"%H\")),\n",
    "                'day_of_week': int(time.strftime(\"%w\"))\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching resource usage for container {container.container_id}: {str(e)}\")\n",
    "            # Return default values in case of an error\n",
    "            return {\n",
    "                'cpu_usage': 0,\n",
    "                'memory_usage': 0,\n",
    "                'network_in': 0,\n",
    "                'network_out': 0,\n",
    "                'disk_read': 0,\n",
    "                'disk_write': 0,\n",
    "                'host_cpu_usage': 0,\n",
    "                'host_memory_usage': 0,\n",
    "                'host_network_in': 0,\n",
    "                'host_network_out': 0,\n",
    "                'time_of_day': int(time.strftime(\"%H\")),\n",
    "                'day_of_week': int(time.strftime(\"%w\"))\n",
    "            }\n",
    "\n",
    "    def predict_future_resource_usage(self, container, current_usage):\n",
    "        features = np.array([list(current_usage.values())])\n",
    "        predicted_usage = self.workload_analyzer.predict_resource_usage(features)[0]\n",
    "        \n",
    "        return {\n",
    "            'predicted_cpu_usage': predicted_usage,\n",
    "            **current_usage  # Include current usage for other resources\n",
    "        }\n",
    "\n",
    "    def should_migrate(self, current_usage, future_usage):\n",
    "        # Define thresholds for migration\n",
    "        CPU_THRESHOLD = 80  # percent\n",
    "        MEMORY_THRESHOLD = 80  # percent\n",
    "        \n",
    "        return (future_usage['predicted_cpu_usage'] > CPU_THRESHOLD or \n",
    "                current_usage['memory_usage'] > MEMORY_THRESHOLD)\n",
    "\n",
    "    def find_best_host(self, container, future_usage, potential_hosts):\n",
    "        best_host = None\n",
    "        min_load = float('inf')\n",
    "        \n",
    "        for host in potential_hosts:\n",
    "            host_load = self.calculate_host_load(host, future_usage)\n",
    "            if host_load < min_load and self.can_host_accommodate(host, container, future_usage):\n",
    "                best_host = host\n",
    "                min_load = host_load\n",
    "        \n",
    "        return best_host\n",
    "\n",
    "    def calculate_host_load(self, host, future_usage):\n",
    "        # This should calculate the projected load on the host\n",
    "        # including the future usage of the container\n",
    "        return host.current_load + future_usage['predicted_cpu_usage']\n",
    "\n",
    "    def can_host_accommodate(self, host, container, future_usage):\n",
    "        # Check if the host has enough resources for the container\n",
    "        return (host.available_cpu >= future_usage['predicted_cpu_usage'] and\n",
    "                host.available_memory >= future_usage['memory_usage'])\n",
    "\n",
    "    def create_migration_request(self, container, source_host, destination_host):\n",
    "        return MigrationRequest(\n",
    "            container_id=container.id,\n",
    "            source_host=source_host,\n",
    "            destination_host=destination_host,\n",
    "            migration_type=self.select_migration_strategy(container, source_host, destination_host)\n",
    "        )\n",
    "\n",
    "    def select_migration_strategy(self, container, source_host, destination_host):\n",
    "        # This method should select the appropriate migration strategy\n",
    "        # based on the container, source and destination hosts\n",
    "        # For simplicity, we'll always choose live migration\n",
    "        return MigrationStrategy.LIVE_MIGRATION\n",
    "\n",
    "    def make_migration_decision(self, container, current_host, potential_hosts):\n",
    "        # Get current resource usage\n",
    "        current_usage = self.get_current_resource_usage(container, current_host)\n",
    "        \n",
    "        # Predict future resource usage\n",
    "        future_usage = self.predict_future_resource_usage(container, current_usage)\n",
    "        \n",
    "        # Check if migration is needed based on current and predicted usage\n",
    "        if self.should_migrate(current_usage, future_usage):\n",
    "            # Find the best host for migration\n",
    "            best_host = self.find_best_host(container, future_usage, potential_hosts)\n",
    "            \n",
    "            if best_host:\n",
    "                # Create a migration request\n",
    "                migration_request = self.create_migration_request(container, current_host, best_host)\n",
    "                \n",
    "                # Check if the migration request satisfies all policies\n",
    "                if self.policy_enforcer.enforce_policies(migration_request):\n",
    "                    return migration_request\n",
    "        \n",
    "        return None  # No migration needed or possible\n",
    "\n",
    "\n",
    "#WorkloadAnalyzer\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class WorkloadAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        self.lstm_model = None\n",
    "        self.scaler = None  # We'll use this to normalize data for LSTM\n",
    "        self.feature_columns = ['cpu_usage', 'memory_usage', 'network_in', 'network_out', 'time_of_day', 'day_of_week']\n",
    "        self.target_column = 'future_cpu_usage'\n",
    "        \n",
    "    def prepare_data(self, data):\n",
    "        # Assume data is a pandas DataFrame with columns matching self.feature_columns\n",
    "        # and a target column 'future_cpu_usage'\n",
    "        X = data[self.feature_columns]\n",
    "        y = data[self.target_column]\n",
    "        \n",
    "        # Normalize data for LSTM\n",
    "        self.scaler = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "            X.values, y.values, length=10, batch_size=32)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def train_rf_model(self, X, y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.rf_model.fit(X_train, y_train)\n",
    "        y_pred = self.rf_model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        logger.info(f\"Random Forest MSE: {mse}\")\n",
    "\n",
    "    def train_lstm_model(self, X, y):\n",
    "        # Reshape data for LSTM [samples, time steps, features]\n",
    "        X_reshaped = X.values.reshape((X.shape[0], 1, X.shape[1]))\n",
    "        \n",
    "        self.lstm_model = Sequential([\n",
    "            LSTM(50, activation='relu', input_shape=(1, X.shape[1])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        self.lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "        self.lstm_model.fit(X_reshaped, y, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "        \n",
    "        y_pred = self.lstm_model.predict(X_reshaped)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        logger.info(f\"LSTM MSE: {mse}\")\n",
    "\n",
    "    def predict_resource_usage(self, features):\n",
    "        # Ensure features is a 2D array\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(1, -1)\n",
    "        \n",
    "        rf_pred = self.rf_model.predict(features)\n",
    "        \n",
    "        # Reshape for LSTM prediction\n",
    "        lstm_features = features.reshape((features.shape[0], 1, features.shape[1]))\n",
    "        lstm_pred = self.lstm_model.predict(lstm_features)\n",
    "        \n",
    "        # Combine predictions (simple average)\n",
    "        combined_pred = (rf_pred + lstm_pred.flatten()) / 2\n",
    "        \n",
    "        return combined_pred\n",
    "\n",
    "    def get_feature_importance(self):\n",
    "        return dict(zip(self.feature_columns, self.rf_model.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource Optimizer\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class ResourceOptimizer:\n",
    "    def __init__(self, performance_metrics_collector, network_manager):\n",
    "        self.hosts = {}\n",
    "        self.containers = {}\n",
    "        self.performance_metrics_collector = performance_metrics_collector\n",
    "        self.network_manager = network_manager\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def optimize_allocation(self, hosts, containers):\n",
    "        allocation = {}\n",
    "        for container in containers:\n",
    "            best_host = self.select_best_host(hosts, container)\n",
    "            if best_host:\n",
    "                allocation[container.container_id] = best_host.host_id\n",
    "        return allocation\n",
    "\n",
    "    def select_best_host(self, hosts: List[Any], container: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Selects the best host for a given container based on multiple criteria.\n",
    "        \n",
    "        Args:\n",
    "            hosts: List of potential host objects.\n",
    "            container: Container object to be migrated.\n",
    "        \n",
    "        Returns:\n",
    "            The best host object for the container, or None if no suitable host is found.\n",
    "        \"\"\"\n",
    "        suitable_hosts = []\n",
    "        host_scores = []\n",
    "\n",
    "        for host in hosts:\n",
    "            if self.can_host_accommodate(host, container):\n",
    "                suitable_hosts.append(host)\n",
    "                host_scores.append(self.calculate_host_score(host, container))\n",
    "\n",
    "        if not suitable_hosts:\n",
    "            return None\n",
    "\n",
    "        best_host_index = np.argmax(host_scores)\n",
    "        return suitable_hosts[best_host_index]\n",
    "\n",
    "    def can_host_accommodate(self, host: Any, container: Any) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if a host can accommodate the container based on resource requirements.\n",
    "        \n",
    "        Args:\n",
    "            host: Host object to check.\n",
    "            container: Container object to be accommodated.\n",
    "        \n",
    "        Returns:\n",
    "            True if the host can accommodate the container, False otherwise.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            host.available_cpu >= container.cpu_limit and\n",
    "            host.available_memory >= container.memory_limit and\n",
    "            host.available_storage >= container.storage_limit\n",
    "        )\n",
    "\n",
    "    def calculate_host_score(self, host: Any, container: Any) -> float:\n",
    "        \"\"\"\n",
    "        Calculates a score for a host based on multiple criteria.\n",
    "        \n",
    "        Args:\n",
    "            host: Host object to score.\n",
    "            container: Container object to be migrated.\n",
    "        \n",
    "        Returns:\n",
    "            A float representing the host's score.\n",
    "        \"\"\"\n",
    "        # Fetch current resource usage\n",
    "        host_metrics = self.performance_metrics_collector.get_host_metrics(host.host_id)\n",
    "        \n",
    "        # Calculate resource utilization after potential migration\n",
    "        cpu_utilization = (host.total_cpu - host.available_cpu + container.cpu_limit) / host.total_cpu\n",
    "        memory_utilization = (host.total_memory - host.available_memory + container.memory_limit) / host.total_memory\n",
    "        storage_utilization = (host.total_storage - host.available_storage + container.storage_limit) / host.total_storage\n",
    "        \n",
    "        # Calculate network latency and bandwidth\n",
    "        network_metrics = self.network_manager.get_network_metrics(container.host, host.host_id)\n",
    "        latency = network_metrics['latency']\n",
    "        bandwidth = network_metrics['bandwidth']\n",
    "        \n",
    "        # Prepare feature vector\n",
    "        features = np.array([\n",
    "            cpu_utilization,\n",
    "            memory_utilization,\n",
    "            storage_utilization,\n",
    "            host_metrics['load_average'],\n",
    "            latency,\n",
    "            bandwidth,\n",
    "            host.power_efficiency,  # Assuming this is a property of the host\n",
    "            len(host.containers)  # Number of containers already on the host\n",
    "        ]).reshape(1, -1)\n",
    "        \n",
    "        # Normalize features\n",
    "        normalized_features = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Define weights for each criterion (adjust these based on your priorities)\n",
    "        weights = np.array([0.2, 0.2, 0.1, 0.1, 0.15, 0.15, 0.05, 0.05])\n",
    "        \n",
    "        # Calculate weighted score\n",
    "        score = np.dot(normalized_features, weights)[0]\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def update_resource_allocation(self, container: Any, old_host: Any, new_host: Any):\n",
    "        \"\"\"\n",
    "        Updates the resource allocation after a container migration.\n",
    "        \n",
    "        Args:\n",
    "            container: The migrated container object.\n",
    "            old_host: The host object from which the container was migrated.\n",
    "            new_host: The host object to which the container was migrated.\n",
    "        \"\"\"\n",
    "        # Update old host\n",
    "        old_host.available_cpu += container.cpu_limit\n",
    "        old_host.available_memory += container.memory_limit\n",
    "        old_host.available_storage += container.storage_limit\n",
    "        old_host.containers.remove(container)\n",
    "\n",
    "        # Update new host\n",
    "        new_host.available_cpu -= container.cpu_limit\n",
    "        new_host.available_memory -= container.memory_limit\n",
    "        new_host.available_storage -= container.storage_limit\n",
    "        new_host.containers.append(container)\n",
    "\n",
    "        # Update container's host\n",
    "        container.host = new_host.host_id\n",
    "\n",
    "    def rebalance_resources(self):\n",
    "        \"\"\"\n",
    "        Periodically rebalances resources across all hosts to optimize overall system performance.\n",
    "        \"\"\"\n",
    "        all_containers = list(self.containers.values())\n",
    "        all_hosts = list(self.hosts.values())\n",
    "        \n",
    "        for container in all_containers:\n",
    "            current_host = self.hosts[container.host]\n",
    "            best_host = self.select_best_host(all_hosts, container)\n",
    "            \n",
    "            if best_host and best_host != current_host:\n",
    "                self.update_resource_allocation(container, current_host, best_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migration Planner\n",
    "\n",
    "import networkx as nx\n",
    "from typing import Dict, Any\n",
    "from typing import List\n",
    "\n",
    "class MigrationPlanner:\n",
    "    def __init__(self, network_manager):\n",
    "        self.network_manager = network_manager\n",
    "\n",
    "    def calculate_migration_cost(self, container_id: str, source_host: str, target_host: str) -> float:\n",
    "        container = self.network_manager.get_container(container_id)\n",
    "        network_path = self.network_manager.compute_optimal_path(source_host, target_host)\n",
    "        \n",
    "        if not network_path:\n",
    "            return float('inf')  # No path available, migration is impossible\n",
    "        \n",
    "        # Calculate data transfer cost\n",
    "        data_size = container.memory_usage + container.disk_usage\n",
    "        bandwidth = min(self.network_manager.get_path_bandwidth(network_path))\n",
    "        transfer_time = data_size / bandwidth\n",
    "        \n",
    "        # Calculate resource usage cost\n",
    "        cpu_cost = container.cpu_usage * self.network_manager.get_cpu_cost(target_host)\n",
    "        memory_cost = container.memory_usage * self.network_manager.get_memory_cost(target_host)\n",
    "        \n",
    "        # Calculate downtime cost\n",
    "        downtime = self.estimate_downtime(container_id, source_host, target_host)\n",
    "        downtime_cost = downtime * container.importance_factor\n",
    "        \n",
    "        total_cost = transfer_time + cpu_cost + memory_cost + downtime_cost\n",
    "        return total_cost\n",
    "\n",
    "    def estimate_downtime(self, container_id: str, source_host: str, target_host: str) -> float:\n",
    "        container = self.network_manager.get_container(container_id)\n",
    "        network_path = self.network_manager.compute_optimal_path(source_host, target_host)\n",
    "        \n",
    "        if not network_path:\n",
    "            return float('inf')  # No path available, migration is impossible\n",
    "        \n",
    "        # Calculate network latency\n",
    "        latency = sum(self.network_manager.get_link_latency(link) for link in network_path)\n",
    "        \n",
    "        # Estimate time for final memory transfer\n",
    "        final_memory_transfer = container.memory_dirty_rate * latency\n",
    "        bandwidth = min(self.network_manager.get_path_bandwidth(network_path))\n",
    "        transfer_time = final_memory_transfer / bandwidth\n",
    "        \n",
    "        # Add time for container stop and start operations\n",
    "        stop_time = 0.1  # Assuming 100ms for container stop\n",
    "        start_time = 0.2  # Assuming 200ms for container start\n",
    "        \n",
    "        total_downtime = latency + transfer_time + stop_time + start_time\n",
    "        return total_downtime\n",
    "\n",
    "    def plan_migration(self, allocation: Dict[str, str], current_allocation: Dict[str, str]) -> List[Dict[str, Any]]:\n",
    "        migration_plan = []\n",
    "        for container_id, target_host_id in allocation.items():\n",
    "            if current_allocation.get(container_id) != target_host_id:\n",
    "                source_host_id = current_allocation.get(container_id)\n",
    "                migration_cost = self.calculate_migration_cost(container_id, source_host_id, target_host_id)\n",
    "                downtime = self.estimate_downtime(container_id, source_host_id, target_host_id)\n",
    "                migration_plan.append({\n",
    "                    'container_id': container_id,\n",
    "                    'source_host': source_host_id,\n",
    "                    'destination_host': target_host_id,\n",
    "                    'migration_cost': migration_cost,\n",
    "                    'downtime': downtime\n",
    "                })\n",
    "        # Sort by migration_cost ascending\n",
    "        migration_plan.sort(key=lambda x: x['migration_cost'])\n",
    "        return migration_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Container Manager Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime Controller\n",
    "\n",
    "import docker\n",
    "import psutil\n",
    "\n",
    "class ContainerRuntimeInterface:\n",
    "    def __init__(self):\n",
    "        self.supported_runtimes = {\n",
    "            'docker': docker.DockerClient,\n",
    "            'containerd': docker.DockerClient,  # Placeholder: Implement containerd client\n",
    "            'cri-o': docker.DockerClient       # Placeholder: Implement CRI-O client\n",
    "        }\n",
    "\n",
    "    def get_runtime_instance(self, runtime_type):\n",
    "        runtime_class = self.supported_runtimes.get(runtime_type.lower())\n",
    "        if runtime_class:\n",
    "            return runtime_class(base_url='tcp://127.0.0.1:2375')  # Example for Docker\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported runtime type: {runtime_type}\")\n",
    "\n",
    "    def create_container(self, runtime, container_spec):\n",
    "        return runtime.containers.create(**container_spec)\n",
    "\n",
    "    def start_container(self, container):\n",
    "        container.start()\n",
    "\n",
    "    def stop_container(self, container):\n",
    "        container.stop()\n",
    "\n",
    "    def checkpoint_container(self, container, checkpoint_name, checkpoint_dir):\n",
    "        subprocess.run(['docker', 'checkpoint', 'create', container.id, checkpoint_name, '--checkpoint-dir', checkpoint_dir], check=True)\n",
    "\n",
    "    def restore_container(self, runtime, container_spec, checkpoint_dir):\n",
    "        container = runtime.containers.run(**container_spec, checkpoint=checkpoint_dir, detach=True)\n",
    "        return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NestedContainerManager\n",
    "import docker\n",
    "import subprocess\n",
    "import os\n",
    "import tempfile\n",
    "from docker.errors import DockerException, APIError, ImageNotFound\n",
    "\n",
    "class NestedContainerManager:\n",
    "    def __init__(self, runtime_interface, state_synchronizer, network_manager):\n",
    "        self.runtime_interface = runtime_interface\n",
    "        self.state_synchronizer = state_synchronizer\n",
    "        self.network_manager = network_manager\n",
    "        self.docker_client = docker.from_env()\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    async def migrate_nested_container(self, nested_container: Dict[str, Any], source_host: Dict[str, Any], target_host: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Migrates a nested container from the source host to the target host.\n",
    "\n",
    "        Args:\n",
    "            nested_container: A dictionary containing information about the nested container.\n",
    "            source_host: Information about the source host.\n",
    "            target_host: Information about the target host.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if migration was successful, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Starting migration of nested container {nested_container['id']} from {source_host['id']} to {target_host['id']}\")\n",
    "\n",
    "            # Step 1: Prepare for migration\n",
    "            await self._prepare_for_migration(nested_container, source_host, target_host)\n",
    "\n",
    "            # Step 2: Checkpoint the outer container\n",
    "            outer_checkpoint = await self._checkpoint_outer_container(nested_container['outer_id'], source_host)\n",
    "\n",
    "            # Step 3: Checkpoint the inner container\n",
    "            inner_checkpoint = await self._checkpoint_inner_container(nested_container['inner_id'], source_host)\n",
    "\n",
    "            # Step 4: Transfer checkpoints to the target host\n",
    "            await self._transfer_checkpoints(outer_checkpoint, inner_checkpoint, source_host, target_host)\n",
    "\n",
    "            # Step 5: Restore the outer container on the target host\n",
    "            new_outer_id = await self._restore_outer_container(outer_checkpoint, target_host)\n",
    "\n",
    "            # Step 6: Restore the inner container on the target host\n",
    "            new_inner_id = await self._restore_inner_container(inner_checkpoint, new_outer_id, target_host)\n",
    "\n",
    "            # Step 7: Verify the restoration\n",
    "            if await self._verify_restoration(new_outer_id, new_inner_id, target_host):\n",
    "                # Step 8: Update network configuration\n",
    "                await self._update_network_configuration(nested_container, new_outer_id, new_inner_id, source_host, target_host)\n",
    "\n",
    "                # Step 9: Clean up source host\n",
    "                await self._cleanup_source(nested_container, source_host)\n",
    "\n",
    "                self.logger.info(f\"Successfully migrated nested container {nested_container['id']} to {target_host['id']}\")\n",
    "                return True\n",
    "            else:\n",
    "                raise Exception(\"Nested container restoration verification failed\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to migrate nested container {nested_container['id']}: {str(e)}\")\n",
    "            await self._rollback_migration(nested_container, source_host, target_host)\n",
    "            return False\n",
    "\n",
    "    async def _prepare_for_migration(self, nested_container: Dict[str, Any], source_host: Dict[str, Any], target_host: Dict[str, Any]):\n",
    "        \"\"\"Prepares the source and target hosts for migration.\"\"\"\n",
    "        # Ensure target host has necessary images\n",
    "        await self._ensure_images(nested_container, target_host)\n",
    "        \n",
    "        # Pre-create networks on target host\n",
    "        await self._pre_create_networks(nested_container, target_host)\n",
    "\n",
    "    async def _ensure_images(self, nested_container: Dict[str, Any], target_host: Dict[str, Any]):\n",
    "        \"\"\"Ensures that the target host has the necessary container images.\"\"\"\n",
    "        outer_image = nested_container['outer_image']\n",
    "        inner_image = nested_container['inner_image']\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for image in [outer_image, inner_image]:\n",
    "                async with session.post(f\"http://{target_host['address']}:2375/images/create\", params={'fromImage': image}) as response:\n",
    "                    if response.status != 200:\n",
    "                        raise Exception(f\"Failed to pull image {image} on target host\")\n",
    "\n",
    "    async def _pre_create_networks(self, nested_container: Dict[str, Any], target_host: Dict[str, Any]):\n",
    "        \"\"\"Pre-creates necessary networks on the target host.\"\"\"\n",
    "        networks = nested_container['networks']\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for network in networks:\n",
    "                async with session.post(f\"http://{target_host['address']}:2375/networks/create\", json=network) as response:\n",
    "                    if response.status != 201:\n",
    "                        raise Exception(f\"Failed to create network {network['Name']} on target host\")\n",
    "\n",
    "    async def _checkpoint_outer_container(self, outer_id: str, source_host: Dict[str, Any]) -> str:\n",
    "        \"\"\"Creates a checkpoint of the outer container.\"\"\"\n",
    "        checkpoint_dir = tempfile.mkdtemp(prefix=\"outer_checkpoint_\")\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(f\"http://{source_host['address']}:2375/containers/{outer_id}/checkpoint\", \n",
    "                                    json={\"CheckpointDir\": checkpoint_path}) as response:\n",
    "                if response.status != 201:\n",
    "                    raise Exception(f\"Failed to checkpoint outer container {outer_id}\")\n",
    "        \n",
    "        return checkpoint_dir\n",
    "\n",
    "    async def _checkpoint_inner_container(self, inner_id: str, source_host: Dict[str, Any]) -> str:\n",
    "        \"\"\"Creates a checkpoint of the inner container.\"\"\"\n",
    "        checkpoint_dir = tempfile.mkdtemp(prefix=\"inner_checkpoint_\")\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(f\"http://{source_host['address']}:2375/containers/{inner_id}/checkpoint\", \n",
    "                                    json={\"CheckpointDir\": checkpoint_path}) as response:\n",
    "                if response.status != 201:\n",
    "                    raise Exception(f\"Failed to checkpoint inner container {inner_id}\")\n",
    "        \n",
    "        return checkpoint_dir\n",
    "\n",
    "    async def _transfer_checkpoints(self, outer_checkpoint: str, inner_checkpoint: str, source_host: Dict[str, Any], target_host: Dict[str, Any]):\n",
    "        \"\"\"Transfers checkpoints from source host to target host.\"\"\"\n",
    "        for checkpoint in [outer_checkpoint, inner_checkpoint]:\n",
    "            tar_path = checkpoint + \".tar.gz\"\n",
    "            await self._create_tar(checkpoint, tar_path)\n",
    "            await self._scp_file(tar_path, source_host, target_host, \"/tmp/\")\n",
    "            await self._extract_tar(tar_path, checkpoint, target_host)\n",
    "\n",
    "    async def _create_tar(self, source_dir: str, tar_path: str):\n",
    "        \"\"\"Creates a tar archive of the checkpoint directory.\"\"\"\n",
    "        with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "            tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "    async def _scp_file(self, file_path: str, source_host: Dict[str, Any], target_host: Dict[str, Any], target_dir: str):\n",
    "        \"\"\"Copies a file from source host to target host using SCP.\"\"\"\n",
    "        cmd = f\"scp {file_path} {target_host['user']}@{target_host['address']}:{target_dir}\"\n",
    "        process = await asyncio.create_subprocess_shell(cmd)\n",
    "        await process.wait()\n",
    "\n",
    "    async def _extract_tar(self, tar_path: str, extract_path: str, host: Dict[str, Any]):\n",
    "        \"\"\"Extracts a tar archive on the specified host.\"\"\"\n",
    "        cmd = f\"ssh {host['user']}@{host['address']} 'tar -xzf {tar_path} -C {os.path.dirname(extract_path)}'\"\n",
    "        process = await asyncio.create_subprocess_shell(cmd)\n",
    "        await process.wait()\n",
    "\n",
    "    async def _restore_outer_container(self, checkpoint: str, target_host: Dict[str, Any]) -> str:\n",
    "        \"\"\"Restores the outer container on the target host.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(f\"http://{target_host['address']}:2375/containers/create\", \n",
    "                                    json={\"Image\": \"outer_image\", \"HostConfig\": {\"NetworkMode\": \"host\"}}) as response:\n",
    "                if response.status != 201:\n",
    "                    raise Exception(\"Failed to create outer container on target host\")\n",
    "                container_id = (await response.json())['Id']\n",
    "\n",
    "            async with session.post(f\"http://{target_host['address']}:2375/containers/{container_id}/start\") as response:\n",
    "                if response.status != 204:\n",
    "                    raise Exception(\"Failed to start outer container on target host\")\n",
    "\n",
    "            async with session.post(f\"http://{target_host['address']}:2375/containers/{container_id}/restore\", \n",
    "                                    json={\"CheckpointDir\": checkpoint}) as response:\n",
    "                if response.status != 204:\n",
    "                    raise Exception(\"Failed to restore outer container from checkpoint\")\n",
    "\n",
    "        return container_id\n",
    "\n",
    "    async def _restore_inner_container(self, checkpoint: str, outer_id: str, target_host: Dict[str, Any]) -> str:\n",
    "        \"\"\"Restores the inner container inside the outer container on the target host.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(f\"http://{target_host['address']}:2375/containers/create\", \n",
    "                                    json={\"Image\": \"inner_image\", \"HostConfig\": {\"NetworkMode\": f\"container:{outer_id}\"}}) as response:\n",
    "                if response.status != 201:\n",
    "                    raise Exception(\"Failed to create inner container on target host\")\n",
    "                container_id = (await response.json())['Id']\n",
    "\n",
    "            async with session.post(f\"http://{target_host['address']}:2375/containers/{container_id}/start\") as response:\n",
    "                if response.status != 204:\n",
    "                    raise Exception(\"Failed to start inner container on target host\")\n",
    "\n",
    "            async with session.post(f\"http://{target_host['address']}:2375/containers/{container_id}/restore\", \n",
    "                                    json={\"CheckpointDir\": checkpoint}) as response:\n",
    "                if response.status != 204:\n",
    "                    raise Exception(\"Failed to restore inner container from checkpoint\")\n",
    "\n",
    "        return container_id\n",
    "\n",
    "    async def _verify_restoration(self, outer_id: str, inner_id: str, target_host: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Verifies that both outer and inner containers are running correctly after restoration.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for container_id in [outer_id, inner_id]:\n",
    "                async with session.get(f\"http://{target_host['address']}:2375/containers/{container_id}/json\") as response:\n",
    "                    if response.status != 200:\n",
    "                        return False\n",
    "                    container_info = await response.json()\n",
    "                    if container_info['State']['Status'] != 'running':\n",
    "                        return False\n",
    "        return True\n",
    "\n",
    "    async def _update_network_configuration(self, nested_container: Dict[str, Any], new_outer_id: str, new_inner_id: str, source_host: Dict[str, Any], target_host: Dict[str, Any]):\n",
    "        \"\"\"Updates network configuration for the migrated nested container.\"\"\"\n",
    "        # Update DNS records\n",
    "        await self.network_manager.update_dns_records(nested_container['id'], target_host['address'])\n",
    "\n",
    "        # Update load balancer\n",
    "        await self.network_manager.update_load_balancer(nested_container['id'], target_host['address'])\n",
    "\n",
    "        # Update SDN flow rules\n",
    "        await self.network_manager.update_sdn_flow_rules(nested_container['id'], source_host['address'], target_host['address'])\n",
    "\n",
    "    async def _cleanup_source(self, nested_container: Dict[str, Any], source_host: Dict[str, Any]):\n",
    "        \"\"\"Cleans up resources on the source host after successful migration.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for container_id in [nested_container['outer_id'], nested_container['inner_id']]:\n",
    "                async with session.post(f\"http://{source_host['address']}:2375/containers/{container_id}/stop\") as response:\n",
    "                    if response.status != 204:\n",
    "                        self.logger.warning(f\"Failed to stop container {container_id} on source host\")\n",
    "\n",
    "                async with session.delete(f\"http://{source_host['address']}:2375/containers/{container_id}\") as response:\n",
    "                    if response.status != 204:\n",
    "                        self.logger.warning(f\"Failed to remove container {container_id} on source host\")\n",
    "\n",
    "    async def _rollback_migration(self, nested_container: Dict[str, Any], source_host: Dict[str, Any], target_host: Dict[str, Any]):\n",
    "        \"\"\"Rolls back the migration if any step fails.\"\"\"\n",
    "        self.logger.info(f\"Rolling back migration for nested container {nested_container['id']}\")\n",
    "\n",
    "        # Stop and remove any containers created on the target host\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for container_type in ['outer', 'inner']:\n",
    "                async with session.get(f\"http://{target_host['address']}:2375/containers/json?filters={json.dumps({'name': [f'{nested_container['id']}_{container_type}']})}\", ) as response:\n",
    "                    if response.status == 200:\n",
    "                        containers = await response.json()\n",
    "                        for container in containers:\n",
    "                            container_id = container['Id']\n",
    "                            await session.post(f\"http://{target_host['address']}:2375/containers/{container_id}/stop\")\n",
    "                            await session.delete(f\"http://{target_host['address']}:2375/containers/{container_id}\")\n",
    "\n",
    "        # Restart the original containers on the source host if they were stopped\n",
    "        for container_id in [nested_container['outer_id'], nested_container['inner_id']]:\n",
    "            await self.docker_client.containers.get(container_id).start()\n",
    "\n",
    "        # Revert network changes\n",
    "        await self.network_manager.revert_network_changes(nested_container['id'], source_host['address'], target_host['address'])\n",
    "\n",
    "        self.logger.info(f\"Rollback completed for nested container {nested_container['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageManager\n",
    "\n",
    "import docker\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "import hashlib\n",
    "\n",
    "import docker\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "import hashlib\n",
    "from docker.errors import DockerException, APIError, ImageNotFound\n",
    "\n",
    "class ImageManager:\n",
    "    def __init__(self, runtime_interface):\n",
    "        self.runtime_interface = runtime_interface\n",
    "        self.client = docker.from_env()\n",
    "        self.logger = logging.getLogger('ImageManager')\n",
    "\n",
    "    def pull_image(self, image_ref):\n",
    "        try:\n",
    "            self.logger.info(f\"Pulling image: {image_ref}\")\n",
    "            image = self.client.images.pull(image_ref)\n",
    "            self.logger.info(f\"Successfully pulled image: {image_ref}\")\n",
    "            return image\n",
    "        except docker.errors.APIError as e:\n",
    "            self.logger.error(f\"Failed to pull image {image_ref}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def push_image(self, image_ref, repository):\n",
    "        try:\n",
    "            self.logger.info(f\"Pushing image {image_ref} to repository {repository}\")\n",
    "            push_output = self.client.images.push(repository, tag=image_ref.split(':')[-1])\n",
    "            self.logger.info(f\"Successfully pushed image {image_ref} to repository {repository}\")\n",
    "            return push_output\n",
    "        except docker.errors.APIError as e:\n",
    "            self.logger.error(f\"Failed to push image {image_ref} to repository {repository}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def optimize_image_distribution(self, source_runtime, target_runtime, image_ref):\n",
    "        try:\n",
    "            self.logger.info(f\"Optimizing distribution of image {image_ref}\")\n",
    "            if self.image_exists(target_runtime, image_ref):\n",
    "                self.logger.debug(f\"Image {image_ref} exists on target, computing diff\")\n",
    "                diff_layers = self._compute_image_diff(source_runtime, target_runtime, image_ref)\n",
    "                self.logger.debug(f\"Transferring {len(diff_layers)} diff layers for {image_ref}\")\n",
    "                self._transfer_image_diff(diff_layers, target_runtime, image_ref)\n",
    "                self.logger.debug(f\"Applying image diff for {image_ref} on target\")\n",
    "                self._apply_image_diff(diff_layers, target_runtime, image_ref)\n",
    "            else:\n",
    "                self.logger.debug(f\"Image {image_ref} does not exist on target, pulling full image\")\n",
    "                self.pull_image(image_ref)\n",
    "            self.logger.info(f\"Successfully optimized distribution of image {image_ref}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to optimize distribution of image {image_ref}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def image_exists(self, runtime, image_ref):\n",
    "        try:\n",
    "            runtime.images.get(image_ref)\n",
    "            self.logger.debug(f\"Image {image_ref} exists\")\n",
    "            return True\n",
    "        except docker.errors.ImageNotFound:\n",
    "            self.logger.debug(f\"Image {image_ref} not found\")\n",
    "            return False\n",
    "\n",
    "    def _compute_image_diff(self, source_runtime, target_runtime, image_ref):\n",
    "        source_layers = self._get_image_layers(source_runtime, image_ref)\n",
    "        target_layers = self._get_image_layers(target_runtime, image_ref)\n",
    "        diff_layers = [layer for layer in source_layers if layer not in target_layers]\n",
    "        self.logger.debug(f\"Computed {len(diff_layers)} diff layers\")\n",
    "        return diff_layers\n",
    "\n",
    "    def _get_image_layers(self, runtime, image_ref):\n",
    "        image = runtime.images.get(image_ref)\n",
    "        return image.history()\n",
    "\n",
    "    def _transfer_image_diff(self, diff_layers, target_runtime, image_ref):\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Save diff layers to tar files\n",
    "            tar_files = []\n",
    "            for layer in diff_layers:\n",
    "                layer_id = layer['Id']\n",
    "                tar_path = os.path.join(temp_dir, f\"{layer_id}.tar\")\n",
    "                self.client.images.get(layer_id).save(tar_path)\n",
    "                tar_files.append(tar_path)\n",
    "            \n",
    "            # Transfer tar files to target runtime\n",
    "            for tar_file in tar_files:\n",
    "                with open(tar_file, 'rb') as f:\n",
    "                    target_runtime.images.load(f.read())\n",
    "\n",
    "    def _apply_image_diff(self, diff_layers, target_runtime, image_ref):\n",
    "        # Create a new image from the base image and apply diff layers\n",
    "        base_image = target_runtime.images.get(image_ref)\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            dockerfile = f\"FROM {image_ref}\\n\"\n",
    "            for layer in diff_layers:\n",
    "                layer_id = layer['Id']\n",
    "                dockerfile += f\"ADD {layer_id}.tar /\\n\"\n",
    "            \n",
    "            dockerfile_path = os.path.join(temp_dir, \"Dockerfile\")\n",
    "            with open(dockerfile_path, \"w\") as f:\n",
    "                f.write(dockerfile)\n",
    "            \n",
    "            new_image, _ = target_runtime.images.build(path=temp_dir, dockerfile=dockerfile_path, tag=image_ref)\n",
    "        \n",
    "        return new_image\n",
    "\n",
    "    def deduplicate_layers(self, image_ref):\n",
    "        try:\n",
    "            self.logger.info(f\"Deduplicating layers for image {image_ref}\")\n",
    "            image = self.client.images.get(image_ref)\n",
    "            layers = image.history()\n",
    "            unique_layers = {}\n",
    "            \n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                for layer in layers:\n",
    "                    layer_id = layer['Id']\n",
    "                    layer_content = self._get_layer_content(layer_id, temp_dir)\n",
    "                    layer_hash = self._compute_layer_hash(layer_content)\n",
    "                    if layer_hash not in unique_layers:\n",
    "                        unique_layers[layer_hash] = layer_id\n",
    "            \n",
    "            self.logger.debug(f\"Found {len(unique_layers)} unique layers out of {len(layers)} total layers\")\n",
    "            new_image = self._rebuild_image_with_unique_layers(image_ref, list(unique_layers.values()))\n",
    "            self.logger.info(f\"Successfully deduplicated layers for image {image_ref}\")\n",
    "            return new_image\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to deduplicate layers for image {image_ref}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _get_layer_content(self, layer_id, temp_dir):\n",
    "        layer_tar = os.path.join(temp_dir, f\"{layer_id}.tar\")\n",
    "        self.client.images.get(layer_id).save(layer_tar)\n",
    "        return layer_tar\n",
    "\n",
    "    def _compute_layer_hash(self, layer_tar):\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        with open(layer_tar, \"rb\") as f:\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "\n",
    "    def _rebuild_image_with_unique_layers(self, image_ref, unique_layer_ids):\n",
    "        self.logger.debug(f\"Rebuilding image {image_ref} with {len(unique_layer_ids)} unique layers\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Create a new Dockerfile\n",
    "            dockerfile = f\"FROM scratch\\n\"\n",
    "            for layer_id in unique_layer_ids:\n",
    "                dockerfile += f\"ADD {layer_id}.tar /\\n\"\n",
    "            \n",
    "            dockerfile_path = os.path.join(temp_dir, \"Dockerfile\")\n",
    "            with open(dockerfile_path, \"w\") as f:\n",
    "                f.write(dockerfile)\n",
    "            \n",
    "            # Save layer tars\n",
    "            for layer_id in unique_layer_ids:\n",
    "                layer_tar = os.path.join(temp_dir, f\"{layer_id}.tar\")\n",
    "                self.client.images.get(layer_id).save(layer_tar)\n",
    "            \n",
    "            # Build new image\n",
    "            new_image, _ = self.client.images.build(path=temp_dir, dockerfile=dockerfile_path, tag=image_ref)\n",
    "        \n",
    "        self.logger.debug(f\"Successfully rebuilt image {image_ref} with unique layers\")\n",
    "        return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Manager Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetworkManager\n",
    "import networkx as nx\n",
    "from typing import Tuple\n",
    "from os_ken.base import app_manager\n",
    "from os_ken.ofproto import ofproto_v1_3\n",
    "from os_ken.controller.handler import set_ev_cls\n",
    "from os_ken.controller import ofp_event\n",
    "from os_ken.controller.handler import MAIN_DISPATCHER, CONFIG_DISPATCHER\n",
    "from os_ken.lib.packet import packet, ethernet, arp, ipv4, tcp, udp\n",
    "from os_ken.topology import event, switches\n",
    "from os_ken.topology.api import get_switch, get_link\n",
    "from os_ken.lib.packet import packet\n",
    "from os_ken.lib.packet import ethernet\n",
    "from os_ken.lib.packet import ether_types\n",
    "\n",
    "class NetworkManager(app_manager.OSKenApp):\n",
    "    OFP_VERSIONS = [ofproto_v1_3.OFP_VERSION]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NetworkManager, self).__init__(*args, **kwargs)\n",
    "        self.network_graph = nx.Graph()\n",
    "        self.mac_to_port = {}\n",
    "        self.containers = {}\n",
    "        self.host_resources = {}\n",
    "        self.migration_manager = None\n",
    "\n",
    "    def set_migration_manager(self, migration_manager):\n",
    "        self.migration_manager = migration_manager\n",
    "        \n",
    "    def get_container(self, container_id: str) -> Any:\n",
    "        return self.containers.get(container_id)\n",
    "\n",
    "    def get_cpu_cost(self, host_id: str) -> float:\n",
    "        return self.host_resources.get(host_id, {}).get('cpu_cost', 1.0)\n",
    "\n",
    "    def get_memory_cost(self, host_id: str) -> float:\n",
    "        return self.host_resources.get(host_id, {}).get('memory_cost', 1.0)\n",
    "\n",
    "    def get_link_latency(self, link: Tuple[str, str]) -> float:\n",
    "        return self.network_graph.edges[link].get('latency', 0.001)  # Default to 1ms if not set\n",
    "    def get_path_bandwidth(self, path: List[str]) -> List[float]:\n",
    "        return [self.network_graph.edges[link].get('bandwidth', float('inf')) for link in zip(path, path[1:])]\n",
    "    \n",
    "    @set_ev_cls(ofp_event.EventOFPSwitchFeatures, CONFIG_DISPATCHER)\n",
    "    def switch_features_handler(self, ev):\n",
    "        datapath = ev.msg.datapath\n",
    "        ofproto = datapath.ofproto\n",
    "        parser = datapath.ofproto_parser\n",
    "\n",
    "        # Install table-miss flow entry\n",
    "        match = parser.OFPMatch()\n",
    "        actions = [parser.OFPActionOutput(ofproto.OFPP_CONTROLLER,\n",
    "                                          ofproto.OFPCML_NO_BUFFER)]\n",
    "        self.add_flow(datapath, 0, match, actions)\n",
    "\n",
    "    @set_ev_cls(event.EventSwitchLeave)\n",
    "    def switch_leave_handler(self, ev):\n",
    "        switch = ev.switch\n",
    "        self.network_graph.remove_node(switch.dp.id)\n",
    "        self.logger.info(f\"Switch {switch.dp.id} removed from the network graph\")\n",
    "\n",
    "    @set_ev_cls(event.EventLinkAdd)\n",
    "    def link_add_handler(self, ev):\n",
    "        link = ev.link\n",
    "        src_dp = link.src.dpid\n",
    "        dst_dp = link.dst.dpid\n",
    "        self.network_graph.add_edge(src_dp, dst_dp, port=link.src.port_no)\n",
    "        self.network_graph.add_edge(dst_dp, src_dp, port=link.dst.port_no)\n",
    "        self.logger.info(f\"Link {link} added to the network graph\")\n",
    "\n",
    "    @set_ev_cls(event.EventLinkDelete)\n",
    "    def link_delete_handler(self, ev):\n",
    "        link = ev.link\n",
    "        src_dp = link.src.dpid\n",
    "        dst_dp = link.dst.dpid\n",
    "        self.network_graph.remove_edge(src_dp, dst_dp)\n",
    "        self.network_graph.remove_edge(dst_dp, src_dp)\n",
    "        self.logger.info(f\"Link {link} removed from the network graph\")\n",
    "\n",
    "    def compute_optimal_path(self, src_dp: str, dst_dp: str) -> List[str]:\n",
    "        try:\n",
    "            # Use Dijkstra's algorithm to find the shortest path based on latency\n",
    "            path = nx.shortest_path(self.network_graph, src_dp, dst_dp, weight='latency')\n",
    "            self.logger.info(f\"Optimal path computed: {path}\")\n",
    "            return path\n",
    "        except nx.NetworkXNoPath:\n",
    "            self.logger.error(f\"No path found between {src_dp} and {dst_dp}\")\n",
    "            return None\n",
    "\n",
    "    def update_flow_tables(self, path: List[str], bandwidth: float):\n",
    "        for i in range(len(path) - 1):\n",
    "            src_dp = path[i]\n",
    "            dst_dp = path[i+1]\n",
    "            out_port = self.network_graph[src_dp][dst_dp]['port']\n",
    "            self.add_flow(src_dp, dst_dp, out_port, bandwidth)\n",
    "\n",
    "    def add_flow(self, src_dp: str, dst_dp: str, out_port: int, bandwidth: float):\n",
    "        datapath = self.datapaths[src_dp]\n",
    "        ofproto = datapath.ofproto\n",
    "        parser = datapath.ofproto_parser\n",
    "\n",
    "        match = parser.OFPMatch(in_port=1, eth_dst=dst_dp)\n",
    "        actions = [parser.OFPActionOutput(out_port)]\n",
    "        inst = [parser.OFPInstructionActions(ofproto.OFPIT_APPLY_ACTIONS, actions)]\n",
    "        \n",
    "        mod = parser.OFPFlowMod(\n",
    "            datapath=datapath,\n",
    "            priority=1,\n",
    "            match=match,\n",
    "            instructions=inst,\n",
    "            hard_timeout=300,  # Flow expires after 5 minutes\n",
    "            flags=ofproto.OFPFF_SEND_FLOW_REM  # Send flow removed message when flow expires\n",
    "        )\n",
    "        datapath.send_msg(mod)\n",
    "\n",
    "        # Apply QoS for bandwidth limitation\n",
    "        queue_id = self.create_queue(datapath, out_port, bandwidth)\n",
    "        match = parser.OFPMatch(in_port=1, eth_dst=dst_dp)\n",
    "        actions = [parser.OFPActionSetQueue(queue_id), parser.OFPActionOutput(out_port)]\n",
    "        inst = [parser.OFPInstructionActions(ofproto.OFPIT_APPLY_ACTIONS, actions)]\n",
    "        mod = parser.OFPFlowMod(\n",
    "            datapath=datapath,\n",
    "            priority=2,\n",
    "            match=match,\n",
    "            instructions=inst,\n",
    "            hard_timeout=300,\n",
    "            flags=ofproto.OFPFF_SEND_FLOW_REM\n",
    "        )\n",
    "        datapath.send_msg(mod)\n",
    "\n",
    "\n",
    "    def handle_migration_request(self, migration_request):\n",
    "        src_host = migration_request.source_host\n",
    "        dst_host = migration_request.destination_host\n",
    "        container = self.migration_manager.decision_engine.resource_optimizer.select_best_host(\n",
    "            self.migration_manager.decision_engine.resource_optimizer.hosts, \n",
    "            self.migration_manager.decision_engine.resource_optimizer.containers.get(migration_request.container_id)\n",
    "        )\n",
    "        # Initiate traffic redirection\n",
    "        self.traffic_redirector.initiate_traffic_redirection(\n",
    "            migration_request.container_id, src_host, dst_host\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDNController\n",
    "import ether_types\n",
    "\n",
    "class SDNControllerInterface:\n",
    "    def __init__(self, network_manager):\n",
    "        self.network_manager = network_manager\n",
    "\n",
    "    def configure_network_path(self, src_dp, dst_dp, flow_rules):\n",
    "        path = self.network_manager.compute_optimal_path(src_dp, dst_dp, flow_rules['bandwidth'])\n",
    "        if not path:\n",
    "            self.network_manager.logger.error(f\"No path found between {src_dp} and {dst_dp}\")\n",
    "            return None\n",
    "        self.network_manager.update_flow_tables(path, flow_rules['bandwidth'])\n",
    "        return path\n",
    "\n",
    "    async def update_network_policies(self, container_id: str, new_location: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Updates network policies for a migrated container.\n",
    "\n",
    "        Args:\n",
    "            container_id: The ID of the migrated container.\n",
    "            new_location: A dictionary containing information about the new location of the container.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if policies were successfully updated, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get the container's current policies\n",
    "            policies = self.container_policies.get(container_id, [])\n",
    "            if not policies:\n",
    "                self.logger.warning(f\"No policies found for container {container_id}\")\n",
    "                return True  # No policies to update\n",
    "\n",
    "            new_switch_dpid = new_location['switch_dpid']\n",
    "            new_port = new_location['port']\n",
    "            new_ip = new_location['ip']\n",
    "\n",
    "            # Update the IP to MAC mapping\n",
    "            new_mac = await self._get_container_mac(container_id, new_location)\n",
    "            self.ip_to_mac[new_ip] = new_mac\n",
    "\n",
    "            for policy in policies:\n",
    "                await self._update_policy(policy, container_id, new_switch_dpid, new_port, new_ip, new_mac)\n",
    "\n",
    "            self.logger.info(f\"Successfully updated network policies for container {container_id}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error updating network policies for container {container_id}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    async def _update_policy(self, policy: Dict[str, Any], container_id: str, new_switch_dpid: str, new_port: int, new_ip: str, new_mac: str):\n",
    "        \"\"\"\n",
    "        Updates a single network policy for the migrated container.\n",
    "        \"\"\"\n",
    "        policy_type = policy['type']\n",
    "        if policy_type == 'ingress':\n",
    "            await self._update_ingress_policy(policy, new_switch_dpid, new_port, new_ip, new_mac)\n",
    "        elif policy_type == 'egress':\n",
    "            await self._update_egress_policy(policy, new_switch_dpid, new_port, new_ip, new_mac)\n",
    "        else:\n",
    "            self.logger.warning(f\"Unknown policy type {policy_type} for container {container_id}\")\n",
    "\n",
    "    async def _update_ingress_policy(self, policy: Dict[str, Any], new_switch_dpid: str, new_port: int, new_ip: str, new_mac: str):\n",
    "        \"\"\"\n",
    "        Updates an ingress policy for the migrated container.\n",
    "        \"\"\"\n",
    "        datapath = self.dpset.get(new_switch_dpid)\n",
    "        ofproto = datapath.ofproto\n",
    "        parser = datapath.ofproto_parser\n",
    "\n",
    "        # Create a match for incoming traffic to the container\n",
    "        match = parser.OFPMatch(\n",
    "            eth_type=ether_types.ETH_TYPE_IP,\n",
    "            ipv4_dst=new_ip\n",
    "        )\n",
    "\n",
    "        # If the policy specifies a source IP or port, add it to the match\n",
    "        if 'src_ip' in policy:\n",
    "            match.set_dl_src(self.ip_to_mac[policy['src_ip']])\n",
    "            match.set_ipv4_src(policy['src_ip'])\n",
    "        if 'src_port' in policy:\n",
    "            if policy.get('protocol') == 'tcp':\n",
    "                match.set_tcp_src(policy['src_port'])\n",
    "            elif policy.get('protocol') == 'udp':\n",
    "                match.set_udp_src(policy['src_port'])\n",
    "\n",
    "        # Create actions to forward the packet to the container\n",
    "        actions = [\n",
    "            parser.OFPActionSetField(eth_dst=new_mac),\n",
    "            parser.OFPActionOutput(new_port)\n",
    "        ]\n",
    "\n",
    "        # Install the flow rule\n",
    "        self._add_flow(datapath, 1, match, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNS Controller Manager\n",
    "import dns\n",
    "\n",
    "class DNSManager:\n",
    "    def __init__(self, dns_server: str, zone: str, ttl: int = 60, key_name: str = None, key_secret: str = None):\n",
    "        self.dns_server = dns_server\n",
    "        self.zone = zone\n",
    "        self.ttl = ttl\n",
    "        self.resolver = aiodns.DNSResolver(nameservers=[dns_server])\n",
    "        self.keyring = None\n",
    "        if key_name and key_secret:\n",
    "            self.keyring = dns.tsigkeyring.from_text({key_name: key_secret})\n",
    "        self.etcd_client = None  # Will be initialized in connect_to_etcd method\n",
    "        self.migration_records = {}\n",
    "\n",
    "    async def connect_to_etcd(self, etcd_host: str, etcd_port: int):\n",
    "        \"\"\"Connects to etcd for distributed DNS record management.\"\"\"\n",
    "        self.etcd_client = aiohttp.ClientSession()\n",
    "        self.etcd_url = f\"http://{etcd_host}:{etcd_port}/v2/keys/dns\"\n",
    "\n",
    "    async def close(self):\n",
    "        \"\"\"Closes the etcd client session.\"\"\"\n",
    "        if self.etcd_client:\n",
    "            await self.etcd_client.close()\n",
    "\n",
    "    async def update_dns_record(self, container_id: str, new_ip: str, hostname: str) -> bool:\n",
    "\n",
    "        try:\n",
    "            # Prepare the DNS update message\n",
    "            update = dns.update.Update(self.zone, keyring=self.keyring)\n",
    "            \n",
    "            # Remove the old record if it exists\n",
    "            update.delete(hostname)\n",
    "            \n",
    "            # Add the new A record\n",
    "            update.add(hostname, self.ttl, 'A', new_ip)\n",
    "            \n",
    "            # Send the update to the DNS server\n",
    "            response = dns.query.tcp(update, self.dns_server)\n",
    "            \n",
    "            if response.rcode() == 0:\n",
    "                print(f\"DNS record updated successfully for {hostname}\")\n",
    "                \n",
    "                # Store the migration record in etcd\n",
    "                await self._store_migration_record(container_id, hostname, new_ip)\n",
    "                \n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Failed to update DNS record for {hostname}. RCODE: {response.rcode()}\")\n",
    "                return False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating DNS record: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    async def prepare_dns_migration(self, container_id: str, source_ip: str, target_ip: str) -> str:\n",
    "\n",
    "        try:\n",
    "            # Get the original hostname for the container\n",
    "            original_hostname = await self._get_hostname_for_ip(source_ip)\n",
    "            if not original_hostname:\n",
    "                raise Exception(f\"No hostname found for IP {source_ip}\")\n",
    "\n",
    "            # Create a temporary CNAME\n",
    "            temp_hostname = f\"{container_id}-temp.{self.zone}\"\n",
    "            \n",
    "            # Create CNAME record pointing to the original hostname\n",
    "            update = dns.update.Update(self.zone, keyring=self.keyring)\n",
    "            update.add(temp_hostname, self.ttl, 'CNAME', original_hostname)\n",
    "            \n",
    "            response = dns.query.tcp(update, self.dns_server)\n",
    "            \n",
    "            if response.rcode() == 0:\n",
    "                print(f\"Temporary CNAME {temp_hostname} created successfully\")\n",
    "                \n",
    "                # Store migration information\n",
    "                self.migration_records[container_id] = {\n",
    "                    'original_hostname': original_hostname,\n",
    "                    'temp_hostname': temp_hostname,\n",
    "                    'new_ip': target_ip\n",
    "                }\n",
    "                \n",
    "                return temp_hostname\n",
    "            else:\n",
    "                raise Exception(f\"Failed to create temporary CNAME. RCODE: {response.rcode()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing DNS migration: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def finalize_dns_migration(self, container_id: str) -> bool:\n",
    "\n",
    "        try:\n",
    "            if container_id not in self.migration_records:\n",
    "                raise Exception(f\"No migration record found for container {container_id}\")\n",
    "\n",
    "            migration_info = self.migration_records[container_id]\n",
    "            original_hostname = migration_info['original_hostname']\n",
    "            temp_hostname = migration_info['temp_hostname']\n",
    "            new_ip = migration_info['new_ip']\n",
    "\n",
    "            # Update the original hostname to point to the new IP\n",
    "            update = dns.update.Update(self.zone, keyring=self.keyring)\n",
    "            update.delete(original_hostname, 'A')\n",
    "            update.add(original_hostname, self.ttl, 'A', new_ip)\n",
    "            \n",
    "            # Remove the temporary CNAME\n",
    "            update.delete(temp_hostname, 'CNAME')\n",
    "\n",
    "            response = dns.query.tcp(update, self.dns_server)\n",
    "            \n",
    "            if response.rcode() == 0:\n",
    "                print(f\"DNS migration finalized successfully for {original_hostname}\")\n",
    "                del self.migration_records[container_id]\n",
    "                return True\n",
    "            else:\n",
    "                raise Exception(f\"Failed to finalize DNS migration. RCODE: {response.rcode()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error finalizing DNS migration: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    async def _get_hostname_for_ip(self, ip: str) -> str:\n",
    "\n",
    "        try:\n",
    "            result = await self.resolver.gethostbyaddr(ip)\n",
    "            return result.name\n",
    "        except aiodns.error.DNSError:\n",
    "            return None\n",
    "\n",
    "    async def _store_migration_record(self, container_id: str, hostname: str, new_ip: str):\n",
    "\n",
    "        if not self.etcd_client:\n",
    "            print(\"etcd client not initialized. Skipping record storage.\")\n",
    "            return\n",
    "\n",
    "        key = f\"{self.etcd_url}/{container_id}\"\n",
    "        value = json.dumps({\n",
    "            \"hostname\": hostname,\n",
    "            \"ip\": new_ip,\n",
    "            \"timestamp\": int(time.time())\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            async with self.etcd_client.put(key, data=value) as response:\n",
    "                if response.status != 200:\n",
    "                    print(f\"Failed to store migration record in etcd. Status: {response.status}\")\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error storing migration record in etcd: {str(e)}\")\n",
    "\n",
    "    async def get_migration_records(self) -> List[Dict[str, Any]]:\n",
    "\n",
    "        if not self.etcd_client:\n",
    "            print(\"etcd client not initialized. Cannot retrieve records.\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            async with self.etcd_client.get(self.etcd_url) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    records = []\n",
    "                    for node in data.get('node', {}).get('nodes', []):\n",
    "                        record = json.loads(node['value'])\n",
    "                        record['container_id'] = node['key'].split('/')[-1]\n",
    "                        records.append(record)\n",
    "                    return records\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve migration records from etcd. Status: {response.status}\")\n",
    "                    return []\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error retrieving migration records from etcd: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    async def cleanup_stale_records(self, max_age: int = 3600):\n",
    "\n",
    "        records = await self.get_migration_records()\n",
    "        current_time = int(time.time())\n",
    "\n",
    "        for record in records:\n",
    "            if current_time - record['timestamp'] > max_age:\n",
    "                container_id = record['container_id']\n",
    "                key = f\"{self.etcd_url}/{container_id}\"\n",
    "                try:\n",
    "                    async with self.etcd_client.delete(key) as response:\n",
    "                        if response.status == 200:\n",
    "                            print(f\"Cleaned up stale record for container {container_id}\")\n",
    "                        else:\n",
    "                            print(f\"Failed to clean up stale record for container {container_id}. Status: {response.status}\")\n",
    "                except aiohttp.ClientError as e:\n",
    "                    print(f\"Error cleaning up stale record for container {container_id}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Traffic Manager\n",
    "class TrafficRedirector:\n",
    "    def __init__(self, network_manager):\n",
    "        self.network_manager = network_manager\n",
    "\n",
    "    def initiate_traffic_redirection(self, container_id, src_host, dst_host):\n",
    "        # Implement traffic redirection logic using SDN rules\n",
    "        print(f\"Initiating traffic redirection for container {container_id} from {src_host.host_id} to {dst_host.host_id}\")\n",
    "        # Example: Modify flow rules to redirect traffic\n",
    "        migration_flow_rules = {\n",
    "            'bandwidth': 100 \n",
    "        }\n",
    "        path = self.network_manager.compute_optimal_path(src_host.host_id, dst_host.host_id, migration_flow_rules['bandwidth'])\n",
    "        if path:\n",
    "            self.network_manager.update_flow_tables(path, migration_flow_rules['bandwidth'])\n",
    "            print(f\"Traffic redirected for container {container_id}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to redirect traffic for container {container_id}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Synchronizer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint Controller\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import zlib\n",
    "import hashlib\n",
    "import tempfile\n",
    "from typing import Dict, Any\n",
    "import asyncio\n",
    "import aiofiles\n",
    "import docker\n",
    "\n",
    "class CheckpointingModule:\n",
    "    def __init__(self, container_runtime, checkpoint_dir, compression_level=6):\n",
    "        self.container_runtime = container_runtime\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.compression_level = compression_level\n",
    "        self.docker_client = docker.from_env()\n",
    "\n",
    "    async def create_checkpoint(self, container: Any, checkpoint_type: str = 'full') -> str:\n",
    "        \"\"\"\n",
    "        Creates a checkpoint for the given container.\n",
    "        \n",
    "        Args:\n",
    "            container: The container object to checkpoint.\n",
    "            checkpoint_type: Type of checkpoint ('full' or 'incremental').\n",
    "        \n",
    "        Returns:\n",
    "            The ID of the created checkpoint.\n",
    "        \n",
    "        Raises:\n",
    "            Exception: If checkpoint creation fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            container_id = container.container_id\n",
    "            checkpoint_id = f\"{container_id}_{checkpoint_type}_{int(time.time())}\"\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_id)\n",
    "\n",
    "            # Ensure checkpoint directory exists\n",
    "            os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "            # Prepare checkpoint options\n",
    "            checkpoint_options = [\n",
    "                \"--checkpoint-dir\", checkpoint_path,\n",
    "                \"--leave-running\",  # Keep the container running during checkpoint\n",
    "            ]\n",
    "\n",
    "            if checkpoint_type == 'incremental':\n",
    "                checkpoint_options.append(\"--previous-checkpoint\")\n",
    "                checkpoint_options.append(self._get_latest_checkpoint(container_id))\n",
    "\n",
    "            # Create checkpoint using container runtime\n",
    "            if self.container_runtime == 'docker':\n",
    "                await self._docker_checkpoint(container_id, checkpoint_id, checkpoint_options)\n",
    "            elif self.container_runtime == 'criu':\n",
    "                await self._criu_checkpoint(container_id, checkpoint_path)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported container runtime: {self.container_runtime}\")\n",
    "\n",
    "            # Collect and store container metadata\n",
    "            metadata = await self._collect_container_metadata(container)\n",
    "            await self._store_metadata(checkpoint_path, metadata)\n",
    "\n",
    "            print(f\"Checkpoint {checkpoint_id} created successfully\")\n",
    "            return checkpoint_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating checkpoint for container {container.container_id}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def optimize_checkpoint(self, checkpoint_id: str) -> str:\n",
    "\n",
    "        try:\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_id)\n",
    "            optimized_checkpoint_id = f\"{checkpoint_id}_optimized\"\n",
    "            optimized_checkpoint_path = os.path.join(self.checkpoint_dir, optimized_checkpoint_id)\n",
    "\n",
    "            # Ensure optimized checkpoint directory exists\n",
    "            os.makedirs(optimized_checkpoint_path, exist_ok=True)\n",
    "\n",
    "            # Compress checkpoint files\n",
    "            await self._compress_checkpoint_files(checkpoint_path, optimized_checkpoint_path)\n",
    "\n",
    "            # Deduplicate data\n",
    "            await self._deduplicate_checkpoint_data(optimized_checkpoint_path)\n",
    "\n",
    "            # Verify optimized checkpoint\n",
    "            if not await self._verify_optimized_checkpoint(checkpoint_path, optimized_checkpoint_path):\n",
    "                raise Exception(\"Optimized checkpoint verification failed\")\n",
    "\n",
    "            print(f\"Checkpoint {checkpoint_id} optimized successfully\")\n",
    "            return optimized_checkpoint_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error optimizing checkpoint {checkpoint_id}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def _docker_checkpoint(self, container_id: str, checkpoint_id: str, options: List[str]):\n",
    "        \"\"\"\n",
    "        Creates a checkpoint using Docker.\n",
    "        \"\"\"\n",
    "        cmd = [\"docker\", \"checkpoint\", \"create\"] + options + [container_id, checkpoint_id]\n",
    "        process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n",
    "        stdout, stderr = await process.communicate()\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            raise Exception(f\"Docker checkpoint creation failed: {stderr.decode()}\")\n",
    "\n",
    "    async def _criu_checkpoint(self, container_id: str, checkpoint_path: str):\n",
    "        \"\"\"\n",
    "        Creates a checkpoint using CRIU.\n",
    "        \"\"\"\n",
    "        cmd = [\n",
    "            \"criu\", \"dump\",\n",
    "            \"-t\", container_id,\n",
    "            \"-D\", checkpoint_path,\n",
    "            \"--shell-job\",\n",
    "            \"--leave-running\",\n",
    "            \"--manage-cgroups\"\n",
    "        ]\n",
    "        process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n",
    "        stdout, stderr = await process.communicate()\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            raise Exception(f\"CRIU checkpoint creation failed: {stderr.decode()}\")\n",
    "\n",
    "    async def _collect_container_metadata(self, container: Any) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Collects metadata about the container.\n",
    "        \"\"\"\n",
    "        inspect_data = await self.docker_client.api.inspect_container(container.container_id)\n",
    "        return {\n",
    "            \"id\": container.container_id,\n",
    "            \"name\": inspect_data[\"Name\"],\n",
    "            \"image\": inspect_data[\"Config\"][\"Image\"],\n",
    "            \"env\": inspect_data[\"Config\"][\"Env\"],\n",
    "            \"cmd\": inspect_data[\"Config\"][\"Cmd\"],\n",
    "            \"volumes\": inspect_data[\"Mounts\"],\n",
    "            \"network_settings\": inspect_data[\"NetworkSettings\"],\n",
    "        }\n",
    "\n",
    "    async def _store_metadata(self, checkpoint_path: str, metadata: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Stores container metadata in the checkpoint directory.\n",
    "        \"\"\"\n",
    "        metadata_path = os.path.join(checkpoint_path, \"metadata.json\")\n",
    "        async with aiofiles.open(metadata_path, 'w') as f:\n",
    "            await f.write(json.dumps(metadata, indent=2))\n",
    "\n",
    "    async def _compress_checkpoint_files(self, source_path: str, destination_path: str):\n",
    "        \"\"\"\n",
    "        Compresses checkpoint files using zlib.\n",
    "        \"\"\"\n",
    "        for root, _, files in os.walk(source_path):\n",
    "            for file in files:\n",
    "                source_file = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(source_file, source_path)\n",
    "                destination_file = os.path.join(destination_path, relative_path + '.zlib')\n",
    "\n",
    "                async with aiofiles.open(source_file, 'rb') as sf, aiofiles.open(destination_file, 'wb') as df:\n",
    "                    data = await sf.read()\n",
    "                    compressed_data = zlib.compress(data, level=self.compression_level)\n",
    "                    await df.write(compressed_data)\n",
    "\n",
    "    async def _deduplicate_checkpoint_data(self, checkpoint_path: str):\n",
    "        \"\"\"\n",
    "        Deduplicates data within the checkpoint using content-defined chunking.\n",
    "        \"\"\"\n",
    "        chunk_size = 4096  # 4KB chunks\n",
    "        chunks = {}\n",
    "\n",
    "        for root, _, files in os.walk(checkpoint_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                async with aiofiles.open(file_path, 'rb') as f:\n",
    "                    while chunk := await f.read(chunk_size):\n",
    "                        chunk_hash = hashlib.sha256(chunk).hexdigest()\n",
    "                        if chunk_hash not in chunks:\n",
    "                            chunks[chunk_hash] = chunk\n",
    "                \n",
    "                # Rewrite file with deduplicated chunks\n",
    "                async with aiofiles.open(file_path, 'wb') as f:\n",
    "                    async with aiofiles.open(file_path, 'rb') as original:\n",
    "                        while chunk := await original.read(chunk_size):\n",
    "                            chunk_hash = hashlib.sha256(chunk).hexdigest()\n",
    "                            await f.write(chunks[chunk_hash])\n",
    "\n",
    "    async def _verify_optimized_checkpoint(self, original_path: str, optimized_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Verifies the integrity of the optimized checkpoint.\n",
    "        \"\"\"\n",
    "        original_files = set(os.listdir(original_path))\n",
    "        optimized_files = set(file.rstrip('.zlib') for file in os.listdir(optimized_path))\n",
    "\n",
    "        if original_files != optimized_files:\n",
    "            return False\n",
    "\n",
    "        for file in original_files:\n",
    "            original_file = os.path.join(original_path, file)\n",
    "            optimized_file = os.path.join(optimized_path, file + '.zlib')\n",
    "\n",
    "            async with aiofiles.open(original_file, 'rb') as of, aiofiles.open(optimized_file, 'rb') as opf:\n",
    "                original_data = await of.read()\n",
    "                optimized_data = zlib.decompress(await opf.read())\n",
    "\n",
    "                if original_data != optimized_data:\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _get_latest_checkpoint(self, container_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves the latest checkpoint for a given container.\n",
    "        \"\"\"\n",
    "        checkpoints = [cp for cp in os.listdir(self.checkpoint_dir) if cp.startswith(container_id)]\n",
    "        if not checkpoints:\n",
    "            raise Exception(f\"No previous checkpoints found for container {container_id}\")\n",
    "        return max(checkpoints, key=lambda cp: os.path.getctime(os.path.join(self.checkpoint_dir, cp)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta Tracker\n",
    "\n",
    "import time\n",
    "\n",
    "class DeltaTracker:\n",
    "    def __init__(self):\n",
    "        self.deltas = {}\n",
    "        self.checkpoints = {}\n",
    "\n",
    "    def record_delta(self, container_id, delta):\n",
    "        if container_id not in self.deltas:\n",
    "            self.deltas[container_id] = []\n",
    "        timestamp = time.time()\n",
    "        self.deltas[container_id].append((timestamp, delta))\n",
    "\n",
    "    def create_checkpoint(self, container_id):\n",
    "        checkpoint_id = f\"{container_id}_{time.time()}\"\n",
    "        self.checkpoints[checkpoint_id] = time.time()\n",
    "        return checkpoint_id\n",
    "\n",
    "    def get_deltas_since_checkpoint(self, container_id, checkpoint_id):\n",
    "        if checkpoint_id not in self.checkpoints:\n",
    "            raise ValueError(f\"Checkpoint {checkpoint_id} not found\")\n",
    "        \n",
    "        checkpoint_time = self.checkpoints[checkpoint_id]\n",
    "        \n",
    "        if container_id not in self.deltas:\n",
    "            return []\n",
    "        \n",
    "        return [delta for timestamp, delta in self.deltas[container_id] if timestamp > checkpoint_time]\n",
    "\n",
    "    def clear_old_deltas(self, container_id, checkpoint_id):\n",
    "        if checkpoint_id not in self.checkpoints:\n",
    "            raise ValueError(f\"Checkpoint {checkpoint_id} not found\")\n",
    "        \n",
    "        checkpoint_time = self.checkpoints[checkpoint_id]\n",
    "        \n",
    "        if container_id in self.deltas:\n",
    "            self.deltas[container_id] = [(t, d) for t, d in self.deltas[container_id] if t > checkpoint_time]\n",
    "\n",
    "    def remove_checkpoint(self, checkpoint_id):\n",
    "        if checkpoint_id in self.checkpoints:\n",
    "            del self.checkpoints[checkpoint_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Restoration\n",
    "import shutil\n",
    "\n",
    "class StateRestorationModule:\n",
    "    def __init__(self, container_runtime: str, checkpoint_dir: str):\n",
    "        self.container_runtime = container_runtime\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.docker_client = docker.from_env()\n",
    "\n",
    "    async def restore_state(self, container_id: str, checkpoint_id: str, destination_host: Any) -> bool:\n",
    "        \"\"\"\n",
    "        Restores the state of a container from a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            container_id: ID of the container to restore.\n",
    "            checkpoint_id: ID of the checkpoint to use for restoration.\n",
    "            destination_host: The host where the container will be restored.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if restoration was successful, False otherwise.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If an error occurs during the restoration process.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_id)\n",
    "            \n",
    "            # Step 1: Verify checkpoint integrity\n",
    "            if not await self._verify_checkpoint(checkpoint_path):\n",
    "                raise Exception(f\"Checkpoint {checkpoint_id} failed integrity check\")\n",
    "\n",
    "            # Step 2: Decompress checkpoint if it's optimized\n",
    "            decompressed_path = await self._decompress_checkpoint(checkpoint_path)\n",
    "\n",
    "            # Step 3: Load container metadata\n",
    "            metadata = await self._load_metadata(decompressed_path)\n",
    "\n",
    "            # Step 4: Prepare the container environment\n",
    "            await self._prepare_container_environment(metadata, destination_host)\n",
    "\n",
    "            # Step 5: Restore container state\n",
    "            if self.container_runtime == 'docker':\n",
    "                await self._docker_restore(container_id, decompressed_path, metadata)\n",
    "            elif self.container_runtime == 'criu':\n",
    "                await self._criu_restore(container_id, decompressed_path, metadata)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported container runtime: {self.container_runtime}\")\n",
    "\n",
    "            # Step 6: Verify restored container\n",
    "            if not await self._verify_restored_container(container_id, metadata):\n",
    "                raise Exception(f\"Restored container {container_id} failed verification\")\n",
    "\n",
    "            print(f\"Container {container_id} restored successfully from checkpoint {checkpoint_id}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error restoring container {container_id} from checkpoint {checkpoint_id}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            # Clean up temporary decompressed checkpoint\n",
    "            if 'decompressed_path' in locals():\n",
    "                shutil.rmtree(decompressed_path, ignore_errors=True)\n",
    "\n",
    "    async def _verify_checkpoint(self, checkpoint_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Verifies the integrity of the checkpoint.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check for essential files\n",
    "            essential_files = ['metadata.json', 'checkpoint']\n",
    "            for file in essential_files:\n",
    "                if not os.path.exists(os.path.join(checkpoint_path, file)):\n",
    "                    print(f\"Missing essential file: {file}\")\n",
    "                    return False\n",
    "\n",
    "            # Verify metadata\n",
    "            metadata = await self._load_metadata(checkpoint_path)\n",
    "            if not all(key in metadata for key in ['id', 'name', 'image', 'env', 'cmd']):\n",
    "                print(\"Incomplete metadata in checkpoint\")\n",
    "                return False\n",
    "\n",
    "            # Verify checkpoint data integrity (example: check file sizes)\n",
    "            checkpoint_size = sum(os.path.getsize(os.path.join(checkpoint_path, f)) for f in os.listdir(checkpoint_path) if os.path.isfile(os.path.join(checkpoint_path, f)))\n",
    "            if checkpoint_size == 0:\n",
    "                print(\"Checkpoint data appears to be empty\")\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error during checkpoint verification: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    async def _decompress_checkpoint(self, checkpoint_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Decompresses the checkpoint if it's compressed.\n",
    "        \"\"\"\n",
    "        decompressed_path = tempfile.mkdtemp(prefix=\"decompressed_checkpoint_\")\n",
    "        \n",
    "        for root, _, files in os.walk(checkpoint_path):\n",
    "            for file in files:\n",
    "                source_file = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(source_file, checkpoint_path)\n",
    "                destination_file = os.path.join(decompressed_path, relative_path)\n",
    "\n",
    "                if file.endswith('.zlib'):\n",
    "                    destination_file = destination_file[:-5]  # Remove .zlib extension\n",
    "                    async with aiofiles.open(source_file, 'rb') as sf, aiofiles.open(destination_file, 'wb') as df:\n",
    "                        compressed_data = await sf.read()\n",
    "                        decompressed_data = zlib.decompress(compressed_data)\n",
    "                        await df.write(decompressed_data)\n",
    "                else:\n",
    "                    # If not compressed, just copy the file\n",
    "                    shutil.copy2(source_file, destination_file)\n",
    "\n",
    "        return decompressed_path\n",
    "\n",
    "    async def _load_metadata(self, checkpoint_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Loads the container metadata from the checkpoint.\n",
    "        \"\"\"\n",
    "        metadata_path = os.path.join(checkpoint_path, \"metadata.json\")\n",
    "        async with aiofiles.open(metadata_path, 'r') as f:\n",
    "            return json.loads(await f.read())\n",
    "\n",
    "    async def _prepare_container_environment(self, metadata: Dict[str, Any], destination_host: Any):\n",
    "        \"\"\"\n",
    "        Prepares the environment for the container on the destination host.\n",
    "        \"\"\"\n",
    "        # Ensure the required image is available\n",
    "        await self._ensure_image(metadata['image'], destination_host)\n",
    "\n",
    "        # Set up volumes\n",
    "        for volume in metadata['volumes']:\n",
    "            await self._setup_volume(volume, destination_host)\n",
    "\n",
    "        # Configure network\n",
    "        await self._configure_network(metadata['network_settings'], destination_host)\n",
    "\n",
    "    async def _ensure_image(self, image: str, destination_host: Any):\n",
    "        \"\"\"\n",
    "        Ensures the required image is available on the destination host.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            await self.docker_client.images.pull(image)\n",
    "        except docker.errors.ImageNotFound:\n",
    "            print(f\"Image {image} not found. Attempting to pull...\")\n",
    "            await self.docker_client.images.pull(image)\n",
    "\n",
    "    async def _setup_volume(self, volume: Dict[str, Any], destination_host: Any):\n",
    "        \"\"\"\n",
    "        Sets up a volume on the destination host.\n",
    "        \"\"\"\n",
    "        volume_name = volume['Name']\n",
    "        if not await self._volume_exists(volume_name, destination_host):\n",
    "            await self._create_volume(volume_name, destination_host)\n",
    "\n",
    "    async def _configure_network(self, network_settings: Dict[str, Any], destination_host: Any):\n",
    "        \"\"\"\n",
    "        Configures the network for the restored container on the destination host.\n",
    "        \"\"\"\n",
    "        network_name = list(network_settings['Networks'].keys())[0]\n",
    "        if not await self._network_exists(network_name, destination_host):\n",
    "            await self._create_network(network_name, destination_host)\n",
    "\n",
    "    async def _docker_restore(self, container_id: str, checkpoint_path: str, metadata: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Restores a container using Docker.\n",
    "        \"\"\"\n",
    "        cmd = [\n",
    "            \"docker\", \"create\",\n",
    "            \"--name\", metadata['name'],\n",
    "            \"--checkpoint-dir\", checkpoint_path,\n",
    "            \"--checkpoint\", \"checkpoint\",\n",
    "        ] + self._build_docker_run_args(metadata)\n",
    "\n",
    "        cmd.append(metadata['image'])\n",
    "        cmd.extend(metadata['cmd'])\n",
    "\n",
    "        process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n",
    "        stdout, stderr = await process.communicate()\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            raise Exception(f\"Docker container creation failed: {stderr.decode()}\")\n",
    "\n",
    "        # Start the restored container\n",
    "        await self.docker_client.containers.get(container_id).start()\n",
    "\n",
    "    async def _criu_restore(self, container_id: str, checkpoint_path: str, metadata: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Restores a container using CRIU.\n",
    "        \"\"\"\n",
    "        cmd = [\n",
    "            \"criu\", \"restore\",\n",
    "            \"-d\", \"--shell-job\",\n",
    "            \"--manage-cgroups\",\n",
    "            \"-D\", checkpoint_path\n",
    "        ]\n",
    "\n",
    "        process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n",
    "        stdout, stderr = await process.communicate()\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            raise Exception(f\"CRIU restore failed: {stderr.decode()}\")\n",
    "\n",
    "    async def _verify_restored_container(self, container_id: str, metadata: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Verifies that the restored container is running and matches the original configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            container = self.docker_client.containers.get(container_id)\n",
    "            \n",
    "            # Check container status\n",
    "            if container.status != 'running':\n",
    "                print(f\"Restored container is not running. Current status: {container.status}\")\n",
    "                return False\n",
    "\n",
    "            # Verify container configuration\n",
    "            inspect_data = await self.docker_client.api.inspect_container(container_id)\n",
    "            \n",
    "            if inspect_data['Config']['Image'] != metadata['image']:\n",
    "                print(f\"Image mismatch. Expected: {metadata['image']}, Actual: {inspect_data['Config']['Image']}\")\n",
    "                return False\n",
    "\n",
    "            if inspect_data['Config']['Cmd'] != metadata['cmd']:\n",
    "                print(f\"Command mismatch. Expected: {metadata['cmd']}, Actual: {inspect_data['Config']['Cmd']}\")\n",
    "                return False\n",
    "\n",
    "            # Verify environment variables\n",
    "            if set(inspect_data['Config']['Env']) != set(metadata['env']):\n",
    "                print(\"Environment variables mismatch\")\n",
    "                return False\n",
    "\n",
    "            # Verify volumes\n",
    "            if len(inspect_data['Mounts']) != len(metadata['volumes']):\n",
    "                print(\"Volume configuration mismatch\")\n",
    "                return False\n",
    "\n",
    "            # Verify network settings\n",
    "            if inspect_data['NetworkSettings']['Networks'].keys() != metadata['network_settings']['Networks'].keys():\n",
    "                print(\"Network configuration mismatch\")\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error verifying restored container: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _build_docker_run_args(self, metadata: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Builds Docker run arguments based on the container metadata.\n",
    "        \"\"\"\n",
    "        args = []\n",
    "        \n",
    "        # Add environment variables\n",
    "        for env in metadata['env']:\n",
    "            args.extend(['-e', env])\n",
    "\n",
    "        # Add volume mounts\n",
    "        for volume in metadata['volumes']:\n",
    "            args.extend(['-v', f\"{volume['Source']}:{volume['Destination']}\"])\n",
    "\n",
    "        # Add network settings\n",
    "        for network, config in metadata['network_settings']['Networks'].items():\n",
    "            args.extend(['--network', network])\n",
    "            if 'IPAddress' in config:\n",
    "                args.extend(['--ip', config['IPAddress']])\n",
    "\n",
    "        return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinates the migration process, integrating all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlexiMigrate Framework\n",
    "## Integrates all components into a cohesive framework.\n",
    "\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "class FlexiMigrate:\n",
    "    def __init__(self, policies):\n",
    "        # Initialize Components\n",
    "        self.performance_metrics_collector = PerformanceMetricsCollector()\n",
    "        self.resource_utilization_analyzer = ResourceUtilizationAnalyzer(thresholds={\n",
    "            'cpu_threshold': 80,\n",
    "            'memory_threshold': 70\n",
    "        })\n",
    "        self.policy_enforcer = PolicyEnforcer(policies)\n",
    "        self.workload_analyzer = WorkloadAnalyzer()\n",
    "        self.resource_optimizer = ResourceOptimizer()\n",
    "        \n",
    "        # Create NetworkManager first\n",
    "        self.network_manager = NetworkManager()\n",
    "        \n",
    "        # Now create MigrationPlanner with network_manager\n",
    "        self.migration_planner = MigrationPlanner(self.network_manager)\n",
    "        \n",
    "        self.decision_engine = DecisionEngine(\n",
    "            workload_analyzer=self.workload_analyzer,\n",
    "            resource_optimizer=self.resource_optimizer,\n",
    "            migration_planner=self.migration_planner,\n",
    "            policies=policies\n",
    "        )\n",
    "        self.checkpointing_module = CheckpointingModule()\n",
    "        self.delta_tracker = DeltaTracker()\n",
    "        self.state_restoration_module = StateRestorationModule()\n",
    "        self.state_synchronizer = StateSynchronizer(\n",
    "            checkpointing_module=self.checkpointing_module,\n",
    "            delta_tracker=self.delta_tracker,\n",
    "            state_restoration_module=self.state_restoration_module\n",
    "        )\n",
    "        self.logging_monitoring = LoggingAndMonitoringModule()\n",
    "        self.migration_strategy_selector = MigrationStrategySelector()\n",
    "        self.migration_manager = MigrationManager(\n",
    "            decision_engine=self.decision_engine,\n",
    "            state_synchronizer=self.state_synchronizer,\n",
    "            resource_allocator=self.resource_allocator\n",
    "            migration_strategy_selector=self.migration_strategy_selector,\n",
    "            logging_monitoring=self.logging_monitoring,\n",
    "        )\n",
    "        self.container_runtime_interface = ContainerRuntimeInterface()\n",
    "        self.nested_container_manager = NestedContainerManager(self.container_runtime_interface)\n",
    "        self.image_manager = ImageManager(self.container_runtime_interface)\n",
    "        \n",
    "        # Update NetworkManager with migration_manager\n",
    "        self.network_manager.set_migration_manager(self.migration_manager)\n",
    "        \n",
    "        self.network_orchestrator = NetworkOrchestrator(migration_coordinator=self.migration_manager)\n",
    "\n",
    "    def run(self):\n",
    "        # Start Prometheus metrics server\n",
    "        start_http_server(8000)\n",
    "        self.logging_monitoring.log(\"FlexiMigrate monitoring started\")\n",
    "\n",
    "        # Start Network Orchestrator\n",
    "        self.network_orchestrator.start()\n",
    "\n",
    "        # Start Migration Manager Threads\n",
    "        monitoring_thread = threading.Thread(target=self._monitoring_loop, daemon=True)\n",
    "        monitoring_thread.start()\n",
    "\n",
    "        migration_thread = threading.Thread(target=self._migration_loop, daemon=True)\n",
    "        migration_thread.start()\n",
    "\n",
    "        # Start Network Manager (OS-Ken App)\n",
    "        # In a real deployment, NetworkManager would be run as a separate application\n",
    "\n",
    "    def _monitoring_loop(self):\n",
    "        while True:\n",
    "            # Update Host Metrics\n",
    "            for host in self.decision_engine.resource_optimizer.hosts.values():\n",
    "                self.performance_metrics_collector.update_host_metrics(host)\n",
    "                over, under = self.resource_utilization_analyzer.analyze_host_utilization(host)\n",
    "                if over:\n",
    "                    self.logging_monitoring.log(f\"Host {host.host_id} is overutilized\")\n",
    "                if under:\n",
    "                    self.logging_monitoring.log(f\"Host {host.host_id} is underutilized\")\n",
    "\n",
    "            # Update Container Metrics\n",
    "            for host in self.decision_engine.resource_optimizer.hosts.values():\n",
    "                for container in host.containers:\n",
    "                    self.performance_metrics_collector.update_container_metrics(container)\n",
    "                    needs_migration = self.resource_utilization_analyzer.analyze_container_utilization(container)\n",
    "                    if needs_migration:\n",
    "                        best_host = self.resource_optimizer.select_best_host(self.resource_optimizer.hosts, container)\n",
    "                        if best_host:\n",
    "                            migration_request = MigrationRequest(\n",
    "                                container_id=container.container_id,\n",
    "                                source_host=host,\n",
    "                                destination_host=best_host,\n",
    "                                migration_type=self.migration_strategy_selector.select_strategy(\n",
    "                                    container,\n",
    "                                    host,\n",
    "                                    best_host\n",
    "                                )\n",
    "                            )\n",
    "                            if self.decision_engine.policy_enforcer.enforce_policies(migration_request):\n",
    "                                self.migration_manager.add_migration_request(migration_request)\n",
    "                                self.network_orchestrator.handle_network_changes(migration_request)\n",
    "            time.sleep(10)  # Adjust the monitoring interval as needed\n",
    "\n",
    "    def _migration_loop(self):\n",
    "        while True:\n",
    "            self.migration_manager.process_migrations()\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [\n",
    "    {\n",
    "        'policy_name': 'adaptive_load_balancing',\n",
    "        'CONTEXT': ['source_cpu_utilization', 'destination_cpu_utilization', 'time_of_day', 'network_congestion_prob', 'service_type'],\n",
    "        'CONDITIONS': '(source_cpu_utilization > 80 and destination_cpu_utilization < 50) or '\n",
    "                      '(time_of_day >= 18 and time_of_day <= 22 and service_type == \"critical\") or '\n",
    "                      '(network_congestion_prob < 0.2)',\n",
    "        'ACTIONS': ['allow_migration', 'set_priority(\"high\")', 'trigger_load_balancer_reconfiguration'],\n",
    "        'CONSTRAINTS': {'max_concurrent_migrations': 5, 'migration_duration': 300},\n",
    "        'PRIORITY': 2\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    policies = [\n",
    "        {\n",
    "            'policy_name': 'adaptive_load_balancing',\n",
    "            'CONTEXT': ['source_cpu_utilization', 'destination_cpu_utilization', 'time_of_day', 'network_congestion_prob', 'service_type'],\n",
    "            'CONDITIONS': '(source_cpu_utilization > 80 and destination_cpu_utilization < 50) or '\n",
    "                          '(time_of_day >= 18 and time_of_day <= 22 and service_type == \"critical\") or '\n",
    "                          '(network_congestion_prob < 0.2)',\n",
    "            'ACTIONS': ['allow_migration', 'set_priority(\"high\")', 'trigger_load_balancer_reconfiguration'],\n",
    "            'CONSTRAINTS': {'max_concurrent_migrations': 5, 'migration_duration': 300},\n",
    "            'PRIORITY': 2\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    flexi_migrate = FlexiMigrate(policies=policies)\n",
    "    flexi_migrate.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "host1 = Host(host_id='host1', total_cpu=16, total_memory=32768, total_storage=1000)\n",
    "host2 = Host(host_id='host2', total_cpu=16, total_memory=32768, total_storage=1000)\n",
    "\n",
    "flexi_migrate.decision_engine.resource_optimizer.hosts = {\n",
    "    host1.host_id: host1,\n",
    "    host2.host_id: host2\n",
    "}\n",
    "\n",
    "container1 = Container(container_id='container1', image='nginx:latest', cpu_limit=4, memory_limit=2048, storage_limit=50)\n",
    "container1.host = 'host1'\n",
    "host1.containers.append(container1)\n",
    "\n",
    "flexi_migrate.decision_engine.resource_optimizer.containers = {\n",
    "    container1.container_id: container1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MigrationRequest:\n",
    "    def __init__(self, container_id, source_host, destination_host, migration_type, priority=1):\n",
    "        self.container_id = container_id\n",
    "        self.source_host = source_host\n",
    "        self.destination_host = destination_host\n",
    "        self.migration_type = migration_type\n",
    "        self.priority = priority\n",
    "        self.state = MigrationState.PENDING\n",
    "\n",
    "migration_request = MigrationRequest(\n",
    "    container_id='container1',\n",
    "    source_host=flexi_migrate.resource_optimizer.hosts['host1'],\n",
    "    destination_host=flexi_migrate.resource_optimizer.hosts['host2'],\n",
    "    migration_type=MigrationStrategy.LIVE_MIGRATION\n",
    ")\n",
    "\n",
    "if flexi_migrate.decision_engine.policy_enforcer.enforce_policies(migration_request):\n",
    "    flexi_migrate.migration_manager.add_migration_request(migration_request)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
